{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GSx2icY1x8KN",
        "4QgbpN3u5lkr",
        "m5t_E08t934w",
        "PxM5ChSj_W8D",
        "S-qaf1QyCGNG",
        "HbFfkaJ8n-7W",
        "RUidctwlk6is",
        "gP1KVjUWl2RJ",
        "DQ5s8mg43YMk",
        "oFgL3tI499yQ",
        "YFbjyvCJT3Sv",
        "q8pGHHxBUqGr",
        "XpUeoS12Ynh5",
        "pLPMy5mlyHRo",
        "yOQ4SsPmFBSl",
        "u2oiRHPULDzT",
        "5JI_2sWvLHfY",
        "co-_JxmXyKBg",
        "86HNLimozRiT",
        "Ah008BaB0uNr",
        "cqGL3W8CPlho",
        "vo7-ZM0XoF1D",
        "4-Iv8k-rzZg2",
        "unkUW5s-YIJx",
        "VB5XWq0ZAm3w",
        "Yk8ltGl7akJT",
        "6GuwuSkEdFVp",
        "mSKQMNOFga3o",
        "cD2vbmfGW8bn",
        "EhaGLq9hsprz",
        "KZcf4sjvzvuI",
        "IHfnel4az30g",
        "9hMzK7cR_o2K"
      ],
      "authorship_tag": "ABX9TyPO64WFCDBVoGmLsLXlm5Zw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsha8745/IBM-WATSON/blob/master/AI_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSx2icY1x8KN"
      },
      "source": [
        "#Introduction to Deep Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY4xM5bu5hbk"
      },
      "source": [
        "##Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QgbpN3u5lkr"
      },
      "source": [
        "###Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFfGDhlm5nC4"
      },
      "source": [
        ">In this video, we will start talking about deep learning and how the recent advances in the field have led to amazing and mind-blowing applications. I am sure that you are aware that deep learning is one of the hottest subjects in data science, if not the hottest, especially with the tremendous amount of fascinating projects that are surfacing with the help of deep learning; projects which people deemed almost impossible with just a little over a decade ago. Therefore, there is a lot of excitement about deep learning. In this video, I will share with you some amazing and recent applications of deep learning that will hopefully inspire and motivate you even more about deep learning. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(847).png?raw=true)\r\n",
        "\r\n",
        ">The first amazing application is color restoration, where a given image in greyscale is automatically turned into a colored one. A group of researchers in Japan built a system using convolutional neural networks that can take a grayscale image, like these ones, and add life to them by turning them into colored ones. You can find many other awesome examples by following this link, which you can also find below the video. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(848).png?raw=true)\r\n",
        "\r\n",
        ">Another really cool but double-sworded application is speech enactment, where an audio clip is synthesized with a video, and the lip movements in the video are synced with the sounds and words in the audio clip. Many attempts have been made in the past to build such a system, but many of them produced results that looked uncanny. Recently, a group of researchers at the University of Washington built the first system that generates realistic results by training a recurrent neural network on a large corpus of video data of a single person. The subject of their case study was the former President of the United States, Barack Obama. Let's look at their example. So here is an audio clip from one of Obama's speeches. \"It's been less than a week since the deadliest mass shooting in American history.\" The audio clip was synthesized with a video of one of his other speeches, and his lip movements were synced with the words and sounds in the audio clip. Let's take a look. \"It's been less than a week since the deadliest mass shooting in American history.\" Anyone watching the video can't really tell that the video was synthesized. Not only that, but their system is also capable of extracting an audio from a video and syncing the lip movements in another video with the audio from the first video. Let's look at an example of this. \"Especially our friends who were lesbian, gay, bisexual, or transgender. I visited with the families of many of the victims on Thursday and one thing I told them is that they're not alone.\" Your jaw dropped yet? \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(849).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(850).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Another fascinating application of deep learning is automatic handwriting generation. Alex Graves at the University of Toronto used recurrent neural networks to design an algorithm that can rewrite a given message in highly realistic cursive handwriting in a wide variety of styles. So you can type some text in this field and you can either select the style of handwriting to be generated or let the algorithm randomly select it for you.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(851).png?raw=true)\r\n",
        "\r\n",
        ">There is a plethora of other applications such as automatic machine translation, where convolutional neural networks are used to translate text in an image on the fly. Another application is automatically adding sounds to silent movies, where a deep learning model uses a database of pre-recorded sounds to select a sound to play that best matches what is taking place in the scene. Not to leave out the popular applications of classifying objects in images and self-driving cars. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(852).png?raw=true)\r\n",
        ">For almost all of the aforementioned applications, you heard me say neural networks again and again. So you must be asking haven't neural networks been around for quite some time? How come all of a sudden they are taking off and becoming very popular with endless applications? In order to answer this question, let's start learning the specifics of neural networks and deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5t_E08t934w"
      },
      "source": [
        "###Neurons and Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3rXh1tv-sWD"
      },
      "source": [
        ">All the algorithms that are used in deep learning are largely inspired by the way neurons and neural networks function and process data in the brain. This image is one of the very first pictures of a neuron. It was drawn by Santiago Ramon y Cajal, back in 1899 based on what he saw after placing a pigeon's brain under the microscope. He is now known as the father of modern neuroscience, but based on his drawing, the neurons, one of them labeled A, have big bodies in the middle and long arms that stretch out and branch off to connect with other neurons. This other image here is that of a neural network and has a bunch or thousands of neurons in what looks like a brain tissue. Tt gives you a sense of how tightly they are packed together and how many of them are in a small brain tissue\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(853).png?raw=true)\r\n",
        "\r\n",
        ">Going back to the drawing of neurons by Ramon y Cajal, let's rotate it 90 degrees to the left. I bet this way it is starting to look a little familiar since it slightly resembles drawings of artificial neural networks that you must have seen. Here is a cartoon drawing of the neuron. The main body of the neuron is called the soma, which contains the nucleus of the neuron. The big network of arms sticking out of the body is called the dendrites, and then the long arm that sticks out of the soma in the other direction is called the axon. The whiskers at the end of the axon are called the terminal buttons or synapses. So the dendrites receive electrical impulses which carry information, or data, from sensors or terminal buttons of other adjoining neurons. The dendrites then carry the impulses or data to the soma. In the nucleus, electrical impulses, or the data, are processed by combining them together, and then they are passed on to the axon. The axon then carries the processed information to the terminal button or synapse, and the output of this neuron becomes the input to thousands of other neurons. Learning in the brain occurs by repeatedly activating certain neural connections over others, and this reinforces those connections. This makes them more likely to produce a desired outcome given a specified input. Once the desired outcome occurs, the neural connections causing that outcome become strengthened. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(854).png?raw=true)\r\n",
        "\r\n",
        ">An artificial neuron behaves in the same way as a biological neuron. So it consists of a soma, dendrites, and an axon to pass on the output of this neuron to other neurons. The end of the axon can branch off to connect to many other neurons, but for simplicity we are just showing one branch here. The learning process also very much resembles the way learning occurs in the brain as you will see in the next couple of videos. Now that we understand the different parts of an artificial neuron, let's learn how we formulate the way artificial neural networks process information.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(855).png?raw=true)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxM5ChSj_W8D"
      },
      "source": [
        "###Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWZ6UwwA-Po"
      },
      "source": [
        ">In this video, we will start learning about the mathematical formulation of neural networks. In the previous video, we established that the shape of an artificial neuron looks like this in order to resemble a real biological neuron. For a network of neurons, we normally divide it into different layers: the first layer that feeds the input into the network is obviously called the input layer. The set of nodes that provide the output of the network is called the output layer. And any sets of nodes in between the input and the output layers are called the hidden layers.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(855).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(856).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">When working with neural networks, the three main topics that we deal with are: forward propagation, backpropagation, and activation functions. The rest of this video will be mainly about forward propagation, and I will explain it through examples with numbers.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(857).png?raw=true)\r\n",
        "\r\n",
        ">Forward propagation is the process through which data passes through layers of neurons in a neural network from the input layer all the way to the output layer. Let's use one neuron and mathematically formulate the way information flows through it. As shown here, the data flows through each neuron by connections or the dendrites. Every connection has a specific weight by which the flow of data is regulated. Here x1 and x2 are the two inputs, they could be an integer or float. When these inputs pass through the connections, they're adjusted depending on the connection weights, w1 and w2. The neuron then processes this information by outputting a weighted sum of these inputs. It also adds a constant to the sum which is referred to as the bias. So, z here is the linear combination of the inputs and weights along with the bias, and a is the output of the network. For consistency, we will stick to these letters throughout the course, so z will always represent the linear combination of the inputs, and a will always represent the output of a neuron. However, simply outputting a weighted sum of the inputs limits the tasks that can be performed by the neural network. Therefore, a better processing of the data would be to map the weighted sum to a nonlinear space. A popular function is the sigmoid function, where if the weighted sum is a very large positive number, then the output of the neuron is close to 1, and if the weighted sum is a very large negative number, then the output of the neuron is close to zero. Non-linear transformations like the sigmoid function are called activation functions. Activation functions are another extremely important feature of artificial neural networks. They basically decide whether a neuron should be activated or not. In other words, whether the information that the neuron is receiving is relevant or should be ignored. The takeaway message here is that a neural network without an activation function is essentially just a linear regression model. The activation function performs non-linear transformation to the input enabling the neural network of learning and performing more complex tasks, such as image classifications and language translations. For further simplification, I am going to proceed with a neural network of one neuron and one input\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(858).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(859).png?raw=true)\r\n",
        "\r\n",
        ">Let's go over an example of how to compute the output. Let's say that the value of x1 is 0.1, and we want to predict the output for this input. The network has optimized weight and bias where w1 is 0.15 and b1 is 0.4. The first step is to calculate z, which is the dot product of the inputs and the corresponding weights plus the bias. So we find that z is 0.415. The neuron then uses the sigmoid function to apply non-linear transformation to z. Therefore, the output of the neuron is 0.6023. For a network with two neurons, the output from the first neuron will be the input to the second neuron. The rest is then exactly the same. The second neuron takes the input and computes the dot product of the input, which is a1 in this case, and the weight which is w2, and adds the bias which is b2. Using a sigmoid function as the activation function, the output of the network would be 0.7153. And this would be the predicted value for the input 0.1. This is in essence how a neural network predicts the output for any given input. No matter how complicated the network gets, it is the same exact process. To summarize, given a neural network with a set of weights and biases, you should be able to compute the output of the network for any given input. In the next video, we will start learning how to train a neural network and optimize the weights and the biases.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(860).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qaf1QyCGNG"
      },
      "source": [
        "###LAB: Artificial Neural Networks - Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "SJLTpplcB9Nh"
      },
      "source": [
        "#### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "mxc5yOBdB9Ni"
      },
      "source": [
        "In this lab, we will build a neural network from scratch and code how it performs predictions using forward propagation. Please note that all deep learning libraries have the entire training and prediction processes implemented, and so in practice you wouldn't really need to build a neural network from scratch. However, hopefully completing this lab will help you understand neural networks and how they work even better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5MxwS1oB9Ni"
      },
      "source": [
        "<h2>Artificial Neural Networks - Forward Propagation</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. Initalize a Network</h5>\n",
        "<h5> 2. Compute Weighted Sum at Each Node. </h5>\n",
        "<h5> 3. Compute Node Activation </h5>\n",
        "<h5> 4. Access your <b>Flask</b> app via a webpage anywhere using a custom link. </h5>     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "fp2zjvMxB9Ni"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>    \n",
        "\n",
        "1.  <a href=\"#item11\">Recap</a>\n",
        "2.  <a href=\"#item12\">Initalize a Network</a>  \n",
        "3.  <a href=\"#item13\">Compute Weighted Sum at Each Node</a>  \n",
        "4.  <a href=\"#item14\">Compute Node Activation</a>  \n",
        "5.  <a href=\"#item15\">Forward Propagation</a>\n",
        "\n",
        "</font>\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "bgwtvOtlB9Nn"
      },
      "source": [
        "#### Recap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "HkNBEuFHB9No"
      },
      "source": [
        "From the videos, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "tFuHZoWkB9Np"
      },
      "source": [
        "<img src=\"http://cocl.us/neural_network_example\" alt=\"Neural Network Example\" width=600px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "ZPnM0vfLB9Nq"
      },
      "source": [
        "Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoIOoQPV5VMO"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "w = weights = np.around(np.random.uniform(size=6),decimals=2) # initialize the weights\r\n",
        "b = biases = np.around(np.random.uniform(size=3),decimals=2) # initialize the weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4ZyzWeErB9Ns"
      },
      "source": [
        "Let's print the weights and biases for sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "FbwM3GmZB9Ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e6102b-07ed-4869-d1bb-10f6388e8a99"
      },
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.55 0.72 0.6  0.54 0.42 0.65]\n",
            "[0.44 0.89 0.96]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "eQKai5rgB9Nt"
      },
      "source": [
        "Now that we have the weights and the biases defined for the network, let's compute the output for a given input, $x_1$ and $x_2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBNHuyCiE3f_"
      },
      "source": [
        "x1 = 0.5\r\n",
        "x2 = 0.85"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "RnWKwhccB9Nu"
      },
      "source": [
        "Let's start by computing the wighted sum of the inputs, $z\\_{1, 1}$, at the first node of the hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev9sW7eeFIo4",
        "outputId": "2d246758-3730-48f7-b58d-7f98d3a95d32"
      },
      "source": [
        "z11 = x1*w[0] + x2*w[1] + b[0]\r\n",
        "z11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "YilmuusAB9Nv"
      },
      "source": [
        "Next, let's compute the weighted sum of the inputs, $z\\_{1, 2}$, at the second node of the hidden layer. Assign the value to **z_12**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgLkvnmDGfRb",
        "outputId": "d45490e7-e735-4847-9cf7-b41d1c9f6047"
      },
      "source": [
        "z12 = x1*w[2] + x2*w[3] + b[1]\r\n",
        "z12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.649"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Xl1gbrlRB9Ny"
      },
      "source": [
        "Next, assuming a sigmoid activation function, let's compute the activation of the first node, $a\\_{1, 1}$, in the hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92PdcGm_HbGY"
      },
      "source": [
        "def sigmoid(z):\r\n",
        "  y = 1/(1+np.exp(-z))\r\n",
        "  return np.round(y,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djWtZTUBNR0K",
        "outputId": "0946af8a-5b65-448f-ef04-65220a9f4222"
      },
      "source": [
        "a11 = sigmoid(z11)\r\n",
        "a11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "IhDuBU4kB9Nz"
      },
      "source": [
        "Let's also compute the activation of the second node, $a\\_{1, 2}$, in the hidden layer. Assign the value to **a_12**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhwRAHUtPsw4",
        "outputId": "9a40c7ba-c476-498a-db54-b26017335182"
      },
      "source": [
        "a12 = sigmoid(z12)\r\n",
        "a12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8388"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "NpfGUeqpB9N0"
      },
      "source": [
        "Now these activations will serve as the inputs to the output layer. So, let's compute the weighted sum of these inputs to the node in the output layer. Assign the value to **z_2**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UnlgdkXP0S7",
        "outputId": "2fd08d3f-7da0-42b0-d833-2bc9569f319c"
      },
      "source": [
        "z2 = a11*w[4] + a12*w[5] +b[2]\r\n",
        "z2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.837146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyMn_RaFQk_Q",
        "outputId": "0c1d18d4-477d-43cc-a652-9ad84dfc19d4"
      },
      "source": [
        "a2 = sigmoid(z2)\r\n",
        "a2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMDAKK3XRlDH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "MyUjfV7oB9N3"
      },
      "source": [
        "Obviously, neural networks for real problems are composed of many hidden layers and many more nodes in each layer. So, we can't continue making predictions using this very inefficient approach of computing the weighted sum at each node and the activation of each node manually. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WHl5YoNcB9N3"
      },
      "source": [
        "In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "x0RObS7SB9N3"
      },
      "source": [
        "<img src=\"http://cocl.us/general_neural_network\" alt=\"Neural Network General\" width=600px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "700NMpS1B9N5"
      },
      "source": [
        "#### Initialize a Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "ZFV6NdItB9N5"
      },
      "source": [
        "Let's start by formally defining the structure of the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPkw4WNfSIn-"
      },
      "source": [
        "n = 2 # number of inputs\r\n",
        "num_hidden_layers = 2 # number of hidden layers\r\n",
        "m = [2, 2] # number of nodes in each hidden layer\r\n",
        "num_nodes_output = 1 # number of nodes in the output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "LuAHPFKoB9N5"
      },
      "source": [
        "Now that we defined the structure of the network, let's go ahead and inititailize the weights and the biases in the network to random numbers. In order to be able to initialize the weights and the biases to random numbers, we will need to import the **Numpy** library.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvqGernBS6qr",
        "outputId": "35153074-2edf-430d-b641-799c7ba128f2"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "num_nodes_prev = n # number of nodes in the previous layer\r\n",
        "\r\n",
        "network = {} # initialize network an an empty dictionary\r\n",
        "\r\n",
        "# loop through each layer and randomly initialize the weights and biases associated with each node\r\n",
        "# notice how we are adding 1 to the number of hidden layers in order to include the output layer\r\n",
        "for layer in range(num_hidden_layers+1):\r\n",
        "\r\n",
        "  # determine name of layer\r\n",
        "  if layer == num_hidden_layers:\r\n",
        "    layer_name = \"output\"\r\n",
        "    num_nodes = num_nodes_output\r\n",
        "  else:\r\n",
        "    layer_name = \"layer_{}\".format(layer+1)\r\n",
        "    num_nodes = m[layer]\r\n",
        "  \r\n",
        "  # initialize weights and biases associated with each node in the current layer\r\n",
        "  network[layer_name] = {}\r\n",
        "  for node in range(num_nodes):\r\n",
        "    node_name = \"node_{}\".format(node+1)\r\n",
        "    network[layer_name][node_name] = {\"weights\":np.round(np.random.uniform(size=num_nodes_prev),2),\r\n",
        "                                      \"bias\":np.round(np.random.uniform(size=1),2)}\r\n",
        "\r\n",
        "    num_nodes_prev = num_nodes\r\n",
        "\r\n",
        "print(network)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'layer_1': {'node_1': {'weights': array([0.55, 0.72]), 'bias': array([0.6])}, 'node_2': {'weights': array([0.54, 0.42]), 'bias': array([0.65])}}, 'layer_2': {'node_1': {'weights': array([0.44, 0.89]), 'bias': array([0.96])}, 'node_2': {'weights': array([0.38, 0.79]), 'bias': array([0.53])}}, 'output': {'node_1': {'weights': array([0.57, 0.93]), 'bias': array([0.07])}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Q-jOwsipB9N6"
      },
      "source": [
        "Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zc_NsknF2wQ"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWbSx4yBajl2"
      },
      "source": [
        "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\r\n",
        "  num_nodes_previous = num_inputs # number of nodes in the previous layer\r\n",
        "  network={}\r\n",
        "\r\n",
        "# loop through each layer and randomly initialize the weights and biases associated with each layer\r\n",
        "  for layer in range(num_hidden_layers+1):\r\n",
        "    \r\n",
        "    if layer == num_hidden_layers:\r\n",
        "      layer_name = \"output\" # name last layer in the network output\r\n",
        "      num_nodes = num_nodes_output\r\n",
        "    else:\r\n",
        "      layer_name = \"layer_{}\".format(layer+1)  # otherwise give the layer a number\r\n",
        "      num_nodes = num_nodes_hidden[layer]\r\n",
        "\r\n",
        "     #initialize weights and bias for each node\r\n",
        "    network[layer_name] = {}\r\n",
        "    for node in range(num_nodes):\r\n",
        "        node_name = \"node_{}\".format(node+1)\r\n",
        "        network[layer_name][node_name] = {\"weights\":np.round(np.random.uniform(size=num_nodes_previous),2),\r\n",
        "                                          \"bias\":np.round(np.random.uniform(size=1),2)} \r\n",
        "                                        \r\n",
        "    num_nodes_previous = num_nodes\r\n",
        "\r\n",
        "  return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "etq-MmnCB9N7"
      },
      "source": [
        "##### Use the _initialize_network_ function to create a network that:\n",
        "\n",
        "1.  takes 5 inputs\n",
        "2.  has three hidden layers\n",
        "3.  has 3 nodes in the first layer, 2 nodes in the second layer, and 3 nodes in the third layer\n",
        "4.  has 1 node in the output layer\n",
        "\n",
        "Call the network **small_network**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUAglMnKFEOD",
        "outputId": "eb0af48b-796b-4a55-921b-85612c0a260a"
      },
      "source": [
        "small_network = initialize_network(5,3,[3,2,3],1)\r\n",
        "small_network"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layer_1': {'node_1': {'bias': array([0.65]),\n",
              "   'weights': array([0.55, 0.72, 0.6 , 0.54, 0.42])},\n",
              "  'node_2': {'bias': array([0.53]),\n",
              "   'weights': array([0.44, 0.89, 0.96, 0.38, 0.79])},\n",
              "  'node_3': {'bias': array([0.83]),\n",
              "   'weights': array([0.57, 0.93, 0.07, 0.09, 0.02])}},\n",
              " 'layer_2': {'node_1': {'bias': array([0.8]),\n",
              "   'weights': array([0.78, 0.87, 0.98])},\n",
              "  'node_2': {'bias': array([0.64]), 'weights': array([0.46, 0.78, 0.12])}},\n",
              " 'layer_3': {'node_1': {'bias': array([0.52]), 'weights': array([0.14, 0.94])},\n",
              "  'node_2': {'bias': array([0.77]), 'weights': array([0.41, 0.26])},\n",
              "  'node_3': {'bias': array([0.02]), 'weights': array([0.46, 0.57])}},\n",
              " 'output': {'node_1': {'bias': array([0.94]),\n",
              "   'weights': array([0.62, 0.61, 0.62])}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "p7MoieHEB9N8"
      },
      "source": [
        "#### Compute Weighted Sum at Each Node\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "qGPf2C9uB9N9"
      },
      "source": [
        "The weighted sum at each node is computed as the dot product of the inputs and the weights plus the bias. So let's create a function called _compute_weighted_sum_ that does just that.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXkd01iqItp5"
      },
      "source": [
        "def weighted_sum(inputs,weights,bias):\r\n",
        "  return np.sum(inputs*weights)+bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "ajeKPipiB9N9"
      },
      "source": [
        "Let's generate 5 inputs that we can feed to **small_network**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-561alCN2f1",
        "outputId": "b43b64a3-96e1-4b0b-cba1-ce5e33b4a4fb"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy.random import seed\r\n",
        "\r\n",
        "seed(12)\r\n",
        "inputs = np.round(np.random.uniform(size=5),2)\r\n",
        "inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.15, 0.74, 0.26, 0.53, 0.01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "8dQTMLcAB9N-"
      },
      "source": [
        "##### Use the _compute_weighted_sum_ function to compute the weighted sum at the first node in the first hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAlcqzcsOkfD",
        "outputId": "b9b165a3-76f4-4586-b282-e2b174d47a00"
      },
      "source": [
        "node_weights = small_network[\"layer_1\"][\"node_1\"][\"weights\"]\r\n",
        "node_bias = small_network[\"layer_1\"][\"node_1\"][\"bias\"]\r\n",
        "\r\n",
        "weighted_sum_11 = weighted_sum(inputs,node_weights,node_bias)\r\n",
        "weighted_sum_11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.7117])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "XmQnZuwBB9N_"
      },
      "source": [
        "#### Compute Node Activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "LixYhvqrB9N_"
      },
      "source": [
        "Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XukcmkvxQIyB"
      },
      "source": [
        "def node_activation(z):\r\n",
        "  return (1/(1+np.exp(-z)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "_kZnhys3B9OA"
      },
      "source": [
        "##### Use the _node_activation_ function to compute the output of the first node in the first hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yReCrw8FQbbZ",
        "outputId": "49856cd1-1f01-4ef5-a29c-e18be0a91012"
      },
      "source": [
        "a11 = np.round(node_activation(weighted_sum_11),4)\r\n",
        "a11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8471])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auFYfewsRM2j",
        "outputId": "1ffd107c-0bb5-44fe-88fd-4787dd5e8c4f"
      },
      "source": [
        "#generalised output\r\n",
        "node_output = np.round(node_activation(weighted_sum(inputs,node_weights,node_bias)),4)\r\n",
        "node_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8471])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "sJkamyK8B9OC"
      },
      "source": [
        "#### Forward Propagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4yCeKMFHB9OC"
      },
      "source": [
        "The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the _compute_weighted_sum_ and _node_activation_ functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "5gMrvYIDB9OD"
      },
      "source": [
        "The way we are going to accomplish this is through the following procedure:\n",
        "\n",
        "1.  Start with the input layer as the input to the first hidden layer.\n",
        "2.  Compute the weighted sum at the nodes of the current layer.\n",
        "3.  Compute the output of the nodes of the current layer.\n",
        "4.  Set the output of the current layer to be the input to the next layer.\n",
        "5.  Move to the next layer in the network.\n",
        "6.  Repeat steps 2 - 4 until we compute the output of the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDB4ShtgS3w9",
        "outputId": "a072cfc8-e087-471a-d1ad-927f7d0cffa9"
      },
      "source": [
        "small_network"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layer_1': {'node_1': {'bias': array([0.65]),\n",
              "   'weights': array([0.55, 0.72, 0.6 , 0.54, 0.42])},\n",
              "  'node_2': {'bias': array([0.53]),\n",
              "   'weights': array([0.44, 0.89, 0.96, 0.38, 0.79])},\n",
              "  'node_3': {'bias': array([0.83]),\n",
              "   'weights': array([0.57, 0.93, 0.07, 0.09, 0.02])}},\n",
              " 'layer_2': {'node_1': {'bias': array([0.8]),\n",
              "   'weights': array([0.78, 0.87, 0.98])},\n",
              "  'node_2': {'bias': array([0.64]), 'weights': array([0.46, 0.78, 0.12])}},\n",
              " 'layer_3': {'node_1': {'bias': array([0.52]), 'weights': array([0.14, 0.94])},\n",
              "  'node_2': {'bias': array([0.77]), 'weights': array([0.41, 0.26])},\n",
              "  'node_3': {'bias': array([0.02]), 'weights': array([0.46, 0.57])}},\n",
              " 'output': {'node_1': {'bias': array([0.94]),\n",
              "   'weights': array([0.62, 0.61, 0.62])}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UiESnwSAfB"
      },
      "source": [
        "def forward_propagate(network,inputs):\r\n",
        "  layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\r\n",
        "    \r\n",
        "  for layer in network:\r\n",
        "      layer_data = network[layer]\r\n",
        "      \r\n",
        "      layer_outputs = []\r\n",
        "      for node in layer_data:\r\n",
        "        node_data = layer_data[node]\r\n",
        "\r\n",
        "        #compute the weighted sum and the output of each node at same time\r\n",
        "        node_output = node_activation(weighted_sum(layer_inputs,node_data[\"weights\"],node_data[\"bias\"]))\r\n",
        "        layer_outputs.append(np.round(node_output[0],4))\r\n",
        "\r\n",
        "      if layer != \"output\":\r\n",
        "        print(\"the outputs of the nodes in hidden layer number {} is {}\".format(layer.split(\"_\")[1],layer_outputs))\r\n",
        "\r\n",
        "      layer_inputs = layer_outputs ## set the output of this layer to be the input to next layer\r\n",
        "\r\n",
        "  network_predictions = layer_outputs\r\n",
        "  return network_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "QE9Sh0QfB9OD"
      },
      "source": [
        "##### Use the _forward_propagate_ function to compute the prediction of our small network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4u3DwfXUrM",
        "outputId": "21b9a99c-21c4-421e-f089-4cb7edf1eb7e"
      },
      "source": [
        "forward_propagate(small_network,inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the outputs of the nodes in hidden layer number 1 is [0.8471, 0.8473, 0.8415]\n",
            "the outputs of the nodes in hidden layer number 2 is [0.9536, 0.8571]\n",
            "the outputs of the nodes in hidden layer number 3 is [0.8114, 0.7996, 0.7206]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9151]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "kzLeDHtIB9OE"
      },
      "source": [
        "So we built the code to define a neural network. We can specify the number of inputs that a neural network can take, the number of hidden layers as well as the number of nodes in each hidden layer, and the number of nodes in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "6ENEkXU7B9OE"
      },
      "source": [
        "We first use the _initialize_network_ to create our neural network and define its weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "TP1RtZ6TB9OE"
      },
      "source": [
        "my_network = initialize_network(5, 3, [2, 3, 2], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "hYGX0wMwB9OE"
      },
      "source": [
        "Then, for a given input,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4bJK3lOEB9OE"
      },
      "source": [
        "inputs = np.around(np.random.uniform(size=5), decimals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "QM97me_lB9OE"
      },
      "source": [
        "we compute the network predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "BrwstO45B9OF"
      },
      "source": [
        "predictions = forward_propagate(my_network, inputs)\n",
        "print('The predicted values by the network for the given input are {}'.format(predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "3H4Lt3t6B9OF"
      },
      "source": [
        "Feel free to play around with the code by creating different networks of different structures and enjoy making predictions using the _forward_propagate_ function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbFfkaJ8n-7W"
      },
      "source": [
        "##Training a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvhetLWDoFNF"
      },
      "source": [
        "###Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMcd1KpWoJM2"
      },
      "source": [
        ">Let's say that we have some data, and here's a scatter plot of the data. For simplicity, I have generated data where z is twice x. And say we want to find the value of w that would generate a line that best fits this data. To do that, we define a cost or a loss function. One common cost or loss function is the one shown here as J, where we take the difference between the z values and the product of w and x's. We square that and we sum the squared difference across all values of z and x. The best value of w would then be the value that results in the minimum value of this cost or loss function. Let's take a look at how this cost function looks like. What makes this loss or cost function attractive is that it is a parabola, and has one global minimum or one unique solution. For the given data, the value of w that makes this cost function minimum, is w equals 2, meaning z equals 2x, which would result in a line that fits the points perfectly. This is a very simplified example as in real world datasets, the target variable z would be dependent on more than one variable and we can't just simply plot the cost function and visually determine the best value of the weights. So how do we determine the best value of w. The best value of w, or w's in case you have many weights to optimize, is determined through an algorithm called gradient descent. Gradient descent is an iterative optimization algorithm for finding the minimum of a function. To find the minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. What does that mean? We start at a random initial value of w. Let's call it w0, and say it's equal to 0.2, and we start taking steps towards the green dot which is w = 2. To determine in which direction to move, we compute the gradient of the loss function at the current value of w, which is 0.2. The gradient is given by the slope of the tangent at w = 0.2, and then the magnitude of the step is controlled by a parameter called the learning rate. The larger the learning rate, the bigger the step we take, and the smaller the learning rate, the smaller the step we take. Then we take the step and we move to w1. w1 is essentially computed as w0 minus the learning rate times the gradient at w0. This represents the first iteration of the algorithm. At w1, we repeat the same process of computing the gradient at w1 and using the same learning rate to control the magnitude of the step towards the minimum. We keep repeating this step again and again until we hit the minimum or a value of the cost function that is very close to the minimum, within a very small predefined threshold. Now when choosing the learning rate, we have to be very careful as a large learning rate can lead to big steps and eventually missing the minimum. On the other hand, a small learning rate can result in very small steps and therefore causing the algorithm to take a long time to find the minimum point. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(861).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29gMjhpGpIEy"
      },
      "source": [
        ">Now let's see how each iteration with a learning rate of 0.4 affects the way the resulting line fits the data on the left. We initialize w to 0, which means z equals 0. It is a horizontal line and therefore the cost is high and the line as you can see is a bad fit. After the 1st iteration, w moves closer to 2 and because the slope is very steep at w=0, the new value of w results in a big drop in the loss function. The resulting line fits better than the initial one but there is still room for improvement. After the 2nd iteration, w continues moving toward w = 2. Because the slope is not as steep as before, the step is not as big but the cost function still drops in value and the resulting line is moving closer to the ideal best fit line. The same observation is noted with the 3rd iteration, and the 4th iteration. After 4 iterations, you can see how we are almost there at w = 2, and the resulting line almost fits the scatterplot perfectly. With each iteration, the weight is updated in a way that's proportional to the negative of the gradient of the function at the current point. Therefore, if you initialize the weight to a value that is to the right of the minimum, then the positive gradient will result in w moving to the left towards the minimum. Now that we understand how to optimize a parameter that a function depends on, we are now ready to start learning about backpropagation and how neural networks learn and optimize their weights and biases.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(862).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQoYJmHfsNYo"
      },
      "source": [
        "###Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOobhKwfsas2"
      },
      "source": [
        ">In the previous videos, when discussing how neural networks make predictions using forward propagation, we assumed that the network had optimized weights and biases. But how do neural networks train and optimize their weights and biases for a given problem and data set? Training is done in a supervised learning setting, where each data point has a corresponding label or ground truth. And training is needed when the predicted value by the neural network obviously does not match the ground truth for a given input. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(863).png?raw=true)\r\n",
        "\r\n",
        ">The training process starts by calculating the error E between the predicted values and the ground truth labels. This error now represents the cost or the loss function that we discussed in the gradient descent video. Therefore, the next step would be to propagate this error back into the network and use it to perform gradient descent on the different weights and biases in the network, to optimize them using the same equations that we saw in the gradient descent video.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(864).png?raw=true) \r\n",
        "\r\n",
        "\r\n",
        ">So for our simple network with only two neurons and that takes one input, we calculate the squared error between the ground truth T and the predicted value a2. This represents our cost or loss function. Of course normally you don't train your network using one data point but thousands and thousands of data points. Therefore, the error is computed as the mean squared error calculated as follows. Then, we use the error to update w2, b2, w1, and b1 by propagating the error back into the network.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(867).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">For updating w2, we know from our gradient descent video that we need to use this equation to do that, but since the error is not explicitly a function of w2, then we will need to use the chain rule to establish the derivative of the error with respect to w2. So we know that E is a function of a2, and we know that a2 is a function of z2, and z2 is a function of w2. Therefore, we can take the derivative of E with respect to a2, and take the derivative of a2 with respect to z2, and take the derivative of z2 with respect to w2. Then the derivative of the error with respect to w2 would be simply the product of these individual derivatives, like so. The derivative of E with respect to a2 is -(T - a2) and the derivative of a2 with respect to z2 is a2(1- a2) and the derivative of z2 with respect to w2 is just the input a1. Therefore, w2 is updated as per the following equation. I'm assuming that you a're familiar with differentiation, so I am going to skip showing you how to find the derivatives, but below the video you will find a document that I prepared showing you in details how to find each of the derivatives shown here. Updating b2 is exactly the same. The only difference is the derivative of z2 with respect to b2 is 1 and not the input a1. Similarly, we use this expression to update w1, but the error is not explicitly related to w1. So we again use the chain rule to establish the derivative of the error with respect to w1. We know that E is a function of a2, and we know that a2 is a function of z2 and z2 is a function of a1, and a1 is a function of z1, and z1 is a function of w1. Therefore, we can take the derivative of E with respect to a2, and take the derivative of a2 with respect to z2, and take the derivative of z2 with respect to a1, and take the derivative of a1 with respect to z1, and then finally take the derivative of z1 with respect to w1. Then the derivative of the error with respect to w1 would simply be the product of these individual derivatives, like so. The derivative of E with respect to a2 is -(T - a2). The derivative of a2 with respect to z2 is a2(1 - a2), and the derivative of z2 with respect to a1 is w2, and the derivative of a1 with respect to z1 is a1(1 - a1), and the derivative of z1 with respect to w1 is the input x1. Therefore, w1 is updated as per the following equation. Using the same approach we figure out the expression to update b1.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_001.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_002.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_003.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_004.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(868).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(869).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(870).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(871).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(872).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(873).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Now let's apply back propagation to our example from the forward propagation video. Recall that we have a network of two neurons, with the initial values of the weights and the biases as shown. We apply forward propagation and we calculate the value of z1, which we found to be 0.415, and the value of a1, which we found to be 0.6023, the value of z2 which we found to be 0.9210, and the value of a2 which we found to be 0.7153. This is the predicted value by the network for an input of 0.1. Now let's assume that the ground truth is 0.25. We first compute the error between the ground truth and the predicted value. Next, we will start updating the weights and biases for either a predefined number of iterations or epochs, like 1,000 for example, or until the error is within a predefined threshold of 0.001 for example. Using a learning rate of 0.4, let's see how the weights and the biases change after the first iteration.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(874.png?raw=true)\r\n",
        "\r\n",
        ">We already know the expression to update w2, so we simply plug in the values of T, a2, and a1 to calculate the gradient. The gradient of the error with respect to w2 is 0.05706, and therefore, w2 gets updated to 0.427 We repeat the same thing for b2. We plug in the values of T and a 2 to calculate the gradient. The gradient of the error with respect to b2 is 0.0948, and therefore, b2 gets updated to 0.612. Similarly, for w1, we plug in the values of T, a2, and w2, a1, and x1 to calculate the gradient. The gradient of the error with respect to w1 is 0.001021, and therefore, w1 gets updated to 0.1496. As for b1, the gradient of the error with respect to b1 is 0.01021, and therefore b1 gets updated to 0.3959. This completes the first iteration or epoch of the training process. With the updated weights and biases, we do another round of forward propagation calculate the predicted value and compare it to the ground truth. Calculate the error and do another round of backpropagation \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(875).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(876).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(877).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(878).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Therefore, to summarize the training algorithm: first, we initialize the weights and biases to random values. Then, we iteratively repeat the following steps: 1. Calculate the network output using forward propagation. 2. Calculate the error between the ground truth and the estimated or predicted output of the network. 3. Update the weights and the biases through backpropagation. Repeat the above three steps until the number of iterations or epochs is reached or the error between the ground truth and the predicted output is below a predefined threshold. In the next video, we will continue our discussion of the backpropagation algorithm and point out a serious shortcoming of the sigmoid function when used as the activation function in the hidden layers of deep networks.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(879).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6a1ILF1tqPS"
      },
      "source": [
        "###Vanishing Gradient\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4trvFuCtsiR"
      },
      "source": [
        ">In this video, we will discuss a problem with the sigmoid activation function that prevented neural networks from booming sooner. This problem is the vanishing gradient problem. Recall from the previous video, with a very simple network of two neurons only, the derivatives of the error with respect to the weights were as follows. See how small the gradients are, but more importantly, how small the gradient of the error with respect to w1. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(880).png?raw=true)\r\n",
        "\r\n",
        ">Well it turns out that because we are using the sigmoid function as the activation function, then all the intermediate values in the network are between 0 and 1. So when we do backpropagation, we keep multiplying factors that are less than 1 by each other, and so their gradients tend to get smaller and smaller as we keep on moving backward in the network. This means that the neurons in the earlier layers learn very slowly as compared to the neurons in the later layers in the network. The earlier layers in the network are the slowest to train. The result is a training process that takes too long and a prediction accuracy that is compromised. Accordingly, this is the reason why we do not use the sigmoid function or similar functions as activation functions, since they are prone to the vanishing gradient problem. Therefore, in the next video we will learn about other activation functions that became so popular and are now the activation functions that get used almost all the time in the hidden layers, since they help in overcoming the vanishing gradient problem.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(881).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPRMXjMUu2Kl"
      },
      "source": [
        "###Activation Functions\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbhK1rNvUka"
      },
      "source": [
        ">As we discussed earlier, activation functions play a major role in the learning process of a neural network. So far, we have used only the sigmoid function as the activation function in our networks, but we saw how the sigmoid function has its shortcomings since it can lead to the vanishing gradient problem for the earlier layers. In this video, we will discuss other activation functions; ones that are more efficient to use and are more applicable to deep learning applications. There are seven types of activation functions that you can use when building a neural network. There is the binary step function, the linear or identity function, there is our old friend the sigmoid or logistic function, there is the hyperbolic tangent, or tanh, function, the rectified linear unit (ReLU) function, the leaky ReLU function, and the softmax function. In this video, we will discuss the popular ones, which are the sigmoid, the hyperbolic tangent, ReLU, and the softmax functions.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(882).png?raw=true)\r\n",
        "\r\n",
        ">This is the sigmoid function. At z = 0, a is equal to 0.5 and when z is a very large positive number, a is close to 1, and when z is a very large negative number, a is close to zero. Sigmoid functions used to be widely used as activation functions in the hidden layers of a neural network. However, as you can see, the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region, the gradients become very small. This results in the vanishing gradient problem that we discussed, and as the gradients approach 0, the network doesn't really learn. Another problem with the sigmoid function is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin. The values received are all positive. Well, not all the times would we desire that values going to the next neuron be all of the same sign. This can be addressed by scaling the sigmoid function, and this brings us to the next activation function: the hyperbolic tangent function. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(883).png?raw=true)\r\n",
        "\r\n",
        ">This is the hyperbolic tangent, or tanh, function. It is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function, but unlike the sigmoid function, it's symmetric over the origin. It ranges from -1 to +1. However, although it overcomes the lack of symmetry of the sigmoid function, it also leads to the vanishing gradient problem in very deep neural networks.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(884).png?raw=true)\r\n",
        "\r\n",
        ">The rectified linear unit, or ReLU, function is the most widely used activation function when designing networks today. In addition to it being nonlinear, the main advantage of using the ReLU, function over the other activation functions is that it does not activate all the neurons at the same time. According to the plot here, if the input is negative it will be converted to 0, and the neuron does not get activated. This means that at a time, only a few neurons are activated, making the network sparse and very efficient. Also, the ReLU function was one of the main advancements in the field of deep learning that led to overcoming the vanishing gradient problem. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(885).png?raw=true)\r\n",
        "\r\n",
        ">One last activation function that we will discuss here is the softmax function. The softmax function is also a type of a sigmoid function, but it is handy when we are trying to handle classification problems. The softmax function is ideally used in the output layer of the classifier where we are actually trying to get the probabilities to define the class of each input. So, if a network with 3 neurons in the output layer outputs [1.6, 0.55, 0.98] then with a softmax activation function, the outputs get converted to [0.51, 0.18, 0.31]. This way, it is easier for us to classify a given data point and determine to which category it belongs.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(886).png?raw=true)\r\n",
        "\r\n",
        ">In conclusion, the sigmoid and the tanh functions are avoided in many applications nowadays since they can lead to the vanishing gradient problem. The ReLU function is the function that's widely used nowadays, and it's important to note that it is only used in the hidden layers. Finally, when building a model, you can begin with using the ReLU function and then you can switch to other activation functions if the ReLU function does not yield a good performance. And this concludes this video on activation functions. I'll see you in the next video.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(887).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtBrlmewfdhy"
      },
      "source": [
        "##Keras and Deep Learning Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUidctwlk6is"
      },
      "source": [
        "###Deep Learning Libraries\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-T9t7QGk6-h"
      },
      "source": [
        ">Before we can start building deep learning networks, we will spend some time learning about the different deep learning libraries and frameworks that are out there. In this video, I will briefly cover the libraries that we'll be teaching in this specialization. The most popular library is in descending order are TensorFlow, Keras, and PyTorch. There is also Theano, a library developed by the Montreal Institute for Learning Algorithms, and was the major library for deep learning development even before TensorFlow and PyTorch. However, the founders can't afford to continuously support it and maintain it, and therefore, the library lost its popularity. Because of that, in this specialization, we will focus on the three other popular libraries.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(888).png?raw=true)\r\n",
        "\r\n",
        ">Among the three libraries. TensorFlow is the most popular one. It is the library that is mostly used in production of deep learning models. It has a very large community. Just a quick look at the number of forks on the library's Github repository as well as the number of commits and pull requests should suffice in giving you an idea of how popular the library is. Tensorflow was developed by Google and released to the public in 2015, and is still being actively used at Google for both research and production needs. PyTorch on the other hand, is the cousin of the Torch framework, which is in Lua, and supports machine learning algorithms running on GPUs in particular. However being derived from the Torch framework, PyTorch isn't just a set of wrappers to support a popular language like Python. Tt was actually rewritten and tailored to be fast and feel native. PyTorch was released in 2016 and has gained immense interest lately and is becoming the preferred language over TensorFlow, especially in academic research settings and applications of deep learning requiring optimizing custom expressions. PyTorch is supported and being actively used at Facebook. However, despite their popularity, both PyTorch and TensorFlow are not easy to use, and have a steep learning curve. So for people who are just starting to learn deep learning, there is no better library to use other than the Keras library. Keras is a high level API for building deep learning models. It has gained favor for its ease of use and syntactic simplicity facilitating fast development. As you'll see in the next couple of videos, building a very complex deep learning network can be achieved with Keras with only few lines of code. Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Kares library, you will have to install TensorFlow first, and when you import Keras, it will be explicitly displayed what backend was used to install the Keras library. Keras is also supported by Google. I won't go into more details about the different libraries, but the take home message here is if you're interested in building something quickly go with the Keras library; you won't be disappointed. However, if you want to have more control over the different nodes and layers in the network, and want to watch closely what happens with the network over time, then PyTorch or TensorFlow would be the right library. It will really boil down to your personal preference. With that, in the next videos, we will start learning how to use the Keras library to build models for regression and classification problems.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(889).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1KVjUWl2RJ"
      },
      "source": [
        "###Regression Models with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4PRFRft0cxl"
      },
      "source": [
        ">Let's take a look at a regression example. Here is a data set of the compressive strength of different samples of concrete based on the volumes of the different materials that were used to make them. So the first concrete sample has 540 cubic meter of cement, 0 cubic meter of blast furnace slag, 0 cubic meter of fly ash, 162 cubic meter of water, 2.5 cubic meter of superplasticizer, 1040 cubic meter of coarse aggregate, and 676 cubic meter of fine aggregate. Such a concrete mix which is 28 days old has a compressive strength of 79.99 MPa. The data is in a pandas dataframe and named concrete_data. So let's say we would like to use the Keras library to quickly build a deep neural network to model this dataset, and so we can automatically determine the compressive strength of a given concrete sample based on its ingredients.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(890).png?raw=true)\r\n",
        "\r\n",
        ">So let's say that the deep neural network that we would like to create takes all the eight features as input, feeds them into a hidden layer of five nodes, which is connected to another hidden layer of five nodes, which is then connected to the output layer with one node that is supposed to output the compressive strength of a given concrete sample. Note that usually you would go with a much larger number of neurons in each hidden layer like 50 or even 100, but we're just using a small network for simplicity. Notice how all the nodes in one layer are connected to all the other nodes in the next layer. Such a network is called a dense network.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(891).png?raw=true)\r\n",
        "\r\n",
        ">Before we begin using the Keras library, let's prepare our data and have it in the right format. The only thing we would need to do is to split the dataframe into two dataframes, one that has the predictors columns and another one that has the target column. We will also name them predictors and target. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(893).png?raw=true)\r\n",
        "\r\n",
        ">Now prepare to see the magic of Keras and how building such a network and training it and using it to predict new samples can be achieved with only few lines of code. The first thing you will need to do is import Keras and the Sequential model from \"keras.models\". Because our network consists of a linear stack of layers, then the Sequential model is what you would want to use. This is the case most of the time unless you are building something out of the ordinary. There are two models in the Keras library. One of them is the Sequential model and the other one is the model class used with the functional API. So to create your model, you simply call the Sequential constructor. Now building your layers is pretty straightforward as well. For that, we would need to import the \"Dense\" type of layers from \"keras.layers\". Then we use the add method to add each dense layer. We specify the number of neurons in each layer and the activation function that we want to use. As per our discussion in the video on activation functions, ReLU is one of the recommended activation functions for hidden layers, so we will use that. And for the first hidden layer we need to pass in the \"input_shape\" parameter, which is the number of columns or predictors in our dataset. Then we repeat the same thing for the other hidden layer, of course without the input_shape parameter, and we create our output layer with one node. Now for training, we need to define an optimizer and the error metric. In the previous module, we used gradient descent as our minimization or optimization algorithm, and the mean squared error as our loss measure between the predicted value and the ground truth. So we will stick with that and use the mean squared error as our loss measure. As for the minimization algorithm, there are actually other more efficient algorithms than the gradient descent for deep learning applications. One of them is \"adam\". One of the main advantages of the \"adam\" optimizer is that you don't need to specify the learning rate that we saw in the gradient descent video. So this saves us the task of optimizing the learning rate for our model. Then we use the fit method to train our model. Once training is complete, we can start making predictions using the predict method.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(894).png?raw=true)\r\n",
        "\r\n",
        "- [Keras Activation Functions:](https://keras.io/activations/)\r\n",
        "\r\n",
        "- [Keras Models:](https://keras.io/models/about-keras-models/#about-keras-models)\r\n",
        "\r\n",
        "- [Keras Optimizers:](https://keras.io/optimizers/)\r\n",
        "\r\n",
        "- [Keras Metrics:](https://keras.io/metrics/)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ5s8mg43YMk"
      },
      "source": [
        "###Classification Models with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVbzwzrA39GW"
      },
      "source": [
        ">Let's say that we would like to build a model that would inform someone whether purchasing a certain car would be a good choice based on the price of the car, the cost to maintain it, and whether it can accommodate two or more people. So, here is a dataset that we are calling \"car_data\". I already cleaned the data, as you can see, where I used one-hot encoding to transform each category of price, maintenance, and how many people the car can accommodate, into separate columns. So the price of the car can be either high, medium, or low. Similarly, the cost of maintaining the car can also be high, medium, or low, and the car can either fit two people or more. If you take the first car in the dataset, it is considered an expensive car, has high maintenance cost, and can fit only two people. The decision is 0, meaning that buying this car would be a bad choice. A decision of 1 means that buying the car is acceptable, a decision of 2 means that buying the car would be a good decision, and a decision of 3 means that buying the car would be a very good decision.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(895).png?raw=true)\r\n",
        "\r\n",
        ">Let's use the same neural network as the one we used for the regression problem that we discussed in the previous video. So a network that still takes eight inputs or predictors, consists of two hidden layers, each of five neurons, and an output layer.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(896).png?raw=true)\r\n",
        "\r\n",
        ">Next, let's divide our dataset into predictors and target. However, with Keras, for classification problems, we can't use the target column as is; we actually need to transform the column into an array with binary values similar to one-hot encoding like the output shown here. We easily achieve that using the \"to_categorical\" function from the Keras utilities package. In other words, our model instead of having just one neuron in the output layer, it would have four neurons, since our target variable consists of four categories. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(897).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(898).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(899).png?raw=true)\r\n",
        "\r\n",
        ">In terms of code, the structure of our code is pretty similar to the one we use to build the model for our regression problem. We start by importing the Keras library and the Sequential model and we use it to construct our model. We also import the \"Dense\" layer since we will be using it to build our network. The additional import statement here is the \"to_categorical\" function in order to transform our target column into an array of binary numbers for classification. Then, we proceed to constructing our layers. We use the add method to create two hidden layers, each with five neurons and the neurons are activated using the ReLU activation function. Notice how here we also specify the softmax function as the activation function for the output layer, so that the sum of the predicted values from all the neurons in the output layer sum nicely to 1. Then in defining our compiler, here we will use the categorical cross-entropy as our loss measure instead of the mean squared error that we use for regression, and we will specify the evaluation metric to be \"accuracy\". \"accuracy\" is a built-in evaluation metric in Keras but you can actually define your own evaluation metric and pass it in the metrics parameter. Then we fit the model. Notice how this time we're specifying the number of epochs for training the model. Although we didn't specify the number of epochs when we built a regression model, but we could have done that. Finally, we use the predict method to make predictions. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(900).png?raw=true)\r\n",
        "\r\n",
        ">Now the output of the Keras predict method would be something like what's shown here. For each data point, the output is the probability that the decision of purchasing a given car belongs to one of the four classes. For each data point, the probabilities should sum to 1, and the higher the probability the more confident is the algorithm that a datapoint belongs to the respective class. So for the first data point or the first car in the test set, the decision would be 0 meaning not acceptable, since the first probability is the highest, with a value of 0.99 or close to 1, in this case. Similarly, for the second datapoint, the decision is also 0 or not acceptable, since the probability for this class is the highest, again with a value of 0.99 or almost 1. For the first three datapoints, the model is very confident that purchasing these cars is not acceptable. As for the last three datapoints, the decision would be 1 or acceptable, since the probabilities for the second class are higher than the rest of the classes. But notice how the probabilities for decision 0 and decision 1 are very close. Therefore, the model is not very confident but it would lean towards accepting purchasing these cars.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(901).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFgL3tI499yQ"
      },
      "source": [
        "###LAB: Regression with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "yDkDCkVj90gQ"
      },
      "source": [
        "#### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "43Z_lsim90gS"
      },
      "source": [
        "As we discussed in the videos, despite the popularity of more powerful libraries such as PyToch and TensorFlow, they are not easy to use and have a steep learning curve. So, for people who are just starting to learn deep learning, there is no better library to use other than the Keras library. \n",
        "\n",
        "Keras is a high-level API for building deep learning models. It has gained favor for its ease of use and syntactic simplicity facilitating fast development. As you will see in this lab and the other labs in this course, building a very complex deep learning network can be achieved with Keras with only few lines of code. You will appreciate Keras even more, once you learn how to build deep models using PyTorch and TensorFlow in the other courses.\n",
        "\n",
        "So, in this lab, you will learn how to use the Keras library to build a regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abyLLOKm90gT"
      },
      "source": [
        "<h2>Regression Models with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. How to use the Keras library to build a regression model.</h5>\n",
        "<h5> 2. Download and Clean dataset </h5>\n",
        "<h5> 3. Build a Neural Network </h5>\n",
        "<h5> 4. Train and Test the Network. </h5>     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "10Mgly6890gT"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item31\">Download and Clean Dataset</a>  \n",
        "2. <a href=\"#item32\">Import Keras</a>  \n",
        "3. <a href=\"#item33\">Build a Neural Network</a>  \n",
        "4. <a href=\"#item34\">Train and Test the Network</a>  \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "h192U57P90gU"
      },
      "source": [
        "<a id=\"item31\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "tvKThLtP90gU"
      },
      "source": [
        "#### Download and Clean Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "qX23BDac90gU"
      },
      "source": [
        "Let's start by importing the <em>pandas</em> and the Numpy libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "FJYAWVaH90gV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "HH4M4nmJ90gV"
      },
      "source": [
        "We will be playing around with the same dataset that we used in the videos.\n",
        "\n",
        "<strong>The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:</strong>\n",
        "\n",
        "<strong>1. Cement</strong>\n",
        "\n",
        "<strong>2. Blast Furnace Slag</strong>\n",
        "\n",
        "<strong>3. Fly Ash</strong>\n",
        "\n",
        "<strong>4. Water</strong>\n",
        "\n",
        "<strong>5. Superplasticizer</strong>\n",
        "\n",
        "<strong>6. Coarse Aggregate</strong>\n",
        "\n",
        "<strong>7. Fine Aggregate</strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Cf7zBah490gW"
      },
      "source": [
        "Let's download the data and read it into a <em>pandas</em> dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYFOrCgD99h5"
      },
      "source": [
        "!wget -q https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Gdn4JmwPmEv6",
        "outputId": "5577fa8c-b647-4a22-992b-b0afc9b4a01c"
      },
      "source": [
        "df = pd.read_csv(\"concrete_data.csv\")\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQD40BhD_SvA",
        "outputId": "0fc7b7d4-b2d5-4f1d-9765-cb4809f53a85"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1030 entries, 0 to 1029\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Cement              1030 non-null   float64\n",
            " 1   Blast Furnace Slag  1030 non-null   float64\n",
            " 2   Fly Ash             1030 non-null   float64\n",
            " 3   Water               1030 non-null   float64\n",
            " 4   Superplasticizer    1030 non-null   float64\n",
            " 5   Coarse Aggregate    1030 non-null   float64\n",
            " 6   Fine Aggregate      1030 non-null   float64\n",
            " 7   Age                 1030 non-null   int64  \n",
            " 8   Strength            1030 non-null   float64\n",
            "dtypes: float64(8), int64(1)\n",
            "memory usage: 72.5 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFngEcP1_UYg",
        "outputId": "d9a774af-406d-411b-a55e-b1ca7b26bee5"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cement                0\n",
              "Blast Furnace Slag    0\n",
              "Fly Ash               0\n",
              "Water                 0\n",
              "Superplasticizer      0\n",
              "Coarse Aggregate      0\n",
              "Fine Aggregate        0\n",
              "Age                   0\n",
              "Strength              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "L4zkY8Me_i3-",
        "outputId": "3206a887-7391-4bae-c30c-aee41f8cbcda"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Y7xLOP5J90gY"
      },
      "source": [
        "The data looks very clean and is ready to be used to build our model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "_nTonJIl90gY"
      },
      "source": [
        "##### Split data into predictors and target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAAKBqXW90gZ"
      },
      "source": [
        "The target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQYPjBIv_wKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c562f38e-bb2e-4874-b188-74ef9f5074c7"
      },
      "source": [
        "df_columns = df.columns\r\n",
        "df_columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',\n",
              "       'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "AdBgD4xPIC8v",
        "outputId": "63ba0664-3261-4e32-9d37-a4ef0e77fb8a"
      },
      "source": [
        "x = df.loc[:,df.columns!='Strength']\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>276.4</td>\n",
              "      <td>116.0</td>\n",
              "      <td>90.3</td>\n",
              "      <td>179.6</td>\n",
              "      <td>8.9</td>\n",
              "      <td>870.1</td>\n",
              "      <td>768.3</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>322.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.6</td>\n",
              "      <td>196.0</td>\n",
              "      <td>10.4</td>\n",
              "      <td>817.9</td>\n",
              "      <td>813.4</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>148.5</td>\n",
              "      <td>139.4</td>\n",
              "      <td>108.6</td>\n",
              "      <td>192.7</td>\n",
              "      <td>6.1</td>\n",
              "      <td>892.4</td>\n",
              "      <td>780.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>159.1</td>\n",
              "      <td>186.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>175.6</td>\n",
              "      <td>11.3</td>\n",
              "      <td>989.6</td>\n",
              "      <td>788.9</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>260.9</td>\n",
              "      <td>100.5</td>\n",
              "      <td>78.3</td>\n",
              "      <td>200.6</td>\n",
              "      <td>8.6</td>\n",
              "      <td>864.5</td>\n",
              "      <td>761.5</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows  8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Cement  Blast Furnace Slag  ...  Fine Aggregate  Age\n",
              "0      540.0                 0.0  ...           676.0   28\n",
              "1      540.0                 0.0  ...           676.0   28\n",
              "2      332.5               142.5  ...           594.0  270\n",
              "3      332.5               142.5  ...           594.0  365\n",
              "4      198.6               132.4  ...           825.5  360\n",
              "...      ...                 ...  ...             ...  ...\n",
              "1025   276.4               116.0  ...           768.3   28\n",
              "1026   322.2                 0.0  ...           813.4   28\n",
              "1027   148.5               139.4  ...           780.0   28\n",
              "1028   159.1               186.7  ...           788.9   28\n",
              "1029   260.9               100.5  ...           761.5   28\n",
              "\n",
              "[1030 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skGhXmuDIp4B",
        "outputId": "2a1698c4-2cf8-4af4-f328-f5782ea73708"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1030, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t_Sta-8Ieoa"
      },
      "source": [
        "y = df['Strength']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "7bD08RvK90ga"
      },
      "source": [
        "Finally, the last step is to normalize the data by substracting the mean and dividing by the standard deviation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Q9rMo1ugIj4M",
        "outputId": "108f3c1f-ad13-4291-b1d2-49c7c7bcf0e3"
      },
      "source": [
        "x_norm = (x-x.mean())/x.std()\r\n",
        "x_norm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>0.862735</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>-0.279597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>1.055651</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>-0.279597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>3.551340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>5.055221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.790075</td>\n",
              "      <td>0.678079</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>0.488555</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>0.070492</td>\n",
              "      <td>0.647569</td>\n",
              "      <td>4.976069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Cement  Blast Furnace Slag  ...  Fine Aggregate       Age\n",
              "0  2.476712           -0.856472  ...       -1.217079 -0.279597\n",
              "1  2.476712           -0.856472  ...       -1.217079 -0.279597\n",
              "2  0.491187            0.795140  ...       -2.239829  3.551340\n",
              "3  0.491187            0.795140  ...       -2.239829  5.055221\n",
              "4 -0.790075            0.678079  ...        0.647569  4.976069\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPAGAoSWJEGB",
        "outputId": "47d05be5-05d6-4c1c-f255-252fbab8b4e8"
      },
      "source": [
        "#no of predictors\r\n",
        "n_cols = len(x_norm.columns)\r\n",
        "n_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WL6MFCTM90gb"
      },
      "source": [
        "#### Import Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "qdeqJRT_90gc"
      },
      "source": [
        "Recall from the videos that Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Keras library, you will have to install TensorFlow first and when you import the Keras library, it will be explicitly displayed what backend was used to install the Keras library. In CC Labs, we used TensorFlow as the backend to install Keras, so it should clearly print that when we import Keras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "jNYd7q-190gc"
      },
      "source": [
        "##### Let's go ahead and import the Keras library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GNyL-mnJuzq"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "gpbtoG3990gc"
      },
      "source": [
        "As you can see, the TensorFlow backend was used to install the Keras library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "9PXaJr6390gc"
      },
      "source": [
        "Let's import the rest of the packages from the Keras library that we will need to build our regressoin model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBMyBRvJ619"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "lEQz14yo90gd"
      },
      "source": [
        "#### Build a Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "nR37vsKJ90gd"
      },
      "source": [
        "Let's define a function that defines our regression model for us so that we can conveniently call it to create our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg1DVA8WKLfa"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  #create model\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Dense(50,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(50,activation=\"relu\"))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model...specify optimisation function and cost function\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDVzIw5F90ge"
      },
      "source": [
        "The above function create a model that has two hidden layers, each of 50 hidden units.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "C2DPyH0k90gf"
      },
      "source": [
        "#### Train and Test the Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqsKW2kQ90gg"
      },
      "source": [
        "Let's call the function now to create our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nSbXpEXLj9m"
      },
      "source": [
        "model = regression_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoH2K4pH90gg"
      },
      "source": [
        "Next, we will train and test the model at the same time using the _fit_ method. We will leave out 30% of the data for validation and we will train the model for 100 epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tq_3dNzOcRE"
      },
      "source": [
        ">**Verbose:**   \r\n",
        "By default verbose = 1,\r\n",
        "\r\n",
        "- verbose = 1, which includes both progress bar and one line per epoch\r\n",
        "\r\n",
        "- verbose = 0, means silent\r\n",
        "\r\n",
        "- verbose = 2, one line per epoch i.e. epoch no./total no. of epochs\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ">validation_split:  \r\n",
        " Float between 0 and 1. The fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling.\r\n",
        "Cross-validation data is used to investigate whether your model over-fits the data or does not. This is what we can understand whether our model has generalization capability or not.\r\n",
        "\r\n",
        ">validation_data:   \r\n",
        " tuple (x_val, y_val) or tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This will override validation_split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DwYwMIWLq2-",
        "outputId": "8eae83f0-4613-448e-df5f-b579dbd4b6c3"
      },
      "source": [
        "#fit the model\r\n",
        "model.fit(x_norm,y,validation_split=0.3,epochs=100,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/23 - 0s - loss: 17.1790 - val_loss: 117.1033\n",
            "Epoch 2/100\n",
            "23/23 - 0s - loss: 17.0617 - val_loss: 123.3737\n",
            "Epoch 3/100\n",
            "23/23 - 0s - loss: 16.6405 - val_loss: 124.2634\n",
            "Epoch 4/100\n",
            "23/23 - 0s - loss: 16.6252 - val_loss: 128.3106\n",
            "Epoch 5/100\n",
            "23/23 - 0s - loss: 16.5083 - val_loss: 118.9804\n",
            "Epoch 6/100\n",
            "23/23 - 0s - loss: 16.3251 - val_loss: 123.0360\n",
            "Epoch 7/100\n",
            "23/23 - 0s - loss: 16.3410 - val_loss: 121.8743\n",
            "Epoch 8/100\n",
            "23/23 - 0s - loss: 16.4991 - val_loss: 121.0805\n",
            "Epoch 9/100\n",
            "23/23 - 0s - loss: 16.7510 - val_loss: 121.8111\n",
            "Epoch 10/100\n",
            "23/23 - 0s - loss: 16.5290 - val_loss: 125.5427\n",
            "Epoch 11/100\n",
            "23/23 - 0s - loss: 16.1788 - val_loss: 127.1908\n",
            "Epoch 12/100\n",
            "23/23 - 0s - loss: 16.4590 - val_loss: 120.8689\n",
            "Epoch 13/100\n",
            "23/23 - 0s - loss: 16.1648 - val_loss: 119.2210\n",
            "Epoch 14/100\n",
            "23/23 - 0s - loss: 15.9418 - val_loss: 116.8745\n",
            "Epoch 15/100\n",
            "23/23 - 0s - loss: 15.8657 - val_loss: 115.3904\n",
            "Epoch 16/100\n",
            "23/23 - 0s - loss: 16.0210 - val_loss: 130.2403\n",
            "Epoch 17/100\n",
            "23/23 - 0s - loss: 16.2172 - val_loss: 127.5238\n",
            "Epoch 18/100\n",
            "23/23 - 0s - loss: 16.4940 - val_loss: 130.9841\n",
            "Epoch 19/100\n",
            "23/23 - 0s - loss: 15.9912 - val_loss: 125.9242\n",
            "Epoch 20/100\n",
            "23/23 - 0s - loss: 16.3901 - val_loss: 128.6080\n",
            "Epoch 21/100\n",
            "23/23 - 0s - loss: 15.8496 - val_loss: 125.3561\n",
            "Epoch 22/100\n",
            "23/23 - 0s - loss: 15.8237 - val_loss: 116.0531\n",
            "Epoch 23/100\n",
            "23/23 - 0s - loss: 15.5758 - val_loss: 120.9095\n",
            "Epoch 24/100\n",
            "23/23 - 0s - loss: 15.2481 - val_loss: 116.7772\n",
            "Epoch 25/100\n",
            "23/23 - 0s - loss: 15.6061 - val_loss: 124.6870\n",
            "Epoch 26/100\n",
            "23/23 - 0s - loss: 15.7026 - val_loss: 116.8156\n",
            "Epoch 27/100\n",
            "23/23 - 0s - loss: 15.7084 - val_loss: 111.6947\n",
            "Epoch 28/100\n",
            "23/23 - 0s - loss: 15.2827 - val_loss: 111.2006\n",
            "Epoch 29/100\n",
            "23/23 - 0s - loss: 15.1580 - val_loss: 113.4407\n",
            "Epoch 30/100\n",
            "23/23 - 0s - loss: 15.4221 - val_loss: 112.4613\n",
            "Epoch 31/100\n",
            "23/23 - 0s - loss: 14.9935 - val_loss: 120.9651\n",
            "Epoch 32/100\n",
            "23/23 - 0s - loss: 15.0817 - val_loss: 120.4396\n",
            "Epoch 33/100\n",
            "23/23 - 0s - loss: 14.9941 - val_loss: 108.4624\n",
            "Epoch 34/100\n",
            "23/23 - 0s - loss: 15.3270 - val_loss: 113.4323\n",
            "Epoch 35/100\n",
            "23/23 - 0s - loss: 16.2248 - val_loss: 117.2679\n",
            "Epoch 36/100\n",
            "23/23 - 0s - loss: 14.9389 - val_loss: 111.4484\n",
            "Epoch 37/100\n",
            "23/23 - 0s - loss: 14.7215 - val_loss: 111.6582\n",
            "Epoch 38/100\n",
            "23/23 - 0s - loss: 14.5297 - val_loss: 108.8954\n",
            "Epoch 39/100\n",
            "23/23 - 0s - loss: 14.8633 - val_loss: 111.2179\n",
            "Epoch 40/100\n",
            "23/23 - 0s - loss: 14.6935 - val_loss: 108.4619\n",
            "Epoch 41/100\n",
            "23/23 - 0s - loss: 14.8073 - val_loss: 99.6928\n",
            "Epoch 42/100\n",
            "23/23 - 0s - loss: 14.8939 - val_loss: 102.2421\n",
            "Epoch 43/100\n",
            "23/23 - 0s - loss: 14.7514 - val_loss: 105.9978\n",
            "Epoch 44/100\n",
            "23/23 - 0s - loss: 14.8169 - val_loss: 107.3738\n",
            "Epoch 45/100\n",
            "23/23 - 0s - loss: 14.5588 - val_loss: 107.3471\n",
            "Epoch 46/100\n",
            "23/23 - 0s - loss: 14.3578 - val_loss: 107.7220\n",
            "Epoch 47/100\n",
            "23/23 - 0s - loss: 14.5230 - val_loss: 108.7005\n",
            "Epoch 48/100\n",
            "23/23 - 0s - loss: 14.6552 - val_loss: 109.0493\n",
            "Epoch 49/100\n",
            "23/23 - 0s - loss: 14.5409 - val_loss: 102.6309\n",
            "Epoch 50/100\n",
            "23/23 - 0s - loss: 14.5692 - val_loss: 104.9035\n",
            "Epoch 51/100\n",
            "23/23 - 0s - loss: 14.8700 - val_loss: 112.3632\n",
            "Epoch 52/100\n",
            "23/23 - 0s - loss: 14.5734 - val_loss: 104.3658\n",
            "Epoch 53/100\n",
            "23/23 - 0s - loss: 14.6500 - val_loss: 111.6282\n",
            "Epoch 54/100\n",
            "23/23 - 0s - loss: 14.9577 - val_loss: 106.9401\n",
            "Epoch 55/100\n",
            "23/23 - 0s - loss: 14.4394 - val_loss: 106.2872\n",
            "Epoch 56/100\n",
            "23/23 - 0s - loss: 14.1220 - val_loss: 106.7646\n",
            "Epoch 57/100\n",
            "23/23 - 0s - loss: 14.2255 - val_loss: 104.2864\n",
            "Epoch 58/100\n",
            "23/23 - 0s - loss: 14.2379 - val_loss: 102.4789\n",
            "Epoch 59/100\n",
            "23/23 - 0s - loss: 14.0016 - val_loss: 111.0648\n",
            "Epoch 60/100\n",
            "23/23 - 0s - loss: 13.9787 - val_loss: 102.7890\n",
            "Epoch 61/100\n",
            "23/23 - 0s - loss: 14.0723 - val_loss: 109.4303\n",
            "Epoch 62/100\n",
            "23/23 - 0s - loss: 14.0656 - val_loss: 102.0480\n",
            "Epoch 63/100\n",
            "23/23 - 0s - loss: 14.4911 - val_loss: 98.3831\n",
            "Epoch 64/100\n",
            "23/23 - 0s - loss: 14.2595 - val_loss: 102.0425\n",
            "Epoch 65/100\n",
            "23/23 - 0s - loss: 13.8506 - val_loss: 104.2471\n",
            "Epoch 66/100\n",
            "23/23 - 0s - loss: 13.9719 - val_loss: 112.1980\n",
            "Epoch 67/100\n",
            "23/23 - 0s - loss: 14.4314 - val_loss: 98.9371\n",
            "Epoch 68/100\n",
            "23/23 - 0s - loss: 14.4488 - val_loss: 101.5803\n",
            "Epoch 69/100\n",
            "23/23 - 0s - loss: 13.6752 - val_loss: 105.4426\n",
            "Epoch 70/100\n",
            "23/23 - 0s - loss: 13.7898 - val_loss: 102.2314\n",
            "Epoch 71/100\n",
            "23/23 - 0s - loss: 13.7958 - val_loss: 110.8073\n",
            "Epoch 72/100\n",
            "23/23 - 0s - loss: 13.7249 - val_loss: 107.8474\n",
            "Epoch 73/100\n",
            "23/23 - 0s - loss: 13.7167 - val_loss: 101.1368\n",
            "Epoch 74/100\n",
            "23/23 - 0s - loss: 13.7036 - val_loss: 102.1868\n",
            "Epoch 75/100\n",
            "23/23 - 0s - loss: 13.9018 - val_loss: 109.7909\n",
            "Epoch 76/100\n",
            "23/23 - 0s - loss: 13.4595 - val_loss: 114.2139\n",
            "Epoch 77/100\n",
            "23/23 - 0s - loss: 14.0384 - val_loss: 97.9043\n",
            "Epoch 78/100\n",
            "23/23 - 0s - loss: 13.7346 - val_loss: 102.2385\n",
            "Epoch 79/100\n",
            "23/23 - 0s - loss: 13.3726 - val_loss: 111.4213\n",
            "Epoch 80/100\n",
            "23/23 - 0s - loss: 13.3669 - val_loss: 99.2604\n",
            "Epoch 81/100\n",
            "23/23 - 0s - loss: 13.3021 - val_loss: 100.2064\n",
            "Epoch 82/100\n",
            "23/23 - 0s - loss: 13.5376 - val_loss: 105.6957\n",
            "Epoch 83/100\n",
            "23/23 - 0s - loss: 13.5340 - val_loss: 112.5290\n",
            "Epoch 84/100\n",
            "23/23 - 0s - loss: 13.7215 - val_loss: 104.8863\n",
            "Epoch 85/100\n",
            "23/23 - 0s - loss: 13.0501 - val_loss: 96.1600\n",
            "Epoch 86/100\n",
            "23/23 - 0s - loss: 13.3682 - val_loss: 105.2716\n",
            "Epoch 87/100\n",
            "23/23 - 0s - loss: 13.0921 - val_loss: 107.5444\n",
            "Epoch 88/100\n",
            "23/23 - 0s - loss: 13.6332 - val_loss: 104.3077\n",
            "Epoch 89/100\n",
            "23/23 - 0s - loss: 13.2197 - val_loss: 102.3261\n",
            "Epoch 90/100\n",
            "23/23 - 0s - loss: 13.3651 - val_loss: 106.0234\n",
            "Epoch 91/100\n",
            "23/23 - 0s - loss: 13.1311 - val_loss: 105.2761\n",
            "Epoch 92/100\n",
            "23/23 - 0s - loss: 13.2952 - val_loss: 114.1159\n",
            "Epoch 93/100\n",
            "23/23 - 0s - loss: 13.8320 - val_loss: 101.8911\n",
            "Epoch 94/100\n",
            "23/23 - 0s - loss: 12.9144 - val_loss: 95.0624\n",
            "Epoch 95/100\n",
            "23/23 - 0s - loss: 12.9908 - val_loss: 100.4465\n",
            "Epoch 96/100\n",
            "23/23 - 0s - loss: 13.1125 - val_loss: 100.9772\n",
            "Epoch 97/100\n",
            "23/23 - 0s - loss: 12.7766 - val_loss: 101.8151\n",
            "Epoch 98/100\n",
            "23/23 - 0s - loss: 12.7020 - val_loss: 97.6959\n",
            "Epoch 99/100\n",
            "23/23 - 0s - loss: 12.8936 - val_loss: 103.4992\n",
            "Epoch 100/100\n",
            "23/23 - 0s - loss: 12.6581 - val_loss: 98.2987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fa6b74400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COkyNtxPTHq5"
      },
      "source": [
        "#prediction\r\n",
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "UzwluhU990gi"
      },
      "source": [
        "<strong>You can refer to this [link](https://keras.io/models/sequential?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ) to learn about other functions that you can use for prediction or evaluation.</strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "kBVFrIPh90gi"
      },
      "source": [
        "Feel free to vary the following and note what impact each change has on the model's performance:\n",
        "\n",
        "1.  Increase or decreate number of neurons in hidden layers\n",
        "2.  Add more hidden layers\n",
        "3.  Increase number of epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aD12D4yP8me"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H55nPJQAP1PW"
      },
      "source": [
        "def model1():\r\n",
        "  model =Sequential()\r\n",
        "  model.add(Dense(100,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux5PN1fAR6gV",
        "outputId": "f249c411-eff1-4cbe-b6bb-b13b44ba09f7"
      },
      "source": [
        "#fit the model\r\n",
        "model1.fit(x_norm,y,validation_split=0.3,epochs=150,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.3153 - val_loss: 130.2595\n",
            "Epoch 2/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 6.1643 - val_loss: 140.7603\n",
            "Epoch 3/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.6716 - val_loss: 130.8577\n",
            "Epoch 4/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.7173 - val_loss: 107.9076\n",
            "Epoch 5/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.8055 - val_loss: 141.0662\n",
            "Epoch 6/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.3645 - val_loss: 118.2120\n",
            "Epoch 7/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7443 - val_loss: 133.5740\n",
            "Epoch 8/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2848 - val_loss: 124.8207\n",
            "Epoch 9/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3678 - val_loss: 133.8175\n",
            "Epoch 10/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1305 - val_loss: 126.8651\n",
            "Epoch 11/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3748 - val_loss: 120.5652\n",
            "Epoch 12/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8116 - val_loss: 135.8880\n",
            "Epoch 13/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6967 - val_loss: 146.5878\n",
            "Epoch 14/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.0459 - val_loss: 116.4828\n",
            "Epoch 15/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.0329 - val_loss: 131.2587\n",
            "Epoch 16/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4325 - val_loss: 130.2632\n",
            "Epoch 17/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4131 - val_loss: 128.2400\n",
            "Epoch 18/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.6848 - val_loss: 130.7573\n",
            "Epoch 19/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.0672 - val_loss: 135.5246\n",
            "Epoch 20/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.2864 - val_loss: 129.5764\n",
            "Epoch 21/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3669 - val_loss: 124.2697\n",
            "Epoch 22/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6065 - val_loss: 148.9418\n",
            "Epoch 23/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2939 - val_loss: 128.2308\n",
            "Epoch 24/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1936 - val_loss: 122.0079\n",
            "Epoch 25/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.2415 - val_loss: 123.6696\n",
            "Epoch 26/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8225 - val_loss: 116.1043\n",
            "Epoch 27/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.9341 - val_loss: 148.0475\n",
            "Epoch 28/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6545 - val_loss: 103.5297\n",
            "Epoch 29/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.8916 - val_loss: 134.5279\n",
            "Epoch 30/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.5276 - val_loss: 132.8243\n",
            "Epoch 31/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7352 - val_loss: 126.6457\n",
            "Epoch 32/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8960 - val_loss: 118.1688\n",
            "Epoch 33/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3125 - val_loss: 126.2022\n",
            "Epoch 34/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2471 - val_loss: 126.3972\n",
            "Epoch 35/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.2976 - val_loss: 136.5154\n",
            "Epoch 36/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8764 - val_loss: 114.6979\n",
            "Epoch 37/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.4574 - val_loss: 132.2731\n",
            "Epoch 38/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.7071 - val_loss: 119.0576\n",
            "Epoch 39/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4239 - val_loss: 127.6899\n",
            "Epoch 40/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3566 - val_loss: 132.0519\n",
            "Epoch 41/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1197 - val_loss: 128.3598\n",
            "Epoch 42/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2663 - val_loss: 125.3497\n",
            "Epoch 43/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1038 - val_loss: 122.1437\n",
            "Epoch 44/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7358 - val_loss: 127.4946\n",
            "Epoch 45/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1528 - val_loss: 125.6672\n",
            "Epoch 46/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 6.9351 - val_loss: 117.9809\n",
            "Epoch 47/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8752 - val_loss: 125.0417\n",
            "Epoch 48/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7404 - val_loss: 120.3708\n",
            "Epoch 49/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0120 - val_loss: 127.0598\n",
            "Epoch 50/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7205 - val_loss: 121.6878\n",
            "Epoch 51/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.9282 - val_loss: 109.1075\n",
            "Epoch 52/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.3986 - val_loss: 135.4813\n",
            "Epoch 53/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.2694 - val_loss: 128.0431\n",
            "Epoch 54/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.6811 - val_loss: 120.8818\n",
            "Epoch 55/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1771 - val_loss: 119.6303\n",
            "Epoch 56/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8571 - val_loss: 138.8430\n",
            "Epoch 57/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4741 - val_loss: 118.6381\n",
            "Epoch 58/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8848 - val_loss: 129.9866\n",
            "Epoch 59/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7332 - val_loss: 127.0029\n",
            "Epoch 60/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.6924 - val_loss: 126.5820\n",
            "Epoch 61/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1239 - val_loss: 137.1362\n",
            "Epoch 62/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9895 - val_loss: 132.0979\n",
            "Epoch 63/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2343 - val_loss: 114.0145\n",
            "Epoch 64/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.0641 - val_loss: 135.2472\n",
            "Epoch 65/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.9327 - val_loss: 127.7070\n",
            "Epoch 66/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6531 - val_loss: 122.4300\n",
            "Epoch 67/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.0405 - val_loss: 126.1846\n",
            "Epoch 68/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7347 - val_loss: 130.3805\n",
            "Epoch 69/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4071 - val_loss: 125.6540\n",
            "Epoch 70/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.2523 - val_loss: 125.2134\n",
            "Epoch 71/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.7304 - val_loss: 124.5673\n",
            "Epoch 72/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1363 - val_loss: 125.5648\n",
            "Epoch 73/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5470 - val_loss: 118.5993\n",
            "Epoch 74/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1302 - val_loss: 128.8624\n",
            "Epoch 75/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.0861 - val_loss: 129.4858\n",
            "Epoch 76/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.5707 - val_loss: 141.7597\n",
            "Epoch 77/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6942 - val_loss: 123.5765\n",
            "Epoch 78/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3426 - val_loss: 113.0753\n",
            "Epoch 79/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2985 - val_loss: 132.0437\n",
            "Epoch 80/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.4330 - val_loss: 136.9445\n",
            "Epoch 81/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3370 - val_loss: 129.6009\n",
            "Epoch 82/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1273 - val_loss: 128.8665\n",
            "Epoch 83/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8530 - val_loss: 128.7555\n",
            "Epoch 84/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.3466 - val_loss: 106.7912\n",
            "Epoch 85/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.2230 - val_loss: 137.7563\n",
            "Epoch 86/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4503 - val_loss: 120.7586\n",
            "Epoch 87/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.7460 - val_loss: 134.6806\n",
            "Epoch 88/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3140 - val_loss: 125.5832\n",
            "Epoch 89/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.0659 - val_loss: 124.4767\n",
            "Epoch 90/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3433 - val_loss: 144.5072\n",
            "Epoch 91/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5291 - val_loss: 127.4905\n",
            "Epoch 92/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5818 - val_loss: 129.4477\n",
            "Epoch 93/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9407 - val_loss: 134.6449\n",
            "Epoch 94/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8364 - val_loss: 123.1944\n",
            "Epoch 95/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5375 - val_loss: 126.0829\n",
            "Epoch 96/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9970 - val_loss: 144.9804\n",
            "Epoch 97/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1120 - val_loss: 138.2728\n",
            "Epoch 98/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9818 - val_loss: 132.2569\n",
            "Epoch 99/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7187 - val_loss: 119.4980\n",
            "Epoch 100/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 4.9097 - val_loss: 134.0714\n",
            "Epoch 101/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9337 - val_loss: 142.6924\n",
            "Epoch 102/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9365 - val_loss: 122.7504\n",
            "Epoch 103/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1373 - val_loss: 139.8323\n",
            "Epoch 104/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0961 - val_loss: 141.1393\n",
            "Epoch 105/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3547 - val_loss: 119.3702\n",
            "Epoch 106/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.3429 - val_loss: 127.0359\n",
            "Epoch 107/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.9548 - val_loss: 127.8182\n",
            "Epoch 108/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5265 - val_loss: 125.9716\n",
            "Epoch 109/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8187 - val_loss: 124.9657\n",
            "Epoch 110/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9550 - val_loss: 125.1862\n",
            "Epoch 111/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1599 - val_loss: 122.1651\n",
            "Epoch 112/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5629 - val_loss: 139.7338\n",
            "Epoch 113/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2599 - val_loss: 143.0835\n",
            "Epoch 114/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.6132 - val_loss: 108.6727\n",
            "Epoch 115/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.9558 - val_loss: 115.3756\n",
            "Epoch 116/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.7965 - val_loss: 155.3394\n",
            "Epoch 117/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.7568 - val_loss: 124.6431\n",
            "Epoch 118/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2746 - val_loss: 145.8196\n",
            "Epoch 119/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1883 - val_loss: 112.2319\n",
            "Epoch 120/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4714 - val_loss: 126.6605\n",
            "Epoch 121/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6192 - val_loss: 124.4411\n",
            "Epoch 122/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6612 - val_loss: 135.3994\n",
            "Epoch 123/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4822 - val_loss: 135.2192\n",
            "Epoch 124/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.9849 - val_loss: 141.8197\n",
            "Epoch 125/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0855 - val_loss: 126.6094\n",
            "Epoch 126/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0797 - val_loss: 131.3882\n",
            "Epoch 127/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5566 - val_loss: 132.8164\n",
            "Epoch 128/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9169 - val_loss: 127.4285\n",
            "Epoch 129/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.5106 - val_loss: 144.7046\n",
            "Epoch 130/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8311 - val_loss: 144.4291\n",
            "Epoch 131/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.0886 - val_loss: 141.4570\n",
            "Epoch 132/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4534 - val_loss: 134.3288\n",
            "Epoch 133/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.2677 - val_loss: 136.2263\n",
            "Epoch 134/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2303 - val_loss: 137.8063\n",
            "Epoch 135/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4232 - val_loss: 122.6769\n",
            "Epoch 136/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.8651 - val_loss: 141.4015\n",
            "Epoch 137/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.0822 - val_loss: 125.7750\n",
            "Epoch 138/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4646 - val_loss: 133.9116\n",
            "Epoch 139/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8038 - val_loss: 140.2432\n",
            "Epoch 140/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.4436 - val_loss: 145.0199\n",
            "Epoch 141/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.9513 - val_loss: 127.7402\n",
            "Epoch 142/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 5.2531 - val_loss: 136.9873\n",
            "Epoch 143/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4231 - val_loss: 149.8390\n",
            "Epoch 144/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6821 - val_loss: 141.8847\n",
            "Epoch 145/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6146 - val_loss: 124.2590\n",
            "Epoch 146/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.8715 - val_loss: 137.1976\n",
            "Epoch 147/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.4622 - val_loss: 113.2527\n",
            "Epoch 148/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2827 - val_loss: 140.0885\n",
            "Epoch 149/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4458 - val_loss: 121.3266\n",
            "Epoch 150/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.5259 - val_loss: 144.8931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fa59ae6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45lK4MLETPqe"
      },
      "source": [
        "#prediction\r\n",
        "model1.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFbjyvCJT3Sv"
      },
      "source": [
        "###LAB: Classification with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "q8pGHHxBUqGr"
      },
      "source": [
        "#### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "CD4Uew4dUqGs"
      },
      "source": [
        "In this lab, we will learn how to use the Keras library to build models for classificaiton problems. We will use the popular MNIST dataset, a dataset of images, for a change. \n",
        "\n",
        "The <strong>MNIST database</strong>, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n",
        "\n",
        "The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau.\n",
        "\n",
        "Also, this way, will get to compare how conventional neural networks compare to convolutional neural networks, that we will build in the next module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze_bFGWFUqGs"
      },
      "source": [
        "<h2>Classification Models with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. Use of MNIST database for training various image processing systems</h5>\n",
        "<h5> 2. Build a Neural Network </h5>\n",
        "<h5> 3. Train and Test the Network. </h5>\n",
        "\n",
        "<p>This link will be used by your peers to assess your project. In your web app, your peers will be able to upload an image, which will then be classified using your custom classifier you connected to the web app. Your project will be graded by how accurately your app can classify <b>Fire</b>, <b>Smoke</b> and <b>Neutral (No Fire or Smoke)</b>.<p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "7MyCmvcMUqGt"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "\n",
        "1.  <a href=\"#item312\">Import Keras and Packages</a>      \n",
        "2.  <a href=\"#item322\">Build a Neural Network</a>     \n",
        "3.  <a href=\"#item332\">Train and Test the Network</a>     \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2kih0lcUqGt"
      },
      "source": [
        "<a id='item312'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "KJ7rA2ZjUqGt"
      },
      "source": [
        "#### Import Keras and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ugzLCSUqGu"
      },
      "source": [
        "Let's start by importing Keras and some of its modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kmvg7ncT6dr"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "N2N54G7VUqGv"
      },
      "source": [
        "Since we are dealing we images, let's also import the Matplotlib scripting layer in order to view the images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "YZmx7J95UqGv"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "PJAAvcGZUqGv"
      },
      "source": [
        "The Keras library conveniently includes the MNIST dataset as part of its API. You can check other datasets within the Keras library [here](https://keras.io/datasets?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ). \n",
        "\n",
        "So, let's load the MNIST dataset from the Keras library. The dataset is readily divided into a training set and a test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7moM_JxVmI3",
        "outputId": "f82deadd-22b2-4ce8-848c-7c041abcee02"
      },
      "source": [
        "#import the data\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "#read the data\r\n",
        "(xtr,ytr),(xte,yte) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "axVezyIMUqGw"
      },
      "source": [
        "Let's confirm the number of images in each set. According to the dataset's documentation, we should have 60000 images in X_train and 10000 images in the X_test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXAzSn9eaRL6",
        "outputId": "883d39e2-5888-493f-e21f-1e3567749d4a"
      },
      "source": [
        "xtr[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hREz0nVHWcgZ",
        "outputId": "23f65859-f13d-4347-cdca-e23548ae7f21"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOfPA5cUWdwh",
        "outputId": "4c61c96e-171e-4fbe-eef2-a84670e7dffb"
      },
      "source": [
        "xte.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "g2qIYypUUqGx"
      },
      "source": [
        "The first number in the output tuple is the number of images, and the other two numbers are the size of the images in datset. So, each image is 28 pixels by 28 pixels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WuDHiPtcUqGx"
      },
      "source": [
        "Let's visualize the first image in the training set using Matplotlib's scripting layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "eht9Ejs3WfFf",
        "outputId": "2be6db25-ecf7-43ef-c77c-f0041e413cde"
      },
      "source": [
        "plt.imshow(xtr[0])  # as u can see its 28*28 size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8ce4d56438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "pdhVevmSUqGx"
      },
      "source": [
        "With conventional neural networks, we cannot feed in the image as input as is. So we need to flatten the images into one-dimensional vectors, each of size 1 x (28 x 28) = 1 x 784.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNYpyh7jW01I"
      },
      "source": [
        "#flatten images into 1d vector\r\n",
        "num_pixels  = xtr.shape[1]*xtr.shape[2]  # find size of one-dimensional vector\r\n",
        "#since we have same num of pixels in xtr,xte we can use the same \r\n",
        "xtr = xtr.reshape(xtr.shape[0],num_pixels).astype(\"float32\") # flatten training images\r\n",
        "xte = xte.reshape(xte.shape[0],num_pixels).astype(\"float32\") # flatten training images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RTVNPz5bxkt",
        "outputId": "5b43ae03-8f26-44c9-8047-adb709b09f8d"
      },
      "source": [
        "xtr[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yly9pLkWbz0d",
        "outputId": "1d7d8f77-c4d3-4bb2-d199-2f38fd5b3481"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Ts_LoA1nUqGy"
      },
      "source": [
        "Since pixel values can range from 0 to 255, let's normalize the vectors to be between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS0p2hhlb2GN"
      },
      "source": [
        "#normalise inputs from 0-255 to 0-1\r\n",
        "xtr =xtr/255\r\n",
        "xte = xte/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg-x1zpfcGCZ",
        "outputId": "370ebb22-e7ea-4bcc-8fcd-e5241e63e1fe"
      },
      "source": [
        "ytr[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Chu8TNrcUqGy"
      },
      "source": [
        "Finally, before we start building our model, remember that for classification we need to divide our target variable into binary values. We use the to_categorical function from the Keras Utilities package.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmJlHtfIcI1a",
        "outputId": "85f93ae8-5019-4a7c-bd5d-fc044c79aad0"
      },
      "source": [
        "#one hot encode outputs, since we have 0-9 digits it will create 10columns\r\n",
        "ytr = to_categorical(ytr)\r\n",
        "yte = to_categorical(yte)\r\n",
        "\r\n",
        "num_classes = yte.shape[1]\r\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "nKKr7AcnUqGz"
      },
      "source": [
        "#### Build a Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_UunfdBc3EK"
      },
      "source": [
        "#define classification model\r\n",
        "def classification_model():\r\n",
        "  #create model\r\n",
        "  model =Sequential()\r\n",
        "  model.add(Dense(num_pixels,activation=\"relu\",input_shape=(num_pixels,)))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(num_classes,activation=\"softmax\"))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "8nTpi3pYUqG0"
      },
      "source": [
        "#### Train and Test the Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCYouc77PIav",
        "outputId": "3a6c9f66-8d51-4d14-9e81-f6096c0df5e3"
      },
      "source": [
        "model = classification_model()\r\n",
        "\r\n",
        "#fit\r\n",
        "model.fit(xtr,ytr,validation_data=(xte,yte),epochs=10,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 - 11s - loss: 0.1861 - accuracy: 0.9445 - val_loss: 0.0856 - val_accuracy: 0.9742\n",
            "Epoch 2/10\n",
            "1875/1875 - 10s - loss: 0.0782 - accuracy: 0.9754 - val_loss: 0.0767 - val_accuracy: 0.9764\n",
            "Epoch 3/10\n",
            "1875/1875 - 10s - loss: 0.0526 - accuracy: 0.9838 - val_loss: 0.0774 - val_accuracy: 0.9763\n",
            "Epoch 4/10\n",
            "1875/1875 - 10s - loss: 0.0400 - accuracy: 0.9868 - val_loss: 0.0639 - val_accuracy: 0.9809\n",
            "Epoch 5/10\n",
            "1875/1875 - 10s - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.0940 - val_accuracy: 0.9745\n",
            "Epoch 6/10\n",
            "1875/1875 - 10s - loss: 0.0257 - accuracy: 0.9912 - val_loss: 0.0890 - val_accuracy: 0.9774\n",
            "Epoch 7/10\n",
            "1875/1875 - 10s - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.0944 - val_accuracy: 0.9798\n",
            "Epoch 8/10\n",
            "1875/1875 - 10s - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1185 - val_accuracy: 0.9732\n",
            "Epoch 9/10\n",
            "1875/1875 - 10s - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.1041 - val_accuracy: 0.9791\n",
            "Epoch 10/10\n",
            "1875/1875 - 10s - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.0992 - val_accuracy: 0.9808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8cd8a31400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzIDJeDfQQU_",
        "outputId": "3e4de07a-bbdf-41ca-9a40-4c006bc7ae0f"
      },
      "source": [
        "#evaluate the model\r\n",
        "scores = model.evaluate(xte,yte,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0992 - accuracy: 0.9808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "bQ465tK0UqG0"
      },
      "source": [
        "Let's print the accuracy and the corresponding error.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR06l6jbQfoX",
        "outputId": "129e2a2c-27e6-42d6-9289-45886125cf34"
      },
      "source": [
        "print(\"Accuracy: {}% \\n Error: {}\".format(scores[1],1-scores[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9807999730110168% \n",
            " Error: 0.019200026988983154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "2ScmlXgyUqG1"
      },
      "source": [
        "Just running 10 epochs could actually take over 20 minutes. But enjoy the results as they are getting generated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "0gCnsG2UUqG2"
      },
      "source": [
        "Sometimes, you cannot afford to retrain your model everytime you want to use it, especially if you are limited on computational resources and training your model can take a long time. Therefore, with the Keras library, you can save your model after training. To do that, we use the save method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohRutWC3RXwb"
      },
      "source": [
        "model.save(\"classification_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "iEutyRCiUqG3"
      },
      "source": [
        "When you are ready to use your model again, you use the load_model function from <strong>keras.models</strong>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcJNpqkRurq"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "\r\n",
        "pretrained_model = load_model(\"classification_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuGXaoxNU3TE"
      },
      "source": [
        "##Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTqWRcvbU6B4"
      },
      "source": [
        "###Shallow Versus Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x83raCU-V5cH"
      },
      "source": [
        ">So far, we have mostly been dealing with not very deep, or shallow, neural networks. And the main reason is that they really do serve as the building block of deep neural networks and are easier to understand due to their simplicity. There isn't really a consensus on the definition of a shallow neural network but a neural network with one hidden layer is considered a shallow neural network whereas a network with many hidden layers and a large number of neurons in each layer is considered a deep neural network. Also, unlike a shallow neural network which takes only input as vectors, deep neural networks are able to take raw data such as images and text and automatically extract the necessary features to learn the data better. We will start learning about deep learning algorithms in the next videos. But if neural networks have been around for quite some time, how come only recently did they turn deep and start taking off resulting in a plethora of cool and exciting applications? \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(902).png?raw=true)\r\n",
        "\r\n",
        ">The sudden boom in the deep learning field can be attributed to three main factors. Number one, advancement in the field itself. We talked about this briefly in the activation functions video, where we mentioned that the ReLU activation function helped overcome the challenge of the vanishing gradient problem, and therefore, opened the door to the creation of very deep networks. Therefore, advancement in the field itself is one factor that helped deep learning take off. Another main reason is the availability of data. Deep neural networks work best when trained with large and large amounts of data, since neural networks learn the training data so well, then large amounts of data have to be used in order to avoid overfitting of the training data. Now that large amounts of data are readily available and easy to acquire like never before, deep learning algorithms are being tried and tested like never before. Especially that the other conventional machine learning algorithms, while they do improve with more data, but up to a certain point. After that, no significant improvement would be observed with more data. That is definitely not the case with deep learning. The more data you feed it the better it performs. Finally, and this goes hand-in-hand with point number 2, is computational power. With NVIDIA's super powerful GPUs, we are now able to train very deep neural networks on tremendous amount of data in a matter of hours as opposed to days or weeks, which is how long it used to take to train very deep neural networks. Therefore, users are able to experiment with different deep neural networks and test different prototypes in much shorter periods of time. These three factors are the main reasons behind the boom of deep learning. In the next video, we will start learning about deep learning algorithms. We will start with supervised deep learning algorithms, and in the next video, we will learn about convolutional neural networks.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(903).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkjESU0ehgY"
      },
      "source": [
        "### Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jxl4xLZZhsC"
      },
      "source": [
        "####Convolutional Neural Networks(ConvNets or CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oREr1KkNZjOz"
      },
      "source": [
        "> Convolutional neural networks, which are supervised deep learning models have revolutionized the field of computer vision, especially object detection in images.Convolutional neural networks are very similar to the neural networks that we have seen so far in this course. They are made up of neurons, which need to have the weights and biases optimized. Each neuron combines the inputs that it receives by computing the dot product between each input and the corresponding weight before it fits the resulting total input into an activation function, ReLU most likely. So then, what is different with these networks and why are they called convolutional neural networks? Well convolutional neural networks, or CNNs for short, make the explicit assumption that the inputs are images, which allows us to incorporate certain properties into their architecture. These properties make the forward propagation step much more efficient and vastly reduces the amount of parameters in the network. Therefore, CNNs are best for solving problems related to image recognition, object detection, and other computer vision applications.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(904).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Here is a typical architecture of a convolutional neural network. As you can see, the network consists of a series of convolutional, ReLU, and pooling layers as well as a number of fully connected layers which are necessary before the output is generated. Now, let's study what happens in each layer.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(905).png?raw=true)\r\n",
        "\r\n",
        ">So far, we have dealt only with conventional neural networks that take an ( n x 1) vector as their input. The input to a convolutional neural network, on the other hand, is mostly an (n x m x 1) for grayscale images or an (n x m x 3) for colored images, where the number 3 represents the red, green, and blue components of each pixel in the image.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(906).png?raw=true)\r\n",
        "\r\n",
        ">Convolutional layers are the major building blocks used in convolutional neural networks.A convolution is the simple application of a filter(kernel) to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image. So in the convolutional layer, we basically define filters(kernels) and we compute the convolution between the defined filters and each of the three images. If we take the red image for example, let's assume these are the pixel values. Now for a (2 x 2) filter with these values, let's create an empty matrix to save the results of the convolution process. We start by sliding(stride) the filter over the image and computing the dot product between the filter and the overlapping pixel values and storing the result in the empty matrix. We repeat this step moving our filter one cell, or one stride is the proper terminology, at a time, and we repeat this until we cover the entire image and fill the empty matrix. Here, I just showed one filter and only one of the three images. The same thing would be applied to the green and blue images and you can apply more than one filter. The more filters we use, the more we are able to preserve the spatial dimensions better. But one question you must be asking yourself at this point is, why would we need to use convolution? Why not flatten the input image into an (n x m) x 1 vector and use that as our input? Well, if we do that, we will end up with a massive number of parameters that will need to be optimized, and it will be super computationally expensive. Also, decreasing the number of parameters would definitely help in preventing the model from overfitting the training data. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(907).png?raw=true)\r\n",
        "\r\n",
        ">It is worth mentioning that a convolutional layer also consists of ReLU's which filter the output of the convolutional step passing only positive values and turning any negative values to 0. \r\n",
        "\r\n",
        ">The next layer in our convolutional neural network is the pooling layer. The pooling layer's main objective is to reduce the spatial dimensions of the data propagating through the network. There are two types of pooling that are widely used in convolutional neural networks. Max- pooling and average pooling. In max-pooling which is the most common of the two, for each section of the image we scan we keep the highest value, like so. Here our filter is moving two strides at a time. Similarly, with average pooling, we compute the average of each area we scan. In addition to reducing the dimension of the data, pooling, or max pooling in particular, provides spatial variance which enables the neural network to recognize objects in an image even if the object does not exactly resemble the original object. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(908).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(909).png?raw=true)\r\n",
        "\r\n",
        ">Finally, in the fully connected layer, we flatten the output of the last convolutional layer and connect every node of the current layer with every other node of the next layer. This layer basically takes as input the output from the preceding layer, whether it is a convolutional layer, ReLU, or pooling layer, and outputs an n-dimensional vector, where n is the number of classes pertaining to the problem at hand. For example, if you are building a network to classify images of digits, the dimension n would be 10, since there are 10 digits. You will be covering convolutional neural networks in much more details in the other courses in this specialization, but this information is more than enough to give you a general understanding of convolutional neural networks. Now let's see how we can use the Keras library to build a convolutional neural network. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(910).png?raw=true)\r\n",
        "\r\n",
        ">Here I will show you how you can use the Keras library to build a convolutional neural network. Training and testing of a convolutional neural network are the same as what we have seen so far. So to begin with, we use the sequential constructor to create our model. Then, we define our input to be the size of the input images. Assuming the input images are 128 by 128 color images, we define the input shape to be a tuple of (128, 128, 3). Next, we start adding layers to the network. We start with a convolutional layer, with 16 filters, each filter being of size 2x2 and slides through the image with a stride of magnitude 1 in the horizontal direction, and of magnitude 1 in the vertical direction. And the layer uses the ReLU activation function. Then, we add a pooling layer and we're using max-pooling here with a filter or pooling size of 2 and the filter slides through the image with a stride of magnitude 2. Next, we add another set of convolutional and pooling layers. The only difference here is we are using more filters in the convolutional layer, actually twice as many filters as the first convolutional layer. Finally, we flatten the output from these layers so that the data can proceed to the fully connected layers. We add another dense layer with 100 nodes and an output layer that has nodes equal to the number of classes in the problem at hand. And we use the softmax activation function in order to convert the outputs into probabilities.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(911).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwZ58hhzez48"
      },
      "source": [
        "####Recurrent Neural Networks(RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNoOK8fvgf87"
      },
      "source": [
        ">In this video, we will learn about another supervised deep learning model, which is the recurrent neural network. So far, we have seen neural networks and deep learning models that see datapoints as independent instances. However, let's say you want to build a model that can analyze scenes in a movie. Well, you cannot assume that scenes in a movie are independent, and therefore, traditional deep learning models are not suitable for this application. Recurrent neural networks overcome this issue. Recurrent neural networks or (RNNs) for short, are networks with loops that don't just take a new input at a time, but also take in as input the output from the previous dat point that was fed into the network. Accordingly, this is how the architecture of a recurrent neural network would look like. Essentially, we can start with a normal neural network. At time t = 0, the network takes in input x0 and outputs a0. Then, at time t = 1, in addition to the input x1, the network also takes a0 as input, weighted with weight w0,1, and so on and so forth. As a result, recurrent neural networks are very good at modelling patterns and sequences of data, such as texts, genomes, handwriting, and stock markets. These algorithms take time and sequence into account, which means that they have a temporal dimension.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(912).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        "> A very popular type of recurrent neural network is the long short-term memory model or the (LSTM) model for short. It has been successfully used for many applications including image generation, where a model trained on many images is used to generate new novel images. Another application is handwriting generation, which I described in the welcome video of this course. Also LSTM models have been successfully used to build algorithms that can automatically describe images as well as streams of videos. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(913).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEKYZsbwhUG-"
      },
      "source": [
        "###Unsupervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYVc6KXvhZHU"
      },
      "source": [
        "####Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8a8mm43hz_f"
      },
      "source": [
        "> In this video, we will switch to an unsupervised deep learning model which is the autoencoder. So what are autoencoders? Autoencoding is a data compression algorithm where the compression and the decompression functions are learned automatically from data. instead of being engineered by a human. Such autoencoders are built using neural networks. Autoencoders are data specific, which means that they will only be able to compress data similar to what they have been trained on. Therefore, an autoencoder trained on pictures of cars would do a rather poor job of compressing pictures of buildings, because the features it would learn would be vehicle or car specific. Some interesting applications of autoencoders are data denoising and dimensionality reduction for data visualization. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(914).png?raw=true)\r\n",
        "\r\n",
        ">Here is the architecture of an autoencoder. It takes an image, for example, as an input and uses an encoder to find the optimal compressed representation of the input image. Then, using a decoder the original image is restored. So an autoencoder is an unsupervised neural network model. It uses backpropagation by setting the target variable to be the same as the input. In other words, it tries to learn an approximation of an identity function. Because of non-linear activation functions in neural networks, autoencoders can learn data projections that are more interesting than a principal component analysis PCA or other basic techniques, which can handle only linear transformations. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(915).png?raw=true)\r\n",
        "\r\n",
        "> A very popular type of autoencoders is the Restricted Boltzmann Machines or (RBMs) for short. RBMs have been successfully used for various applications, including fixing imbalanced datasets. Because RBMs learn the input in order to be able to regenerate it, then they can learn the distribution of the minority class in an imbalance dataset ,and then generate more data points of that class, transforming the imbalance dataset into a balanced data set. Similarly, RBMs can also be used to estimate missing values in different features of a data set. Another popular application of Restricted Boltzmann Machines is automatic feature extraction of especially unstructured data. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(916).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcpPN9c7kwVv"
      },
      "source": [
        "###LAB: Convolutional Neural Networks with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwlbvEdqkdEl"
      },
      "source": [
        "In this lab, we will learn how to use the Keras library to build convolutional neural networks. We will also use the popular MNIST dataset and we will compare our results to using a conventional neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3fQrafzkdEm"
      },
      "source": [
        "<h2>Convolutional Neural Networks with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. How to use the Keras library to build convolutional neural networks.</h5>\n",
        "<h5> 2. Convolutional Neural Network with One Convolutional and Pooling Layers.</h5>\n",
        "<h5> 3. Convolutional Neural Network with Two Convolutional and Pooling Layers.</h5>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZkQAnRPkdEm"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "      \n",
        "1. <a href=\"#item41\">Import Keras and Packages</a>   \n",
        "2. <a href=\"#item42\">Convolutional Neural Network with One Convolutional and Pooling Layers</a>  \n",
        "3. <a href=\"#item43\">Convolutional Neural Network with Two Convolutional and Pooling Layers</a>  \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLrUPKAkdEn"
      },
      "source": [
        "<a id='item41'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1wbrMETkdEn"
      },
      "source": [
        "#### Import Keras and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD-WjbfjkdEn"
      },
      "source": [
        "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5m9KJxck2oo"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nonYzI-IkdEo"
      },
      "source": [
        "When working with convolutional neural networks in particular, we will need additional packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c6j5JIDlUHh"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D        # to add convolutional layers\r\n",
        "from keras.layers.convolutional import MaxPooling2D  # to add pooling layers\r\n",
        "from keras.layers import Flatten                   # to flatten data for fully connected layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN6qr2bukdEp"
      },
      "source": [
        "#### Convolutional Layer with One set of convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrV5SrA2merk"
      },
      "source": [
        "#import data\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "#load data\r\n",
        "(xtr,ytr),(xte,yte) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi4iwBdWm00y",
        "outputId": "12cef3e6-ea17-41d1-dbbf-bec7e91457c8"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjmGIASm3Qh",
        "outputId": "47258860-3608-42bb-c533-ca329a0fc00c"
      },
      "source": [
        "ytr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlAcpYiNm6KE",
        "outputId": "f3937083-4192-4704-bfc8-3a028fd153e8"
      },
      "source": [
        "xte.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJU4ythCm8f9",
        "outputId": "609a63a0-2fce-4e85-9979-515bcb27b95b"
      },
      "source": [
        "yte.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDQ9DwmKm-qg"
      },
      "source": [
        "# reshape to be [samples][pixels][width][height]\r\n",
        "xtr = xtr.reshape(xtr.shape[0],28,28,1).astype(\"float32\")\r\n",
        "xte = xte.reshape(xte.shape[0],28,28,1).astype(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrZ3r9vVnqSb",
        "outputId": "529c95ab-b168-47c1-b21a-99982e02e079"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXxYmILLnrem",
        "outputId": "c17b0def-3b8f-449e-da58-ff11747cce0d"
      },
      "source": [
        "xte.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNqxseEkdEq"
      },
      "source": [
        "Let's normalize the pixel values to be between 0 and 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMlf5CmjnyNV"
      },
      "source": [
        "xtr = xtr/255   # normalize training data\r\n",
        "xte = xte/255   # normalize test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NPT_yU1kdEr"
      },
      "source": [
        "Next, let's convert the target variable into binary categories\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNliT2Yyn-rX"
      },
      "source": [
        "ytr = to_categorical(ytr)\r\n",
        "yte = to_categorical(yte)\r\n",
        "\r\n",
        "num_classes = yte.shape[1]   # number of categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoLmsFNPkdEr"
      },
      "source": [
        "Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4rASNdyD9n"
      },
      "source": [
        "**defualt values for conv layer:**  \r\n",
        "filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None\r\n",
        "\r\n",
        "only filters and kernel_size parameters must be given, others are optional or has default values next to them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV2MIdjmoXS1"
      },
      "source": [
        "def convolution_model():\r\n",
        "  \r\n",
        "  #create model\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Conv2D(filters=16,kernel_size=(5,5),strides=(1,1),activation = \"relu\",input_shape=(28,28,1)))  #conolution layer\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))  #pooling layer\r\n",
        " \r\n",
        "  model.add(Flatten()) #flatten data into n*1 array\r\n",
        "  model.add(Dense(100,activation=\"relu\")) #fully connected layer\r\n",
        "  model.add(Dense(num_classes,activation=\"softmax\")) #output layer\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yqxqYXgkdEs"
      },
      "source": [
        "Finally, let's call the function to create the model, and then let's train it and evaluate it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi17mddlr7zj"
      },
      "source": [
        ">The batch size defines the number of samples that will be propagated through the network.  \r\n",
        "For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdJZMLBqqc-G",
        "outputId": "a029af41-76c3-47e5-f8a9-18a7949aa0b7"
      },
      "source": [
        "model = convolution_model()\r\n",
        "\r\n",
        "#fit the model\r\n",
        "model.fit(xtr,ytr,validation_data=(xte,yte),epochs=10,batch_size=200,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "Epoch 1/10\n",
            "300/300 - 17s - loss: 0.3038 - accuracy: 0.9153 - val_loss: 0.1026 - val_accuracy: 0.9695\n",
            "Epoch 2/10\n",
            "300/300 - 17s - loss: 0.0818 - accuracy: 0.9765 - val_loss: 0.0644 - val_accuracy: 0.9803\n",
            "Epoch 3/10\n",
            "300/300 - 17s - loss: 0.0554 - accuracy: 0.9838 - val_loss: 0.0480 - val_accuracy: 0.9856\n",
            "Epoch 4/10\n",
            "300/300 - 17s - loss: 0.0439 - accuracy: 0.9866 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
            "Epoch 5/10\n",
            "300/300 - 17s - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.0369 - val_accuracy: 0.9872\n",
            "Epoch 6/10\n",
            "300/300 - 17s - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0371 - val_accuracy: 0.9878\n",
            "Epoch 7/10\n",
            "300/300 - 17s - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.0388 - val_accuracy: 0.9869\n",
            "Epoch 8/10\n",
            "300/300 - 17s - loss: 0.0186 - accuracy: 0.9944 - val_loss: 0.0394 - val_accuracy: 0.9871\n",
            "Epoch 9/10\n",
            "300/300 - 17s - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.0380 - val_accuracy: 0.9876\n",
            "Epoch 10/10\n",
            "300/300 - 17s - loss: 0.0135 - accuracy: 0.9960 - val_loss: 0.0427 - val_accuracy: 0.9870\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8cd437aac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kT7sE6Rv7Xa",
        "outputId": "64e0b9ff-f566-49ce-bfb4-91ab1520649a"
      },
      "source": [
        "#to know the output names of the metrics used for a given loss function\r\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4v6u41yswcJ",
        "outputId": "628264e5-85d0-4977-e773-63357e3fedf6"
      },
      "source": [
        "#evaluate the model\r\n",
        "scores = model.evaluate(xte,yte,verbose=0)\r\n",
        "print(\"accuracy = {} \\n error = {}\".format(scores[1],100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.9869999885559082 \n",
            " error = 1.3000011444091797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOHvHPjHtZ38",
        "outputId": "8be51ea4-9279-4427-b1a8-c6acda960b4f"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04270915687084198, 0.9869999885559082]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qVV95SGkdEu"
      },
      "source": [
        "#### Convolutional Layer with two sets of convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4dXVCslkdEv"
      },
      "source": [
        "Let's redefine our convolutional model so that it has two convolutional and pooling layers instead of just one layer of each.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJwrSmKHtbHI"
      },
      "source": [
        "def convolution_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Conv2D(16,(5,5),activation=\"relu\",input_shape=(28,28,1)))  #default stride=(1,1)\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\r\n",
        "\r\n",
        "  model.add(Conv2D(8,(2,2),activation=\"relu\"))\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\r\n",
        "\r\n",
        "  model.add(Flatten())\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(num_classes,activation=\"softmax\"))\r\n",
        "\r\n",
        "  #compile\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcH5Yjl1kdEx"
      },
      "source": [
        "Now, let's call the function to create our new convolutional neural network, and then let's train it and evaluate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3ggfc3Yz_ee",
        "outputId": "032cd832-fcee-4d64-d13e-2bd2fb4516c4"
      },
      "source": [
        "model = convolution_model()\r\n",
        "\r\n",
        "#fit\r\n",
        "model.fit(xtr,ytr,validation_data=(xte,yte),epochs=10,batch_size=200,verbose=2)\r\n",
        "\r\n",
        "#evaluate\r\n",
        "scores = model.evaluate(xte,yte,verbose=0)\r\n",
        "print(\"accuracy = {} \\n error = {}\".format(scores[1],100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 19s - loss: 0.4334 - accuracy: 0.8750 - val_loss: 0.1060 - val_accuracy: 0.9674\n",
            "Epoch 2/10\n",
            "300/300 - 18s - loss: 0.1001 - accuracy: 0.9698 - val_loss: 0.0702 - val_accuracy: 0.9787\n",
            "Epoch 3/10\n",
            "300/300 - 18s - loss: 0.0752 - accuracy: 0.9772 - val_loss: 0.0592 - val_accuracy: 0.9817\n",
            "Epoch 4/10\n",
            "300/300 - 18s - loss: 0.0621 - accuracy: 0.9814 - val_loss: 0.0505 - val_accuracy: 0.9843\n",
            "Epoch 5/10\n",
            "300/300 - 18s - loss: 0.0533 - accuracy: 0.9837 - val_loss: 0.0457 - val_accuracy: 0.9859\n",
            "Epoch 6/10\n",
            "300/300 - 19s - loss: 0.0469 - accuracy: 0.9855 - val_loss: 0.0446 - val_accuracy: 0.9860\n",
            "Epoch 7/10\n",
            "300/300 - 19s - loss: 0.0431 - accuracy: 0.9867 - val_loss: 0.0438 - val_accuracy: 0.9859\n",
            "Epoch 8/10\n",
            "300/300 - 19s - loss: 0.0378 - accuracy: 0.9883 - val_loss: 0.0527 - val_accuracy: 0.9833\n",
            "Epoch 9/10\n",
            "300/300 - 18s - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.0362 - val_accuracy: 0.9881\n",
            "Epoch 10/10\n",
            "300/300 - 19s - loss: 0.0303 - accuracy: 0.9908 - val_loss: 0.0368 - val_accuracy: 0.9887\n",
            "accuracy = 0.9886999726295471 \n",
            " error = 1.130002737045288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpUeoS12Ynh5"
      },
      "source": [
        "###Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDulhpo7Y4c_"
      },
      "source": [
        ">Let's quickly recap what we covered in this course. In module 1, we went over a number of exciting and motivating applications of deep learning. We also briefly covered neurons and neural networks in the brain to appreciate how they inspire artificial neural networks, and then we learned how neural networks make predictions through the forward propagation process. In module 2, we learned how an artificial neural network learns through gradient descent and backpropagation. We also learned about the vanishing gradient problem, and which activation functions are best to overcome this problem. In module 3, we learnt about the Keras library and how to use it to build models for regression and classification problems. In module 4, we learned about supervised and unsupervised deep neural networks, and we used the Keras library to build a convolutional neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLPMy5mlyHRo"
      },
      "source": [
        "##Final project: Building a Regression Model in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klg2-CZuXj2I"
      },
      "source": [
        "####Part A: Build a baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQciPOrmXFUH"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVzYfF4tX9Ds"
      },
      "source": [
        "!wget -q https://cocl.us/concrete_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "4R-4DYCxYA_i",
        "outputId": "c7bb27dd-5881-45ba-a84d-459e9fa3dd72"
      },
      "source": [
        "df = pd.read_csv(\"concrete_data\")\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbE-b9vYg5T"
      },
      "source": [
        "####Data wrangling and EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwvfIZa0Ygob",
        "outputId": "8dbdbb54-6f0f-47c1-b3dd-1cc26b10d087"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1030 entries, 0 to 1029\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Cement              1030 non-null   float64\n",
            " 1   Blast Furnace Slag  1030 non-null   float64\n",
            " 2   Fly Ash             1030 non-null   float64\n",
            " 3   Water               1030 non-null   float64\n",
            " 4   Superplasticizer    1030 non-null   float64\n",
            " 5   Coarse Aggregate    1030 non-null   float64\n",
            " 6   Fine Aggregate      1030 non-null   float64\n",
            " 7   Age                 1030 non-null   int64  \n",
            " 8   Strength            1030 non-null   float64\n",
            "dtypes: float64(8), int64(1)\n",
            "memory usage: 72.5 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PF2014_YS0J",
        "outputId": "12a0d3c3-ad15-4703-deb5-082aacd9857d"
      },
      "source": [
        "#checking for nulls\r\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cement                0\n",
              "Blast Furnace Slag    0\n",
              "Fly Ash               0\n",
              "Water                 0\n",
              "Superplasticizer      0\n",
              "Coarse Aggregate      0\n",
              "Fine Aggregate        0\n",
              "Age                   0\n",
              "Strength              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Zb7yQJKkYqcL",
        "outputId": "2d2a307e-46a3-43d1-9a43-000eb8502e2c"
      },
      "source": [
        "#descriptive statistics\r\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-En7NvM-acjX"
      },
      "source": [
        "#####Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "7sA6tf4OaaYi",
        "outputId": "6aebb7a4-3dda-4ffb-91ae-2b567136b382"
      },
      "source": [
        "x = df.loc[:,df.columns!=\"Strength\"]\r\n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Coarse Aggregate  Fine Aggregate  Age\n",
              "0   540.0                 0.0      0.0  ...            1040.0           676.0   28\n",
              "1   540.0                 0.0      0.0  ...            1055.0           676.0   28\n",
              "2   332.5               142.5      0.0  ...             932.0           594.0  270\n",
              "3   332.5               142.5      0.0  ...             932.0           594.0  365\n",
              "4   198.6               132.4      0.0  ...             978.4           825.5  360\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH3ZZ7D3bFLF",
        "outputId": "3d6e28e7-87cb-4b3f-bc77-c86f64286b52"
      },
      "source": [
        "#target variable\r\n",
        "y =df[\"Strength\"]\r\n",
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    79.99\n",
              "1    61.89\n",
              "2    40.27\n",
              "3    41.05\n",
              "4    44.30\n",
              "Name: Strength, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bju-0Ukl4Xi",
        "outputId": "4e9ba0ab-819b-4a8d-e37f-3a966d12d7b3"
      },
      "source": [
        "n_cols =x.shape[1]\r\n",
        "n_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jeSndzYbpb-"
      },
      "source": [
        "####Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHUZUpHlcXCg"
      },
      "source": [
        "#####Import keras, required libraries from keras "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiRuVl0ebma6"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe6dWofybwVj"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcO4RtVecVAT"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0F_i1bGeKEt",
        "outputId": "0bdee0f6-dd3f-4a9d-f78a-67d405b0bfda"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "  model.fit(xtr,ytr,epochs=50,verbose=0)\r\n",
        "  yhat = model.predict(xte)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[208.78719075219684,\n",
              " 110.21765980852055,\n",
              " 75.44642882028447,\n",
              " 57.179537742292446,\n",
              " 56.47588452747049,\n",
              " 56.30609398773349,\n",
              " 46.64302230558537,\n",
              " 44.6077556154711,\n",
              " 52.70194513936033,\n",
              " 58.34934149306482,\n",
              " 56.212630614337705,\n",
              " 56.49682386971899,\n",
              " 50.736910164330446,\n",
              " 47.067397453212266,\n",
              " 47.31957236864499,\n",
              " 52.51699821467591,\n",
              " 51.634915422363555,\n",
              " 51.2249242656657,\n",
              " 50.8234092497561,\n",
              " 57.833287754181,\n",
              " 50.94099504406682,\n",
              " 51.988153800656896,\n",
              " 46.15085897921341,\n",
              " 51.28483948004605,\n",
              " 46.604172953359225,\n",
              " 47.51488430830116,\n",
              " 50.601613160477235,\n",
              " 49.79918686274293,\n",
              " 49.82773759271403,\n",
              " 57.93126720184942,\n",
              " 49.44300303841611,\n",
              " 55.06164998385387,\n",
              " 49.40068809302663,\n",
              " 63.59704391510421,\n",
              " 47.55153618653243,\n",
              " 48.058632745777,\n",
              " 49.23373680839788,\n",
              " 49.27686962127714,\n",
              " 49.94821498457236,\n",
              " 48.995377631904,\n",
              " 46.97622441870908,\n",
              " 49.68288942900039,\n",
              " 55.33990990410741,\n",
              " 49.41690952812501,\n",
              " 58.686521942304935,\n",
              " 48.75190087889846,\n",
              " 42.968063550531575,\n",
              " 47.41789116646781,\n",
              " 45.20853947128713,\n",
              " 54.86924929032545]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vtHuQdMg8Zm",
        "outputId": "3fa33937-ebc4-4ea0-e741-ab654460c0af"
      },
      "source": [
        "len(mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6juhF7PmDV9",
        "outputId": "3822b15a-45e8-4f81-f0ee-a0971fe2c87a"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  56.02220583081826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGHfM6SBmpZV",
        "outputId": "ec83b2ec-1b71-4c0e-ac4e-f95bc8449b09"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  23.932111596695954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL8v3vGQm7gc"
      },
      "source": [
        "####Part B: Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TuoWwePoiWo"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvaWJVRvnkzi"
      },
      "source": [
        ">We have to normalize data after splitting inorder to avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzCd8qogm7Ho",
        "outputId": "187da696-7e0e-40f7-efd2-340b2b301609"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model2 = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "\r\n",
        "  #normalising train data and test data\r\n",
        "  xtr_norm = (xtr-xtr.mean())/xtr.std()\r\n",
        "  xte_norm = (xte-xte.mean())/xte.std()\r\n",
        "\r\n",
        "  model2.fit(xtr_norm,ytr,epochs=50,verbose=0)\r\n",
        "  yhat = model2.predict(xte_norm)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[439.84951565829823,\n",
              " 167.2881323805372,\n",
              " 116.52696255574693,\n",
              " 83.54628245267295,\n",
              " 76.60663386104797,\n",
              " 61.757964141753845,\n",
              " 56.253277830378764,\n",
              " 56.64436415803959,\n",
              " 54.520585508517506,\n",
              " 45.980364352914584,\n",
              " 49.14363910108146,\n",
              " 56.37780249059305,\n",
              " 46.39790492551496,\n",
              " 39.16821135704735,\n",
              " 41.974978936292786,\n",
              " 36.369992648610655,\n",
              " 48.216746779332325,\n",
              " 41.29339616500098,\n",
              " 41.626030411861464,\n",
              " 44.85540131943483,\n",
              " 40.44387756597624,\n",
              " 37.30751674425388,\n",
              " 35.29120919049977,\n",
              " 42.85916715023342,\n",
              " 36.067394689528435,\n",
              " 32.37308262335113,\n",
              " 33.67781526401075,\n",
              " 41.80650290679822,\n",
              " 29.221944303021292,\n",
              " 37.43589107325537,\n",
              " 28.78095468224503,\n",
              " 33.88428831292457,\n",
              " 31.07476143130202,\n",
              " 39.16864360017536,\n",
              " 31.385877187884176,\n",
              " 30.19296198398804,\n",
              " 29.71447800150181,\n",
              " 37.6188543020724,\n",
              " 36.15119229833056,\n",
              " 35.38366137592524,\n",
              " 33.790906348089905,\n",
              " 30.742139197309513,\n",
              " 31.611501065877704,\n",
              " 51.31374322062494,\n",
              " 36.54802304981925,\n",
              " 36.34287063024487,\n",
              " 33.746110452582286,\n",
              " 33.93495880154476,\n",
              " 40.0080606198992,\n",
              " 37.82650036534979]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vzCEkSppRFX",
        "outputId": "19333fd6-2866-46b3-84ac-820b80d775b7"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  53.402061509465945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6B64Bp7pRFa",
        "outputId": "54504aaf-59e1-445a-b51a-061e4c587b97"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  59.87128600220436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hro04rWXpaCd"
      },
      "source": [
        "Hence the mean of mse in part B is less than that of part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInrIIyqpnRq"
      },
      "source": [
        "####Part C: Increate the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVgimlfypvkl"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbELMosNpvku"
      },
      "source": [
        ">We have to normalize data after splitting inorder to avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCth-Fm9pvku",
        "outputId": "8ac69620-6253-4972-821d-793952b103ef"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model3 = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "\r\n",
        "  #normalising train data and test data\r\n",
        "  xtr_norm = (xtr-xtr.mean())/xtr.std()\r\n",
        "  xte_norm = (xte-xte.mean())/xte.std()\r\n",
        "\r\n",
        "  model3.fit(xtr_norm,ytr,epochs=100,verbose=0)  #using 100 epochs for training.\r\n",
        "  yhat = model3.predict(xte_norm)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[146.77663814658078,\n",
              " 93.36411251868697,\n",
              " 53.60856827550833,\n",
              " 54.160375988856494,\n",
              " 50.51217518092866,\n",
              " 42.72444408334272,\n",
              " 55.367063618185384,\n",
              " 52.721157784312624,\n",
              " 61.00882527227466,\n",
              " 40.69873948647878,\n",
              " 60.98705863141358,\n",
              " 58.68928764528565,\n",
              " 40.69438765651557,\n",
              " 48.67502476964363,\n",
              " 46.67612239466983,\n",
              " 41.95206048508998,\n",
              " 51.85971513273133,\n",
              " 48.0355478780723,\n",
              " 39.94775579386421,\n",
              " 49.31739933600077,\n",
              " 43.25639887985103,\n",
              " 43.50107340879277,\n",
              " 38.36548572429632,\n",
              " 48.24544365234896,\n",
              " 56.538003419331446,\n",
              " 38.00388134570902,\n",
              " 48.68388051667007,\n",
              " 43.580515103974314,\n",
              " 55.934734579020464,\n",
              " 45.840462613160085,\n",
              " 48.21558522152279,\n",
              " 57.90777854192687,\n",
              " 39.84193420736204,\n",
              " 36.00487964896491,\n",
              " 52.767859032660624,\n",
              " 45.70420930506415,\n",
              " 47.252787767547495,\n",
              " 48.21848723054029,\n",
              " 43.63666131184858,\n",
              " 39.89076369166843,\n",
              " 46.330738199374245,\n",
              " 42.68404208436195,\n",
              " 53.90925068991481,\n",
              " 42.09314420320541,\n",
              " 38.93413012029169,\n",
              " 44.34910885547187,\n",
              " 51.21606395428008,\n",
              " 47.93173970997465,\n",
              " 43.56194492410869,\n",
              " 46.28256957860196]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SgIu2hJpvkv",
        "outputId": "93ad0577-6aa0-4ebc-9fac-723914d90c01"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  50.32920035200576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJo7WJkupvkw",
        "outputId": "1630de8c-d282-4729-e1d5-0c29cf3e6aaf"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  16.401320812546707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CBO5vqpvkw"
      },
      "source": [
        "Hence the mean of mse in part C is less than that of part B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM7YTlrrqQZn"
      },
      "source": [
        "####Part D: Increase the number of hidden layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHi1kqQPqz0j"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(10,activation=\"relu\"))\r\n",
        "  model.add(Dense(10,activation=\"relu\"))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZkloCQpqz0v"
      },
      "source": [
        ">We have to normalize data after splitting inorder to avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDopkmfXqz0v",
        "outputId": "a8a5b02a-d7a6-47ee-ce52-be74e7984b72"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model4 = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "\r\n",
        "  #normalising train data and test data\r\n",
        "  xtr_norm = (xtr-xtr.mean())/xtr.std()\r\n",
        "  xte_norm = (xte-xte.mean())/xte.std()\r\n",
        "\r\n",
        "  model4.fit(xtr_norm,ytr,epochs=50,verbose=0)\r\n",
        "  yhat = model4.predict(xte_norm)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[167.13197699572015,\n",
              " 96.2181010256609,\n",
              " 68.9870875378906,\n",
              " 42.25445851966935,\n",
              " 43.125303642492945,\n",
              " 38.411045990772784,\n",
              " 43.05971860450492,\n",
              " 38.25554518064909,\n",
              " 41.131142892565244,\n",
              " 37.03294072599632,\n",
              " 36.32436007675568,\n",
              " 35.05650862971943,\n",
              " 32.80361489017227,\n",
              " 34.80491866617692,\n",
              " 32.394063226787836,\n",
              " 38.160598284182896,\n",
              " 35.21455992636211,\n",
              " 31.58021913726483,\n",
              " 31.401842463556147,\n",
              " 37.76445877871854,\n",
              " 32.85106630840039,\n",
              " 36.36219789288243,\n",
              " 50.485952043655494,\n",
              " 28.418990079617483,\n",
              " 32.543285587148425,\n",
              " 38.2472257795662,\n",
              " 34.67497844712749,\n",
              " 30.52514468383634,\n",
              " 65.3176453713059,\n",
              " 26.329527555873863,\n",
              " 26.83905808883635,\n",
              " 26.328902546426644,\n",
              " 28.20501476400095,\n",
              " 30.21827387605979,\n",
              " 27.850911193939886,\n",
              " 31.407096335029994,\n",
              " 28.94632415062459,\n",
              " 28.226737672174693,\n",
              " 30.480730782856906,\n",
              " 27.580197703325997,\n",
              " 36.23556069028866,\n",
              " 23.631209582008577,\n",
              " 27.007136601673878,\n",
              " 27.04049110129364,\n",
              " 27.326691068751657,\n",
              " 25.69224623322013,\n",
              " 23.726137453164625,\n",
              " 26.83396615744322,\n",
              " 29.003817274573755,\n",
              " 21.629016840631508]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BIXSnl1qz0w",
        "outputId": "2a61f0fd-6361-41e9-cff5-6c79e828dca9"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  37.82155998122717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rYEUTc1qz0w",
        "outputId": "ca72987c-6b89-40c9-c661-746f0e90bae8"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  22.285606184757278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbG3QsJGqz0x"
      },
      "source": [
        "Hence the mean of mse in part D is less than that of part B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdhiHItHyLYU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOQ4SsPmFBSl"
      },
      "source": [
        "#Building Deep Learning Models with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2oiRHPULDzT"
      },
      "source": [
        "##Neural Networks, Deep Learning, and TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JI_2sWvLHfY"
      },
      "source": [
        "###Introduction to TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uSIeTMFLJPd"
      },
      "source": [
        ">Hello, and welcome! In this video, well be going through a brief overview of the structure and capabilities of the TensorFlow library. So lets get started. TensorFlow is an open source library developed by the Google Brain Team. Its an extremely versatile library, originally created for tasks that require heavy numerical computations. For this reason, TensorFlow was geared towards the problem of machine learning, and deep neural networks. Due to a C/C++ backend, TensorFlow is able to run faster than pure Python code. The last thing Id like to mention is that a TensorFlow application uses a structure known as a data flow graph, which is very useful to first build and then execute it in a session. It is also a very common programming model for parallel computing. Well cover this in more detail shortly. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(917).png?raw=true)\r\n",
        "\r\n",
        ">TensorFlow offers several advantages for an application. For instance, it provides both a Python and a C++ API. It should be noted, though, that Python API is more complete and its generally easier to use. TensorFlow also has great compilation times in comparison to the alternative deep learning libraries. And it supports CPUs, GPUs, and even distributed processing in a cluster. It is a very important feature as you can train a neural network using CPU and multiple GPUs, which makes the models very efficient on large-scale systems.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(918).png?raw=true)\r\n",
        "\r\n",
        ">TensorFlows structure is based on the execution of a data flow graph. So, lets take a closer look at this. A data flow graph, has two basic units. The nodes that represent a mathematical operation, and the edges which represent the multi-dimensional arrays, known as a tensors. So this high-level abstraction reveals how the data flows between operations. Please notice that using the data flow graph, we can easily visualize different parts of the graph, which is not an option while using other Python libraries such as Numpy or SciKit. The standard usage is to build a graph first and then execute it in a session. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(919).png?raw=true)\r\n",
        "\r\n",
        ">So, What is a Tensor? Well as weve already noted, the data thats passed between the operations are Tensors. In effect, a Tensor is a multidimensional array. It can be zero dimensional, such as scalar values, one dimensional as a line or vector, or 2-dimensional, such as a Matrix, and so on. The Tensor structure helps us by giving us the freedom to shape the dataset the way we want. Its also particularly helpful when dealing with images, due to the nature of how the information within images is encoded. Indeed, when you think about an image, its easy to understand that it has height and width, so it would make sense to represent the information contained in it with a two dimensional structure, such as a matrix, for example. But as you know, images have colors, and to add information about the colors, we need another dimension, and thats when a 3-dimensional tensor becomes particularly helpful.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(948).png?raw=true)\r\n",
        "\r\n",
        ">Now, lets take a look at a dataflow graph and see how tensors and operations build the graph. As mentioned before, in a dataflow graph, the nodes are called operations, which represent units of computation. The edges are tensors which represent the data consumed or produced by an operation. In this graph, Feature matrix is a placeholder. Placeholders can be seen as \"holes\" in your model -- \"holes\" through which you can pass the data from outside of the graph. Placeholders allow us to create our operations in the graph, without needing the data. When we want to execute the graph, we have to feed placeholders with our input data. This is why we need to initialize placeholders before using them. Lets look at another operation which builds the variables for our program. In this graph, Weight Matrix is a variable. TensorFlow variables, are used to share and persist some values, that are manipulated by the program Please notice that when you define a place- holder or variable, TensorFlow adds an operation to your graph. In our graph, Weight matrix and Feature matrix should be multiplied using a multiplication operation. After that, Add operation is called, which adds the result of the previous operation with bias term. The output of each operation is a tensor. The resulting tensors of each operation crosses the next one until the end where it's possible to get the desired result. After adding all these operations in a graph, we can create a session to run the graph, and perform the computations. For a better understanding of TensorFlow graphs, I recommend that you run the labs from this module, which walk you through different elements of the data flow, in the TensorFlow library. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(920).png?raw=true)\r\n",
        "\r\n",
        ">As previously mentioned, TensorFlow comes with an easy to use Python interface to build and execute your computational graphs. But what makes TensorFlow so popular today, is its architecture. TensorFlows flexible architecture allows you to deploy computation on one or more CPUs, or GPUs, or on a desktop, server, or even a mobile device.  This means you build your program once, and then you can run it easily on different devices. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(921).png?raw=true)\r\n",
        "\r\n",
        ">So lets briefly review the reasons why TensorFlow is well-suited for deep learning applications. First, TensorFlow has built-in support for deep learning and neural networks, so its easy to assemble a net, assign parameters, and run the training process. Second, it also has a collection of simple, trainable mathematical functions that are useful for neural networks. Finally, deep learning as a gradient-based machine learning algorithm will benefit from TensorFlows auto-differentiation and optimizers.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(922).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-_JxmXyKBg"
      },
      "source": [
        "###TensorFlow 2.x and Eager Execution\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpZkC6McyUHR"
      },
      "source": [
        ">Hello and welcome! In this video we'll be talking about TensorFlow 2.x and Eager Execution. This course was originally created in the days of TensorFlow 1.x, but now has been updated for TensorFlow version 2.x, which includes a major update and some important new capabilities. One of the major changes within TensorFlow 2.x is that the Keras framework has become the official high level API for TensorFlow. Keras is a Deep Learning API written in Python and is known for its user-friendliness. It offers abstractions that make it easy to develop deep learning models. However, Keras does not have its own execution engine and is dependent on other frameworks like Theano or TensorFlow. With TensorFlow 2.x, TensorFlow has not only become the default execution engine for Keras, but they are also now more tightly integrated. What this means for TensorFlow users, especially Python developers, is that they can now develop models more easily using Keras interfaces, yet leverage the powerful capabilities of TensorFlow in the back-end. Another TensorFlow 2.x enhancement is performance optimizations, multi-GPU support, and improved APIs for better usability for GPU acceleration. Besides this, one very prominent change in TensorFlow 2.x is Eager Execution. In cases where Keras, which is a high level API, isnt sufficient for your needs, you'll still need to use the TensorFlow low-level API. In such cases, Eager Execution mode is activated by default in TensorFlow 2.x, and, in fact, is the recommended way of using the TensorFlow low level API. Please keep in mind, however, that the low level TensorFlow API involves the use of linear algebra to express neural network layers.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(923).png?raw=true)\r\n",
        "\r\n",
        ">So, what is Eager Execution mode? Well, lets have a look at this code snippet. As you can see, we are initializing two tensors, A and B. Then we compute the tensor dot product and assign the result to C. But so far no computation has happened. C only represents an execution graph, which does not yet have a value. Only once executed within a TensorFlow session does a computation happen and a value is assigned to C. As such, this makes TensorFlow code very hard to debug as intermediate results are not accessible.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(924).png?raw=true)\r\n",
        "\r\n",
        ">This is where eager execution kicks in. With eager execution enabled, which by the way is the default in TensorFlow 2.x, code gets immediately executed, line by line, and intermediate results are instantly available. Eager Execution makes TensorFlow code look like ordinary Python code. And this is achieved without even needing to change your code when jumping between the two TensorFlow versions. Lets see how.... \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(925).png?raw=true)\r\n",
        "\r\n",
        ">The trick is in the data type. In TensorFlow 1.x each tensor is of type tensorflow.python.framework.ops.Tensor. With eager execution enabled, the type changes to EagerTensor. While having programmatically similar behavior, eager tensors have additional functionality. This way intermediate results can be obtained at any time.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(926).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86HNLimozRiT"
      },
      "source": [
        "###Introduction to Deep Learning\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn-77ejlzTD7"
      },
      "source": [
        ">Hello, and welcome! In this video, we will provide an introduction to Deep Learning, and see why it is such a hot topic today. It is not an overstatement to say that Deep learning can be found all around us, as it is used in countless ways across all industries. For example, Deep Learning is trying to help the health care industry on tasks such as cancer detection and drug discovery. In the internet service and mobile phone industries, we can see various apps which are using Deep Learning for image/video classification and speech recognition, such as, Google Voice, Apple Siri, Microsoft Skype, and so on. In media, entertainment, and news, we can see applications such as video captioning, real-time translation and personalization, or recommendation systems such as Netflix. In the development of self-driving cars, Deep Learning is helping researchers to overcome significant research problems, such as sign and passenger detection or lane tracking. In the Security field, Deep Learning is used for face recognition and video surveillance. These are just a few examples of the industries in which Deep Learning is being applied; and it is being used in many other fields and domains as well.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(927).png?raw=true)\r\n",
        "\r\n",
        ">The increasing popularity of Deep Learning today comes from three recent advances in our field: First, in the dramatic increases in computer processing capabilities; second, in the availability of massive amounts of data for training computer systems; and third, in the advances in machine learning algorithms and research.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(928).png?raw=true)\r\n",
        "\r\n",
        ">So, what is deep learning? Deep Learning is a series of supervised, semi-supervised and unsupervised methods that try to solve some machine learning problems using deep neural networks. A deep neural network is a neural network which often has more than two layers, and uses specific mathematical modeling in each layer to process data. Generally speaking, these networks are trying to automatically extract feature sets from the data, and this is why, they are mostly used in data types where the feature selection process is difficult, such as when analyzing unstructured datasets, such as image data, videos, sound and text. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(929).png?raw=true)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah008BaB0uNr"
      },
      "source": [
        "###Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbyehggF0vOF"
      },
      "source": [
        ">Hello, and welcome! In this video, we will provide an overview of several deep neural network models, and their applications. To better understand Deep Learning, lets first take a look at different deep neural networks and their applications, namely:  Convolutional Neural Networks (or CNNs)  Recurrent Neural Networks (or RNNs)  Restricted Boltzmann Machines (or RBMs)  Deep Belief Networks (or DBNs), and finally  Autoencoders.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(930).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">**Convolutional Neural Network**  \r\n",
        "Lets start with the Convolutional Neural Network, and see how it helps us to do a task, such as image classification. Assume that you have a dataset made up of a great many photos of cats and dogs, and you want to build a model that can recognize and differentiate them. This model is supposed to look at this particular sample set of images and learn from them, toward becoming trained. Later, given an unseen image of either a cat or a dog, we should be able to use this trained model to recognize the image as being one or the other. Traditionally, your first step in building such a model would be feature extraction. That is, to choose the best features from your images, and then use those features in a classification algorithm, such as a shallow Neural Network. Ideally, the result would be a model that, upon analyzing a new image, could accurately distinguish the animal in that photo as being either a cat or a dog. There are countless features that could be extracted from the image, such as color, object edges, pixel location, and so on. Of course, the better that you can define the feature sets, the more accurate and efficient your image-classification will be. In fact, in the last two decades, there has been a lot of scientific research in the field of image processing, that is devoted to helping data scientists find the best feature sets from images, for the purposes of classification. However, as you can imagine, the process of selecting and using the best features is tremendously time-consuming and is often ineffective. Furthermore, extending the features to other types of images becomes an even greater challenge. Indeed, its easy to see how difficult it would be to apply the features weve used to discriminate cats and dogs, to other discrimination tasks, such as recognizing hand-written digits, for example. Therefore, the importance of feature selection cant be overstated. Enter convolutional neural networks. Suddenly, without having to find or select features, this network automatically and effectively finds the best features for you. So instead of you choosing what image features to use in classifying dogs vs. cats, Convolutional Neural Networks can automatically find those features and classify the images for you. So, we can say that a Convolutional Neural Network - or CNN for short - is a deep learning approach that learns directly from samples in a way that is much more effective than traditional Neural networks. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(931).png?raw=true)\r\n",
        "\r\n",
        ">CNNs achieve this type of automatic feature selection and classification through multiple specific layers of sophisticated mathematical operations. Through multiple layers, a CNN learns multiple levels of feature sets at different levels of abstraction. And this leads to very effective classification.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(932).png?raw=true)\r\n",
        "\r\n",
        ">CNNs have gained a lot of attention in the machine learning community over the last few years. This is due to the wide range of applications where CNNs excel, especially machine vision projects, including image recognition or classification, such as distinguishing animal photos or digit recognition, to skin cancer classification. CNNs are also used in object detection, for example real-time recognition of passengers in images captured by self-driving cars. Or coloring black and white images, and creating art images. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(933).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(935).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(936).png?raw=true)\r\n",
        "\r\n",
        ">**Recurrent Neural networks**   \r\n",
        "Now, lets look at Recurrent Neural networks and the types of situations in which they can be used, to solve a problem. A Recurrent Neural Network, or RNN for short, is a type of deep learning approach, that tries to solve the problem of modeling sequential data. Whenever the points in a dataset are dependent on the previous points, the data is said to be sequential. For example, a stock market price is a sequential type of data because the price of any given stock in tomorrows market, to a great extent, depends on its price today. As such, predicting the stock price tomorrow, is something that RNNs can be used for. We simply need to feed the network with the sequential data, it then maintains the context of the data and thus, learns the patterns within the data.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(938).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">We can also use RNNs for sentiment analysis. Lets say for example, youre scrolling through your product catalogue on a social network site and you see many comments related to a particular product of yours. Rather than reading through dozens and dozens of comments yourself and having to manually calculate if they were mostly positive, you can let an RNN do that for you. Indeed, an RNN can examine the sentiment of keywords in those reviews. Please remember, though, that the sequence of the words or sentences, as well as the context in which they are used, is very important as well. By feeding a sentence into an RNN, it takes all of this into account and determines if it the sentiment within it those product reviews are positive or negative. RNNs can also be used to predict the next word in a sentence. Im sure weve all seen how our mobile phone suggests words when were typing an email or a text. This is a type of language modeling within RNN, where the model has learned from a big textual corpus, and now can predict the next word in the sentence. As you can see, thinking in a sequential way, the word being suggested is very dependent on the previously typed words and the context of that message. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(939).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">When needing quick translation of certain words into another language, a great many people today, use the translation service of Google translator. We enter a sequence of words in English and it outputs a sequence of the words in French, as seen here. This type of text translation is another example of how RNNs can be used. This task is not based on a word-by-word translation and applying grammar rules. Instead, it is a probability model that has been trained on lots of data where the exact same text is translated into another language. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(940).png?raw=true)\r\n",
        "\r\n",
        ">Speech-to-text is yet another useful and increasingly common application of RNNs. In this case, the recognized voice is not only based on the word sound; RNNs also use the context around that sound to accurately recognize of the words being spoken into the devices microphone. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(941).png?raw=true)\r\n",
        "\r\n",
        ">**Restricted Boltzman Machine**\r\n",
        "Now, lets look at another type of neural network called a Restricted Boltzman Machine. Restricted Boltzman Machines, or RBMs, are used to find the patterns in data in an unsupervised manner. They are shallow neural nets that learn to reconstruct data by themselves. They are very important models, because they can automatically extract meaningful features from a given input, without the need to label them.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(942).png?raw=true)\r\n",
        "\r\n",
        ">RBMs might not be outstanding if you look at them as independent networks, but they are significant as building blocks of other networks, such as Deep Belief Networks. Essentially, RBMs are useful for unsupervised tasks such as:  feature extraction,  dimensionality reduction,  pattern recognition,  recommender systems,  handling missing values, and  topic modeling. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(944).png?raw=true)\r\n",
        "\r\n",
        ">**Deep Belief Networks**  \r\n",
        "Now lets look at Deep Belief Networks and see how they are built on top of RBMs. A Deep Belief Network is a network that was invented to solve an old problem in traditional artificial neural networks. Which problem? The back-propagation problem, that can often cause local minima or vanishing gradients issues in the learning process. A DBN is built to solve this by the stacking of multiple RBMs. So, what are the applications of DBNs? DBNs are generally used for classification -- same as traditional MLPs. So, one of the most important applications of DBNs is image recognition. The important part to remember, here, is that a DBN is a very accurate discriminative classifier. As such, we dont need a big set of labeled data to train a Deep Belief Network; in fact, a small set works fine because feature extraction is unsupervised by a stack of RBMs. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(945).png?raw=true)\r\n",
        "\r\n",
        ">**Autoencoders**  \r\n",
        "Now, lets look at Autoencoders. Much like RBMs, Autoencoders were invented to address the issue of extracting desirable features. And again, much like RBMs, Autoencoders try to recreate a given input, but do so with a slightly different network architecture and learning method. Autoencoders take a set of unlabeled inputs, encodes them into short codes, and then uses those to reconstruct the original image, while extracting the most valuable information from the data. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(946).png?raw=true)\r\n",
        "\r\n",
        ">What are the applications of Autoencoders? Well, Autoencoders are employed in some of the largest deep learning applications, especially for unsupervised tasks. As the encoder part of the network, Autoencoders compress data from the input layer into a short code -- a method that can be used for dimensionality reduction tasks. Also, in stacking multiple Autoencoder layers, the network learns multiple levels of representation at different levels of abstraction. For example, to detect a face in an image, the network encodes the primitive features, like the edges of a face. Then, the first layer's output goes to the second Autoencoder, to encode the less local features, like the nose, and so on. Therefore, it can be used for Feature Extraction and image recognition. By now, you should have a good sense of deep neural networks. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(947).png?raw=true)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crGfOJTTFCjl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqGL3W8CPlho"
      },
      "source": [
        "###LAB: TensorFlow - Hello World"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xn9BNldPPVL"
      },
      "source": [
        "<h2>TENSORFLOW'S HELLO WORLD</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. How does TensorFlow work?</h5>\n",
        "<h5> 2. Building a Graph. </h5>\n",
        "<h5> 3. Meaning of Tensor? </h5>\n",
        "<h5> 4. Defining multidimensional arrays using TensorFlow. </h5>\n",
        "<h5> 5. How TensorFlow handles Variables. </h5>\n",
        "<h5> 6. What are these Placeholders and what do they do? </h5>\n",
        "<h5> 7. Learn Operations using TensorFlow. </h5>     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ex3VuK6PPVN"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "<font size = 3><strong>In this notebook we will overview the basics of TensorFlow, learn it's structure and see what is the motivation to use it</strong></font>\n",
        "<br>\n",
        "<h2>Table of Contents</h2>\n",
        "<ol>\n",
        "    <li><a href=\"#ref2\">How does TensorFlow work?</a></li>\n",
        "    <li><a href=\"#ref3\">tf.function and AutoGraph</a></li>\n",
        "    <li><a href=\"#ref4\">Defining multidimensional arrays using TensorFlow</a></li>\n",
        "    <li><a href=\"#ref5\">Why Tensors?</a></li>\n",
        "    <li><a href=\"#ref6\">Variables</a></li>\n",
        "    <li><a href=\"#ref7\">Operations</a></li>\n",
        "</ol>\n",
        "<p></p>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCVZ1A_zPPVN"
      },
      "source": [
        "<a id=\"ref2\"></a>\n",
        "\n",
        "<h2>How does TensorFlow work?</h2>\n",
        "\n",
        "TensorFlow defines computations as Graphs, and these are made with operations (also know as ops). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.\n",
        "\n",
        "For example, the image below represents a graph in TensorFlow. _W_, _x_ and _b_ are tensors over the edges of this graph. _MatMul_ is an operation over the tensors _W_ and _x_, after that _Add_ is called and add the result of the previous operator with _b_. The resultant tensors of each operation cross the next one until the end where it's possible to get the wanted result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVUCbQlGPPVP"
      },
      "source": [
        "<img src='https://ibm.box.com/shared/static/a94cgezzwbkrq02jzfjjljrcaozu5s2q.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH_896kOPPVQ"
      },
      "source": [
        "With TensorFlow 2.x, **Eager Execution** is enabled by default. This allows TensorFlow code to be executed and evaluated line by line. Before version 2.x was released, every graph had to be run wihthin a TensorFlow session. This only allowed for the entire graph to be run all at once. This would make debugging the computation graph each time. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn1cAO58PPVQ"
      },
      "source": [
        "<h2>Installing TensorFlow </h2>\n",
        "\n",
        "We begin by installing TensorFlow version 2.2.0 and its required prerequistes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heOHmU43PwER"
      },
      "source": [
        "!pip install -q grpcio\r\n",
        "!pip install -q tensorflow "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_w5e2ynPPVS"
      },
      "source": [
        "<h2>Importing TensorFlow</h2>\n",
        "<p>To use TensorFlow, we need to import the library. We imported it and optionally gave it the name \"tf\", so the modules can be accessed by <b>tf.module-name</b>:</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gr249vjJUUep",
        "outputId": "f8fe0a0d-0585-4739-f8ae-0bf1ad318303"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55uJ5h0JPPVS"
      },
      "source": [
        "IMPORTANT! => Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Outout\" and wait until all output disapears. Then your changes are beeing picked up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-8DdkTPPVT"
      },
      "source": [
        "<a id=\"ref3\"></a>\n",
        "\n",
        "#### tf.function and AutoGraph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcUgzX15PPVT"
      },
      "source": [
        "Now we call the TensorFlow functions that construct new <b>tf.Operation</b> and <b>tf.Tensor</b> objects. As mentioned, each <b>tf.Operation</b> is a <b>node</b> and each <b>tf.Tensor</b> is an edge in the graph.\n",
        "\n",
        "Lets add 2 constants to our graph. For example, calling tf.constant([2], name = 'constant_a') adds a single <b>tf.Operation</b> to the default graph. This operation produces the value 2, and returns a <b>tf.Tensor</b> that represents the value of the constant.  \n",
        "<b>Notice:</b> tf.constant([2], name=\"constant_a\") creates a new tf.Operation named \"constant_a\" and returns a tf.Tensor named \"constant_a:0\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkyMMzjXVigm"
      },
      "source": [
        "a = tf.constant([2], name =\"constant_a\")\r\n",
        "b = tf.constant([3], name = \"constant_b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-OvuPP9PPVU"
      },
      "source": [
        "Lets look at the tensor **a**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_8fF-NoXKyv",
        "outputId": "86352faf-2f1d-47ce-d4be-de642ec02dbd"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V8PVQBDXLRU",
        "outputId": "6d503478-de0f-4123-a3be-e113e2c4ed29"
      },
      "source": [
        "tf.print(a.numpy()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GekUtperPPVV"
      },
      "source": [
        "Annotating the python functions with **tf.function** uses TensorFlow Autograph to create a TensorFlow static execution graph for the function.   tf.function annotation tells TensorFlow Autograph to transform function _add_ into TensorFlow control flow, which then defines the TensorFlow static execution graph. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQG7iBXqZ8eV"
      },
      "source": [
        "@tf.function\r\n",
        "def add(a,b):\r\n",
        "  c = a+b\r\n",
        "  print(c)\r\n",
        "  return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMLmOXzQaJFZ",
        "outputId": "e3922429-8207-473d-dccb-3075d037336a"
      },
      "source": [
        "result = add(a,b)\r\n",
        "tf.print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"add:0\", shape=(1,), dtype=int32)\n",
            "[5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n61uN_qOaQUN",
        "outputId": "0b8bd502-04e9-4e7e-8c76-2b1b8987bf22"
      },
      "source": [
        "tf.print(result[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644U5vHyPPVV"
      },
      "source": [
        "Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your operations (In this case our constants and _tf.add_), define a Python function named _add_ and decorate it with using the _tf.function_ annotator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3l_UlbGPPVW"
      },
      "source": [
        "<h3>What is the meaning of Tensor?</h3>\n",
        "\n",
        "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
        "<font size = 3><strong>In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.</strong></font>\n",
        "<br>\n",
        "<br>\n",
        "    The word <b>tensor</b> from new latin means \"that which stretches\". It is a mathematical object that is named \"tensor\" because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays. \n",
        "\n",
        "</div>\n",
        "\n",
        "That's great, but... what are these multidimensional arrays? \n",
        "\n",
        "Going back a little bit to physics to understand the concept of dimensions:<br>\n",
        "<img src=\"https://ibm.box.com/shared/static/ymn0hl3hf8s3xb4k15v22y5vmuodnue1.svg\"/>\n",
        "\n",
        "<div style=\"text-align:center\"><a href=\"https://en.wikipedia.org/wiki/Dimension\">Image Source</a></div>\n",
        "<br>\n",
        "\n",
        "The zero dimension can be seen as a point, a single object or a single item.\n",
        "\n",
        "The first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements.\n",
        "\n",
        "The second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. \n",
        "\n",
        "The third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line.\n",
        "\n",
        "The Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4chMsgpPPVW"
      },
      "source": [
        "As mathematical objects: <br><br>\n",
        "<img src=\"https://ibm.box.com/shared/static/kmxz570uai8eeg6i6ynqdz6kmlx1m422.png\">\n",
        "\n",
        "<div style=\"text-align: center\"><a href=\"https://book.mql4.com/variables/arrays\">Image Source</a></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7DJNcdUPPVW"
      },
      "source": [
        "Summarizing:<br><br>\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td><b>Dimension</b></td>\n",
        "    <td><b>Physical Representation</b></td> \n",
        "    <td><b>Mathematical Object</b></td>\n",
        "    <td><b>In Code</b></td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>Zero </td>\n",
        "    <td>Point</td> \n",
        "    <td>Scalar (Single Number)</td>\n",
        "    <td>[ 1 ]</td>\n",
        "  </tr>\n",
        "\n",
        "  <tr>\n",
        "    <td>One</td>\n",
        "    <td>Line</td> \n",
        "    <td>Vector (Series of Numbers) </td>\n",
        "    <td>[ 1,2,3,4,... ]</td>\n",
        "  </tr>\n",
        "  \n",
        "   <tr>\n",
        "    <td>Two</td>\n",
        "    <td>Surface</td> \n",
        "    <td>Matrix (Table of Numbers)</td>\n",
        "       <td>[ [1,2,3,4,...], [1,2,3,4,...], [1,2,3,4,...],... ]</td>\n",
        "  </tr>\n",
        "  \n",
        "   <tr>\n",
        "    <td>Three</td>\n",
        "    <td>Volume</td> \n",
        "    <td>Tensor (Cube of Numbers)</td>\n",
        "    <td>[ [[1,2,...], [1,2,...], [1,2,...],...], [[1,2,...], [1,2,...], [1,2,...],...], [[1,2,...], [1,2,...], [1,2,...] ,...]... ]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAKVlkxoPPVX"
      },
      "source": [
        "* * *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LthigEqyPPVX"
      },
      "source": [
        "<a id=\"ref4\"></a>\n",
        "\n",
        "####<h2>Defining multidimensional arrays using TensorFlow</h2>\n",
        "Now we will try to define such arrays using TensorFlow:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAiQfv10aXoV"
      },
      "source": [
        "scalar = tf.constant(2)       \r\n",
        "vector = tf.constant([2,3,4])\r\n",
        "matrix = tf.constant([[2,3,4],[4,5,6],[7,8,9]])\r\n",
        "tensor = tf.constant([ [[2,3,4],[4,5,6],[7,8,9]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNyFsuIUd2rl",
        "outputId": "004efb37-cd8b-44e9-a8f0-6546571d88f4"
      },
      "source": [
        "print (\"scalar (1 entry):\\n %s \\n\" % scalar)\r\n",
        "\r\n",
        "print (\"vector (3 entries) :\\n %s \\n\" % vector)\r\n",
        "\r\n",
        "print (\"matrix (3x3 entries):\\n %s \\n\" % matrix)\r\n",
        "\r\n",
        "print (\"tensor (3x3x3 entries) :\\n %s \\n\" % tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scalar (1 entry):\n",
            " tf.Tensor(2, shape=(), dtype=int32) \n",
            "\n",
            "vector (3 entries) :\n",
            " tf.Tensor([2 3 4], shape=(3,), dtype=int32) \n",
            "\n",
            "matrix (3x3 entries):\n",
            " tf.Tensor(\n",
            "[[2 3 4]\n",
            " [4 5 6]\n",
            " [7 8 9]], shape=(3, 3), dtype=int32) \n",
            "\n",
            "tensor (3x3x3 entries) :\n",
            " tf.Tensor(\n",
            "[[[ 2  3  4]\n",
            "  [ 4  5  6]\n",
            "  [ 7  8  9]]\n",
            "\n",
            " [[ 4  5  6]\n",
            "  [ 5  6  7]\n",
            "  [ 6  7  8]]\n",
            "\n",
            " [[ 7  8  9]\n",
            "  [ 8  9 10]\n",
            "  [ 9 10 11]]], shape=(3, 3, 3), dtype=int32) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWHN8rp1PPVY"
      },
      "source": [
        "<b>tf.shape</b> returns the shape of our data structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAIMZ8S5eUdm",
        "outputId": "01efa9c6-2512-4be5-a666-b6421df575d8"
      },
      "source": [
        "scalar.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAtL9SgFeXRp",
        "outputId": "2da034cb-7c8a-4bc1-8efd-50a606a8d7cc"
      },
      "source": [
        "tensor.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESrW7BEhPPVY"
      },
      "source": [
        "Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORE5FHj2eYbU",
        "outputId": "f988489f-79b2-4a93-9fbe-9ecda9ba4d7d"
      },
      "source": [
        "matrix1 = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\r\n",
        "matrix2 = tf.constant([[2,2,2],[2,2,2],[2,2,2]])\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def add():\r\n",
        "    add1_op = tf.add(matrix1,matrix2)\r\n",
        "    return add1_op\r\n",
        "\r\n",
        "\r\n",
        "print(\"Defined using tensor flow function:\")\r\n",
        "add1_op = add()\r\n",
        "print(add1_op)\r\n",
        "print (\"Defined using normal expressions :\")\r\n",
        "add2_op = matrix1 + matrix2\r\n",
        "print(add2_op)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defined using tensor flow function:\n",
            "tf.Tensor(\n",
            "[[3 4 5]\n",
            " [4 5 6]\n",
            " [5 6 7]], shape=(3, 3), dtype=int32)\n",
            "Defined using normal expressions :\n",
            "tf.Tensor(\n",
            "[[3 4 5]\n",
            " [4 5 6]\n",
            " [5 6 7]], shape=(3, 3), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKdwY2ilPPVZ"
      },
      "source": [
        "With the regular symbol definition and also the TensorFlow function we were able to get an element-wise multiplication, also known as Hadamard product. <br>\n",
        "\n",
        "But what if we want the regular matrix product?\n",
        "\n",
        "We then need to use another TensorFlow function called <b>tf.matmul()<b>:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ymVwC1mgAtb",
        "outputId": "8e6e28c3-b865-4758-ab9b-9b2bdac833bb"
      },
      "source": [
        "mat1 = tf.constant([[2,3],[3,4]])\r\n",
        "mat2 = tf.constant([[2,3],[3,4]])\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def matrixmul():     #matrix multuplication\r\n",
        "    return tf.matmul(mat1,mat2)\r\n",
        "\r\n",
        "mul_op = matrixmul()\r\n",
        "print(\"Defined using tensor flow function:\")\r\n",
        "print(mul_op)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defined using tensor flow function:\n",
            "tf.Tensor(\n",
            "[[13 18]\n",
            " [18 25]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w_yzN3VPPVa"
      },
      "source": [
        "We could also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiXbB5_dPPVa"
      },
      "source": [
        "* * *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pezk9COgPPVb"
      },
      "source": [
        "<a id=\"ref5\"></a>\n",
        "\n",
        "<h2>Why Tensors?</h2>\n",
        "\n",
        "The Tensor structure helps us by giving the freedom to shape the dataset in the way we want.\n",
        "\n",
        "And it is particularly helpful when dealing with images, due to the nature of how information in images are encoded,\n",
        "\n",
        "Thinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional structure (a matrix)... until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particularly helpful.\n",
        "\n",
        "Images are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity(0-255) of each channel color into the width and height of the image, just like this:\n",
        "\n",
        "<img src='https://ibm.box.com/shared/static/xlpv9h5xws248c09k1rlx7cer69y4grh.png'>\n",
        "<a href=\"https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131.aspx\">Image Source</a>\n",
        "\n",
        "So the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0CGuZ7uPPVd"
      },
      "source": [
        "* * *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29-QxWGWPPVd"
      },
      "source": [
        "<a id=\"ref6\"></a>\n",
        "\n",
        "#### Variables\n",
        "\n",
        "Now that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables.\n",
        "<b>First of all, having tensors, why do we need variables?</b>  \n",
        "TensorFlow variables are used to share and persist some stats that are manipulated by our program. That is, when you define a variable, TensorFlow adds a <b>tf.Operation</b> to your graph. Then, this operation will store a writable tensor value. So, you can update the value of a variable through each run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL03FKQfPPVe"
      },
      "source": [
        "Let's first create a simple counter, by first initializing a variable _v_ that will be increased one unit at a time:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rNxEAOiM4A"
      },
      "source": [
        "v = tf.Variable(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04S8VRTyPPVg"
      },
      "source": [
        "We now create a python method _increment_by_one_. This method will internally call _td.add_ that takes in two arguments, the <b>reference_variable</b> to update, and assign it to the <b>value_to_update</b> it by.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2eF5cORiZIl"
      },
      "source": [
        "@tf.function\r\n",
        "\r\n",
        "def increment1(v):\r\n",
        "   v = tf.add(v,1)\r\n",
        "   return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gQQGchkPPVg"
      },
      "source": [
        "To update the value of the variable _v_, we simply call the _increment_by_one_ method and pass the variable to it. We will invoke this method thrice. This method will increment the variable by one and print the updated value each time. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ip6tNWgkK-J",
        "outputId": "0dcdc521-397b-4feb-9725-2b11cead7e73"
      },
      "source": [
        "for i in range(3):\r\n",
        "  v = increment1(v)\r\n",
        "  print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7M_CX6PPVh"
      },
      "source": [
        "<a id=\"ref7\"></a>\n",
        "\n",
        "####<h2>Operations</h2>\n",
        "\n",
        "Operations are nodes that represent the mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensor or maybe an activation function.\n",
        "\n",
        "<b>tf.constant</b>, <b>tf.matmul</b>, <b>tf.add</b>, <b>tf.nn.sigmoid</b> are some of the operations in TensorFlow. These are like functions in python but operate directly over tensors and each one does a specific thing. \n",
        "\n",
        "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">Other operations can be easily found in: <a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html\">https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html</a></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9LiaW7ikVfa",
        "outputId": "bd464f17-24b8-4385-8d98-0dcfed82c2c1"
      },
      "source": [
        "a = tf.constant(5)\r\n",
        "b = tf.constant(2)\r\n",
        "\r\n",
        "c = tf.add(a,b)\r\n",
        "d = tf.subtract(a,b)\r\n",
        "\r\n",
        "print(c, \"\\n\" ,d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int32) \n",
            " tf.Tensor(3, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToN8xUmPPVi"
      },
      "source": [
        "<b>tf.nn.sigmoid</b> is an activation function, it's a little more complicated, but this function helps learning models to evaluate what kind of information is good or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPweRlfwPPVi"
      },
      "source": [
        "Want to learn more?\n",
        "\n",
        "Running deep learning programs usually needs a high performance platform. **PowerAI** speeds up deep learning and AI. Built on IBMs Power Systems, **PowerAI** is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The **PowerAI** platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IMB Cloud](https://cocl.us/ML0120EN_PAI).\n",
        "\n",
        "Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets.**Watson Studio** is IBMs leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgwa9wMGnpVD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7-ZM0XoF1D"
      },
      "source": [
        "### LAB: TensorFlow 2.x and Eager Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mI--ERFuGZ4"
      },
      "source": [
        "Objectives  \r\n",
        "After completing this lab you will be able to:\r\n",
        "\r\n",
        "- Understand the impact of eager execution and the need to enable it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37KuWwQeoAfs"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "<font size = 3><strong>In this notebook we will overview Eager Execution in  TensorFlow 2.x</strong></font>\n",
        "<br>\n",
        "<h2>Table of Contents</h2>\n",
        "<ol>\n",
        "    <li>Instructions</li>\n",
        "    <li>Eager Execution</li>\n",
        "    <li>Tensorflow Operations Without Eager Execution Mode</li>\n",
        "    <li>Tensorflow Operations With Eager Execution Mode</li>\n",
        "    <li>Dynamic Control Flow</li>\n",
        "    <li>Thank You</li>\n",
        "</ol>\n",
        "<p></p>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c4KAku0oAft"
      },
      "source": [
        "Instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB4EsYuCoAft"
      },
      "source": [
        "#### Installing TensorFlow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPg99vkAoAfu"
      },
      "source": [
        "We begin by installing TensorFlow version 2.2.0 and its required prerequistes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV09OdneoLLn"
      },
      "source": [
        "!pip install -q grpcio\r\n",
        "!pip install -q tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3GG28SFuXL6",
        "outputId": "a0a39884-647e-4257-afff-b98fca06ad13"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx1rOyXpoAfw"
      },
      "source": [
        "#### Eager Execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzOgRI5QoAfw"
      },
      "source": [
        "TensorFlow's **eager execution** is an imperative programming environment that evaluates operations immediately, without building graphs, operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6V7ZAmoAfx"
      },
      "source": [
        "With **TensorFlow 2.x**, **Eager Execution is enabled by default**. This allows TensorFlow code to be executed and evaluated line by line. Before version 2.x was released, every graph had to be run wihthin a TensorFlow **session**. This only allowed for the entire graph to be run all at once. This made it hard to debug the computation graph. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3hmwSjLoAfx"
      },
      "source": [
        "Eager execution is a flexible machine learning platform for research and experimentation, providing:\n",
        "\n",
        "-   **An intuitive interface**-Structure your code naturally and use Python data structures. Quickly iterate on small models and   small data.\n",
        "\n",
        "\n",
        "-   **Easier debugging**- Execute operations directly to inspect code line by line and test changes. Use standard Python debugging tools for immediate error reporting.\n",
        "\n",
        "\n",
        "-   **Natural control flow**Use Python control flow instead of graph control flow, simplifying the specification of dynamic models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_jzJAmoAfx"
      },
      "source": [
        "As I mentioned above, in **Tensorflow 2.x**, eager execution is enabled by default. To verify that please run the below code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6XdCANCucOq",
        "outputId": "6b4342ef-f2d2-45ca-c3ec-991300e6a96b"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCHCyLoQoAfy"
      },
      "source": [
        "Now you can run TensorFlow operations and the results will return immediately:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LNibMXHoAfy"
      },
      "source": [
        "But first let me show you how things get done without the eager execution in tensorflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHSze45AoAfz"
      },
      "source": [
        "#### Tensorflow Operations Without Eager Execution Mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vuFOiEJoAfz"
      },
      "source": [
        "So, there is a **disable_eager_execution()** function in TensorFlow 2.x. You can call the function like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXgOBk5UoAfz"
      },
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDdjQ9rfoAfz"
      },
      "source": [
        "#### Note: This function can only be called at the beginning before any Graphs, Ops, or Tensors have been  created.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uCZmAlyoAf0"
      },
      "source": [
        "Now, verify that the eager execution is disabled or not by running the below code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCh4RVduoAf0",
        "outputId": "24b2de3e-ef21-4b6d-b916-32a376da8e88"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf2izJz3oAf0"
      },
      "source": [
        "As you can see **False** in the output that means it is disabled now.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUUH4Q8qoAf0"
      },
      "source": [
        "Just execute the next cell. You will notice that we've created an object **a** of type **tensorflow.python.framework.ops.Tensor**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWcgw5y4oAf1",
        "outputId": "52c7b5e2-2050-47cc-86b3-77a6c663bfba"
      },
      "source": [
        "import numpy as np\n",
        "a = tf.constant(np.array([1., 2., 3.]))\n",
        "type(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3MenArwoAf2"
      },
      "source": [
        "Let's create another Tensor **b** and apply the dot product between them. This gives us **c**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emv_YCI5oAf3",
        "outputId": "379e43e0-e41b-4d05-b5dc-2878da33e36d"
      },
      "source": [
        "b = tf.constant(np.array([4.,5.,6.]))\n",
        "c = tf.tensordot(a, b, 1)\n",
        "type(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjYkWulwoAf4",
        "outputId": "7bc42b2a-f921-4a7c-ab9e-8874a75468ba"
      },
      "source": [
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Tensordot:0\", shape=(), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W854ZOUXoAf4"
      },
      "source": [
        "Note that **c** is a **tensorflow.python.framework.ops.Tensor** as well. So any node of the execution graph resembles a Tensor type. **But so far not a single computation happened**. You need to execute the graph. You can pass any graph or subgraph to the TensorFlow runtime for execution. Each TensorFlow graph runs within a TensorFlow Session, therefore we need to create it first:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc_TBAOoAf5"
      },
      "source": [
        "**Note:** Session can be accessed via **tf.compat.v1.Session()** in Tensorflow 2.x.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7DF1aJ_oAf6",
        "outputId": "332a93e4-7fa3-4aeb-e62c-4946d4080d56"
      },
      "source": [
        "session = tf.compat.v1.Session()\n",
        "output = session.run(c)\n",
        "session.close()\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGQ1XgA5oAf6"
      },
      "source": [
        "Now you see the correct result of 32. But the problem is that debugging is pretty hard if you can only run complete graphs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldby3uF8oAf7"
      },
      "source": [
        "So let's actually re-enable the **eager execution**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLX4rRDqoAf7"
      },
      "source": [
        "#### Tensorflow Operations With Eager Execution Mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWH94MTjoAf8"
      },
      "source": [
        "#### IMPORTANT! => Please don't forget restart the kernel by clicking on \"Kernel\"->\"Restart\" so that the changes take effect.\n",
        "\n",
        "**Enable or Disable Eager execution has to happen on program startup. This is the reason we have to restart the kernel.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxjhd_OdoAf9"
      },
      "source": [
        "Import the required libraries again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqAU5PmXoAf9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijCBpcmWoAf-"
      },
      "source": [
        "Run the below command to re-enable the eager execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-51u1y-oAf-"
      },
      "source": [
        "from tensorflow.python.framework.ops import enable_eager_execution\n",
        "enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0KSPPG1oAf_"
      },
      "source": [
        "Now you can run TensorFlow operations and the results will return immediately:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mu6ImDjoAgA",
        "outputId": "28672d25-6945-4c87-83d2-434fb54fb50b"
      },
      "source": [
        "x = [[4]]\n",
        "m = tf.matmul(x, x)\n",
        "print(\"Result, {}\".format(m))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result, [[16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OygdAIDcoAgA"
      },
      "source": [
        "Enabling eager execution changes how TensorFlow operations behavenow they immediately evaluate and return their values to Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KZiJNj2oAgB"
      },
      "source": [
        "Since there isn't a computational graph to build and run later in a session, it's easy to inspect results using print() or a debugger.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOGi0v3NoAgB",
        "outputId": "e45afd07-0d29-489c-e448-4334ff297e9b"
      },
      "source": [
        "a = tf.constant(np.array([1., 2., 3.]))\n",
        "type(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTun2FhIoAgC"
      },
      "source": [
        "So the very same code created a different type of object. So now **a** is of type **tensorflow.python.framework.ops.EagerTensor**. This is great, because without changing code we obtain a tensor object which allows us to have a look inside, without execting a graph in a session:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t72abVVPoAgD",
        "outputId": "c687d65c-a1cc-4a38-e5fb-fa6c7e7b6ad2"
      },
      "source": [
        "print(a.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 2. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_wAjrvDoAgD"
      },
      "source": [
        "Isn't this amazing? So from now on we can threat Tensors like ordinary python objects, work with them as usual, insert debug statements at any point or even use a debugger. So let's continue this example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_UqSFYNoAgE",
        "outputId": "cf2c9437-ab3f-4fa8-8f6f-af777679d5c8"
      },
      "source": [
        "b = tf.constant(np.array([4.,5.,6.]))\n",
        "c = tf.tensordot(a, b,1)\n",
        "type(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EySuUahjoAgF"
      },
      "source": [
        "Again, **c** is an **tensorflow.python.framework.ops.EagerTensor** object which can be directly read:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WYKJG4foAgF",
        "outputId": "eb2d45a1-27c0-4799-a558-256590a0482a"
      },
      "source": [
        "print(c.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7wpW6xsoAgG"
      },
      "source": [
        "Without creating a session or a graph we obtained the result of the defined computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhr3B--poAgH"
      },
      "source": [
        "#### Dynamic Control Flow\n",
        "\n",
        "A major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write [fizzbuzz](https://en.wikipedia.org/wiki/Fizz_buzz?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejj8_2Dmu-Z2"
      },
      "source": [
        "def fizzbuzz(max_num):\r\n",
        "  \r\n",
        "  max_num = tf.constant(max_num)\r\n",
        "\r\n",
        "  for i in range (1,max_num.numpy()+1):\r\n",
        "    i = tf.constant(i)\r\n",
        "    if int(i%3) == 0 and int(i%5) == 0:\r\n",
        "      print(\"fizzbuzz\")\r\n",
        "    elif int(i%3) == 0:\r\n",
        "        print(\"fizz\")\r\n",
        "    elif int(i%5) == 0:\r\n",
        "        print(\"buzz\")\r\n",
        "    else:\r\n",
        "        print(i.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkWY9q_9ymvc",
        "outputId": "2abd19bf-6e30-4fc5-ae20-9d7dcde2a2b7"
      },
      "source": [
        "fizzbuzz(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "fizz\n",
            "4\n",
            "buzz\n",
            "fizz\n",
            "7\n",
            "8\n",
            "fizz\n",
            "buzz\n",
            "11\n",
            "fizz\n",
            "13\n",
            "14\n",
            "fizzbuzz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3fC4ecsoAgK"
      },
      "source": [
        "It prints these values at runtime. It behaves just like any other Python code. It is direct and intuitive. We can use pure Python if, while, and for in the control flow. So that's it for now, stay tuned and have fun.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Iv8k-rzZg2"
      },
      "source": [
        "###LAB: Linear Regression with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6gEq2WazXQV"
      },
      "source": [
        "<h2>LINEAR REGRESSION WITH TENSORFLOW</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. What is Linear Regression</h5>\n",
        "<h5> 2. Linear Regression with TensorFlow. </h5>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRs73Oo4zXQW"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "<font size=\"3\"><strong>In this notebook we will overview the implementation of Linear Regression with TensorFlow</strong></font>\n",
        "<br>\n",
        "<br>\n",
        "<h2>Table of Contents</h2>\n",
        "<ol>\n",
        " <li><a href=\"#ref1\">Linear Regression</a></li>\n",
        " <li><a href=\"#ref2\">Linear Regression with TensorFlow</a></li>\n",
        "</ol>\n",
        "</div>\n",
        "<br>\n",
        "<br>\n",
        "<p></p>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W64JwHXzXQW"
      },
      "source": [
        "<a id=\"ref1\"></a>\n",
        "\n",
        "<h1>Linear Regression</h1>\n",
        "\n",
        "Defining a linear regression in simple terms, is the approximation of a linear model used to describe the relationship between two or more variables. In a simple linear regression there are two variables, the dependent variable, which can be seen as the \"state\" or \"final goal\" that we study and try to predict, and the independent variables, also known as explanatory variables, which can be seen as the \"causes\" of the \"states\". \n",
        "\n",
        "When more than one independent variable is present the process is called multiple linear regression. <br>\n",
        "When multiple dependent variables are predicted the process is known as multivariate linear regression.\n",
        "\n",
        "The equation of a simple linear model is\n",
        "\n",
        "$$Y = a X + b $$\n",
        "\n",
        "Where Y is the dependent variable and X is the independent variable, and <b>a</b> and <b>b</b> being the parameters we adjust. <b>a</b> is known as \"slope\" or \"gradient\" and <b>b</b> is the \"intercept\". You can interpret this equation as Y being a function of X, or Y being dependent on X.\n",
        "\n",
        "If you plot the model, you will see it is a line, and by adjusting the \"slope\" parameter you will change the angle between the line and the independent variable axis, and the \"intercept parameter\" will affect where it crosses the dependent variable's axis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLiWyi0-zXQX"
      },
      "source": [
        "We begin by installing TensorFlow version 2.2.0 and its required prerequistes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK2QtPr5zXQX"
      },
      "source": [
        "!pip install -q  grpcio\n",
        "!pip install -q tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNsk3OhvzXQY"
      },
      "source": [
        "**Restart kernel for latest version of TensorFlow to be activated**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPipyEIzzXQY"
      },
      "source": [
        "Next, let's first import the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbYR3In4zXQY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pylab as pl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUaikks3zXQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37f03ea-735a-4bf5-f33b-320c7ea57200"
      },
      "source": [
        "print(tf.__version__)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOTk5aXVzXQZ"
      },
      "source": [
        "IMPORTANT! => Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Outout\" and wait until all output disapears. Then your changes are beeing picked up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw6LNQjFzXQZ"
      },
      "source": [
        "Let's define the independent variable:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hYI5MnIypcg"
      },
      "source": [
        "x = np.arange(0,5,0.1)\r\n",
        "a=1\r\n",
        "b=0\r\n",
        "y = a*x+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "rTZxzoll0pJr",
        "outputId": "cf45aea6-00b7-4c50-b6f8-a3bef61fcf38"
      },
      "source": [
        "plt.plot(x,y)\r\n",
        "plt.ylabel('Dependent Variable')\r\n",
        "plt.xlabel('Indepdendent Variable')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAO3IcQozXQa"
      },
      "source": [
        "OK... but how can we see this concept of linear relations with a more meaningful point of view?\n",
        "\n",
        "Simple linear relations were used to try to describe and quantify many observable physical phenomena, the easiest to understand are speed and distance traveled:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPpy0VBXzXQa"
      },
      "source": [
        "$$Distance Traveled = Speed \\\\times Time + Initial Distance$$\n",
        "\n",
        "$$Speed = Acceleration \\\\times Time + Initial Speed$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfKb_dmYzXQa"
      },
      "source": [
        "They are also used to describe properties of different materials:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPe-DcgKzXQb"
      },
      "source": [
        "$$Force = Deformation \\\\times Stiffness$$\n",
        "\n",
        "$$Heat Transfered = Temperature Difference \\\\times Thermal Conductivity$$\n",
        "\n",
        "$$Electrical Tension (Voltage) = Electrical Current \\\\times Resistance$$\n",
        "\n",
        "$$Mass =  Volume \\\\times Density$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-3VTJmazXQb"
      },
      "source": [
        "When we perform an experiment and gather the data, or if we already have a dataset and we want to perform a linear regression, what we will do is adjust a simple linear model to the dataset, we adjust the \"slope\" and \"intercept\" parameters to the data the best way possible, because the closer the model comes to describing each ocurrence, the better it will be at representing them.\n",
        "\n",
        "So how is this \"regression\" performed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So3IhiHlzXQb"
      },
      "source": [
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maZcO8s_zXQb"
      },
      "source": [
        "<a id=\"ref2\"></a>\n",
        "\n",
        "<h1>Linear Regression with TensorFlow</h1>\n",
        "A simple example of a linear function can help us understand the basic mechanism behind TensorFlow.\n",
        "\n",
        "For the first part we will use a sample dataset, and then we'll use TensorFlow to adjust and get the right parameters. We download a dataset that is related to fuel consumption and Carbon dioxide emission of cars. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioVmhLYA0sMh",
        "outputId": "f3898c2e-d1fa-46c5-8008-3bf42989236e"
      },
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-24 12:58:11--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72629 (71K) [text/csv]\n",
            "Saving to: FuelConsumptionCo2.csv\n",
            "\n",
            "FuelConsumptionCo2. 100%[===================>]  70.93K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-12-24 12:58:12 (2.72 MB/s) - FuelConsumptionCo2.csv saved [72629/72629]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F44kaj4rzXQc"
      },
      "source": [
        "<h2>Understanding the Data</h2>\n",
        "\n",
        "<h3><code>FuelConsumption.csv</code>:</h3>\n",
        "We have downloaded a fuel consumption dataset, <b><code>FuelConsumption.csv</code></b>, which contains model-specific fuel consumption ratings and estimated carbon dioxide emissions for new light-duty vehicles for retail sale in Canada. <a href=\"http://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64\">Dataset source</a>\n",
        "\n",
        "-   **MODELYEAR** e.g. 2014\n",
        "-   **MAKE** e.g. Acura\n",
        "-   **MODEL** e.g. ILX\n",
        "-   **VEHICLE CLASS** e.g. SUV\n",
        "-   **ENGINE SIZE** e.g. 4.7\n",
        "-   **CYLINDERS** e.g 6\n",
        "-   **TRANSMISSION** e.g. A6\n",
        "-   **FUEL CONSUMPTION in CITY(L/100 km)** e.g. 9.9\n",
        "-   **FUEL CONSUMPTION in HWY (L/100 km)** e.g. 8.9\n",
        "-   **FUEL CONSUMPTION COMB (L/100 km)** e.g. 9.2\n",
        "-   **CO2 EMISSIONS (g/km)** e.g. 182   --> low --> 0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "Tkgs-RlX1Gop",
        "outputId": "5b2a795d-134d-4e85-f687-1c1d123d553e"
      },
      "source": [
        "df = pd.read_csv(\"FuelConsumptionCo2.csv\")\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MODELYEAR</th>\n",
              "      <th>MAKE</th>\n",
              "      <th>MODEL</th>\n",
              "      <th>VEHICLECLASS</th>\n",
              "      <th>ENGINESIZE</th>\n",
              "      <th>CYLINDERS</th>\n",
              "      <th>TRANSMISSION</th>\n",
              "      <th>FUELTYPE</th>\n",
              "      <th>FUELCONSUMPTION_CITY</th>\n",
              "      <th>FUELCONSUMPTION_HWY</th>\n",
              "      <th>FUELCONSUMPTION_COMB</th>\n",
              "      <th>FUELCONSUMPTION_COMB_MPG</th>\n",
              "      <th>CO2EMISSIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>ACURA</td>\n",
              "      <td>ILX</td>\n",
              "      <td>COMPACT</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "      <td>AS5</td>\n",
              "      <td>Z</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>8.5</td>\n",
              "      <td>33</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014</td>\n",
              "      <td>ACURA</td>\n",
              "      <td>ILX</td>\n",
              "      <td>COMPACT</td>\n",
              "      <td>2.4</td>\n",
              "      <td>4</td>\n",
              "      <td>M6</td>\n",
              "      <td>Z</td>\n",
              "      <td>11.2</td>\n",
              "      <td>7.7</td>\n",
              "      <td>9.6</td>\n",
              "      <td>29</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014</td>\n",
              "      <td>ACURA</td>\n",
              "      <td>ILX HYBRID</td>\n",
              "      <td>COMPACT</td>\n",
              "      <td>1.5</td>\n",
              "      <td>4</td>\n",
              "      <td>AV7</td>\n",
              "      <td>Z</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>5.9</td>\n",
              "      <td>48</td>\n",
              "      <td>136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014</td>\n",
              "      <td>ACURA</td>\n",
              "      <td>MDX 4WD</td>\n",
              "      <td>SUV - SMALL</td>\n",
              "      <td>3.5</td>\n",
              "      <td>6</td>\n",
              "      <td>AS6</td>\n",
              "      <td>Z</td>\n",
              "      <td>12.7</td>\n",
              "      <td>9.1</td>\n",
              "      <td>11.1</td>\n",
              "      <td>25</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>ACURA</td>\n",
              "      <td>RDX AWD</td>\n",
              "      <td>SUV - SMALL</td>\n",
              "      <td>3.5</td>\n",
              "      <td>6</td>\n",
              "      <td>AS6</td>\n",
              "      <td>Z</td>\n",
              "      <td>12.1</td>\n",
              "      <td>8.7</td>\n",
              "      <td>10.6</td>\n",
              "      <td>27</td>\n",
              "      <td>244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MODELYEAR   MAKE  ... FUELCONSUMPTION_COMB_MPG CO2EMISSIONS\n",
              "0       2014  ACURA  ...                       33          196\n",
              "1       2014  ACURA  ...                       29          221\n",
              "2       2014  ACURA  ...                       48          136\n",
              "3       2014  ACURA  ...                       25          255\n",
              "4       2014  ACURA  ...                       27          244\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNbEFWGMzXQd"
      },
      "source": [
        "Lets say we want to use linear regression to predict Co2Emission of cars based on their engine size. So, lets define X and Y value for the linear regression, that is, train_x and train_y:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9aBKyfY1MS1"
      },
      "source": [
        "xtr = np.asanyarray(df[['ENGINESIZE']])\r\n",
        "ytr = np.asanyarray(df[['CO2EMISSIONS']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThEbtRx2zXQd"
      },
      "source": [
        "First, we initialize the variables <b>a</b> and <b>b</b>, with any random guess, and then we define the linear function:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mLDdK0U1tPY"
      },
      "source": [
        "a = tf.Variable(30.2)\r\n",
        "b = tf.Variable(20.0)\r\n",
        "\r\n",
        "def h(x):\r\n",
        "    y = a + b*x\r\n",
        "    return y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvh2j7vfzXQd"
      },
      "source": [
        "Now, we are going to define a loss function for our regression, so we can train our model to better fit our data. In a linear regression, we minimize the squared error of the difference between the predicted values(obtained from the equation) and the target values (the data that we have). In other words we want to minimize the square of the predicted values minus the target value. So we define the equation to be minimized as loss.\n",
        "\n",
        "To find value of our loss, we use <b>tf.reduce_mean()</b>. This function finds the mean of a multidimensional tensor, and the result can have a different dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkN3YFWE32Pz"
      },
      "source": [
        "def loss_object(y,ytr):\r\n",
        "  return tf.reduce_mean(tf.square(y - ytr))\r\n",
        "\r\n",
        "# Below is a predefined method offered by TensorFlow to calculate loss function\r\n",
        "#loss_object = tf.keras.losses.MeanSquaredLogarithmicError()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzii0VemzXQe"
      },
      "source": [
        "Now we are ready to start training and run the graph. We use GradientTape to calculate gradients:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFLPQlxE4PjZ"
      },
      "source": [
        "nu = 0.01 #learning rate\r\n",
        "train_param = [] #a,b through different iterations\r\n",
        "cost = []\r\n",
        "\r\n",
        "# steps of looping through all your data to update the parameters\r\n",
        "epochs = 200\r\n",
        "\r\n",
        "# train model\r\n",
        "for epoch in range(epochs):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        yhat = h(xtr)\r\n",
        "        loss_value = loss_object(ytr,yhat)\r\n",
        "        cost.append(loss_value)\r\n",
        "\r\n",
        "        # get gradients\r\n",
        "        gradients = tape.gradient(loss_value, [a,b])\r\n",
        "        \r\n",
        "        # compute and adjust weights\r\n",
        "        a.assign_sub(gradients[0]*nu)\r\n",
        "        b.assign_sub(gradients[1]*nu)\r\n",
        "\r\n",
        "        if epoch % 5 == 0:\r\n",
        "            train_param.append([b.numpy(), a.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzsPML8zXQe"
      },
      "source": [
        "Lets plot the loss values to see how it has changed during the training:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "wythZMTT_4AR",
        "outputId": "4a749f61-ffff-4130-ad89-63b834498c0e"
      },
      "source": [
        "plt.plot(cost,\"ro\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8e4d0007b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHPEPlNzXQf"
      },
      "source": [
        "Lets visualize how the coefficient and intercept of line has changed to fit the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "okxeAHvgzXQf",
        "outputId": "4d6d39aa-e837-48d0-f283-d03603533104"
      },
      "source": [
        "cr, cg, cb = (1.0, 1.0, 0.0)\n",
        "for f in train_param:\n",
        "    cb += 1.0 / len(train_param)\n",
        "    cg -= 1.0 / len(train_param)\n",
        "    if cb > 1.0: cb = 1.0\n",
        "    if cg < 0.0: cg = 0.0\n",
        "    [b, a] = f\n",
        "    f_y = np.vectorize(lambda x: a + b*x)(xtr)\n",
        "    line = plt.plot(xtr, f_y)\n",
        "    plt.setp(line, color=(cr,cg,cb))\n",
        "\n",
        "plt.plot(xtr,ytr, 'ro')\n",
        "green_line = mpatches.Patch(color='red', label='Data Points')\n",
        "\n",
        "plt.legend(handles=[green_line])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unkUW5s-YIJx"
      },
      "source": [
        "###Lab: Logistic Regression with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IegND-DUYMSz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB5XWq0ZAm3w"
      },
      "source": [
        "##Supervised Deep Learning Models: Convolutional Neural Networks(CNN / ConvNets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk8ltGl7akJT"
      },
      "source": [
        "###Introduction to Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLuPIyuyakjt"
      },
      "source": [
        ">Convolutional neural networks or CNNs have gained a lot of attention in the machine learning community over the last few years. This is due to the wide range of applications where CNNs excel, such as object detection and speech recognition. In a later part of this video, we'll examine the object recognition problem, which is a key consideration when using CNNs. You'll also get an idea of how a CNN structure allows it to extract the elements from an image. For example, you'll see how a CNN learns to pick out a person or a dog or even chairs from an image, even if the chairs are only partially visible. Indeed, this is not an easy task for computers and it took years of dedicated research to achieve this. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(950).png?raw=true)\r\n",
        "\r\n",
        ">The original goal of the CNN was to form the best possible representation of our visual world in order to support recognition tasks. The CNN solution needed to have two key features to make it viable. First, It needed to be able to detect the objects in the image and place them into the appropriate category. Second, it also needed to be robust against differences in pose, scale, illumination, confirmation and clutter. The importance of this second feature simply cannot be overstated, since this was historically a huge limitation of hand-coded algorithms.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(951).png?raw=true)\r\n",
        "\r\n",
        ">Interestingly enough, the solution to the object recognition issue was inspired by examining the way our own visual cortex operates. In short, the CNN starts with an input image. It then extracts a few primitive features and combines those features to form certain parts of the object. Finally, it pulls together all of the various parts to form the object itself. In essence, it is a hierarchical way of seeing objects. That is, in the first layer, very simple features are detected. Then these are combined to shape more complicated features in the second layer and so on to detect things like a cat or a dog or any other object we're needing to detect. The convolutional neural network was developed from this sequence of steps.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(952).png?raw=true)\r\n",
        "\r\n",
        ">Let's consider the building in the lower right-hand corner of this image. How can a trained CNN learn and recognize that it's an image of a building, rather than some of the other elements that are situated in the photograph, such as a person or even those clouds in the sky?\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(953).png?raw=true)\r\n",
        "\r\n",
        ">Well, in the training phase, CNNs would receive many building images as input. Then CNNs will automatically find that the best primitive features for a building would be things like horizontal and vertical lines. Much like the neurons firing in a person's visual cortex, the first layer in a CNN reacts when it receives an image that has horizontal or vertical lines. Once these simpler features such as lines are combined, CNNs learn to form higher abstract components, like the windows of the building and the building's external shape. It can then use these basic parts to form the complete object and learn how the entire building essentially looks like. Later, if it sees an image of a building that it hasn't seen before, CNNs can make a decision if the new input image is of a building based on the persistence of the various features it has stored. It is the same process for learning and detecting other objects, such as faces, animals, cars, and so on. So as you can see, the CNN is a set of layers with each of them being responsible to detect a set of feature sets, and these features are going to be more abstract as it goes further into examining the next layer. So at this point, you should have a basic understanding of the principles behind a convolutional neural network, and how CNNs might be used in an application.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(954).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GuwuSkEdFVp"
      },
      "source": [
        "###Convolutional Neural Networks (CNNs) for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8NQBRN6dkfb"
      },
      "source": [
        ">Hello, and welcome! In this video, well explain how CNN can solve an image classification problem such as digit recognition. First, lets see why traditional shallow neural networks do not work as well as CNN. As mentioned before, one of the important steps in the modeling for classification with Shallow Neural Networks, is the feature extraction step. These chosen features could simply be the color, object edges, pixel location, or countless other features that could be extracted from the images. Of course, the better and more effective the feature sets you find, the more accurate and efficient the image classification you can obtain. However, as you can imagine, the process of selecting the best features is tremendously time-consuming, and is often ineffective. Also, extending the features to other types of images becomes an even greater problem.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(955).png?raw=true)\r\n",
        "\r\n",
        ">Convolutional neural networks (or CNNs) try to solve this problem by using more hidden layers, and also with more specific layers. So, when using CNN, instead of you choosing image features, to classify dogs vs. cats, for instance, CNNs can automatically find those features and classify the images for you.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(956).png?raw=true)\r\n",
        "\r\n",
        ">Instead of using a dogs and cats dataset, though, lets use a more practical example here. The MNIST dataset is a \"database of handwritten digits that has a training set of 60,000 examples. The great thing about the MNIST dataset is that the digits have been size-normalized and centered in a fixed-size image\".\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(957).png?raw=true)\r\n",
        "\r\n",
        ">Now, consider the digit recognition problem. We would like to classify images of handwritten numbers, where the observations are the intensity of the pixels. And, the target will be a digit, from 0 to 9. So, our objective here is to build a digit recognition system using CNN. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(958).png?raw=true)\r\n",
        "\r\n",
        ">Now lets look at it from a higher level. Basically, if we look at the pipeline of our deep learning process, we can see the following phases: First, pre-processing of input data. Second, training the deep learning model. And third, inference and deployment of the model. In the first part, we have to convert the images to a readable and proper format for our network. Then, an untrained network is fed with a big dataset of the images in the Training phase, so as to allow the network to learn. That is, we build a CNN, and train it with many hand-written images in the training set. Finally, we use the trained model in the Inference phase, which classifies a new image by inferring its similarity to the trained model. So, this model can be deployed and used as a digit recognition model for unseen cases.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(959).png?raw=true)\r\n",
        "\r\n",
        ">Now lets focus on the training process. As mentioned before, a deep neural network not only has multiple hidden layers, the type of layers and their connectivity also is different from a shallow neural network, in that it usually has multiple Convolutional layers, pooling layers, as well as fully connected layers. The convolutional layer applies a convolution operation to the input, and passes the result to the next layer. The pooling layer combines the outputs of a cluster of neurons in the previous layer into a single neuron in the next layer. And then, the fully connected layers connect every neuron in the previous layer to every neuron in the next layer. In the next video, you will learn more about these layers and their specifications, in detail. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(960).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSKQMNOFga3o"
      },
      "source": [
        "###Convolutional Neural Networks (CNNs) Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgjKU-bjpmMY"
      },
      "source": [
        "> In this video, we'll be examining the architecture of the Convolutional Neural Network Model. As previously mentioned, CNN is a type of neural network empowered with some specific hidden layers, including the convolutional layer, the pooling layer, and the fully connected layer\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(961).png?raw=true)\r\n",
        "\r\n",
        ">Let's start with the first layer, the convolutional layer. The main purpose of the convolutional layer is to detect different patterns or features from an input image. For example, it's edges. Assume that you have an image and you want to find the edges from the photo so that you can define a filter which is also called a kernel. Now, if you slide the filter over the image and apply the dot product of the filter to the image pixels, the result would be a new image with all of the edges. That is, applying the filter to the left image will result in the right features that are typically useful for the initial layers of a CNN. In fact, the output is one of the very first primitive feature sets in the hierarchy of features, it is one of the key concepts behind CNNs. Now, the question is, what really happens mathematically when we apply a filter or kernel on an image?\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(962).png?raw=true)\r\n",
        "\r\n",
        ">This brings us to the convolution process. Convolution is a function that can detect edges, orientations, and small patterns in an image. The convolution operation can be seen as a mathematical function. Imagine that we have a simple black and white image as you see here. We can convert the image to a matrix of pixels with binary values where one means a white pixel and zero represents a black pixel. Please keep in mind however, that the most common usage is a value between 0 and 255 for a gray scale image or a three-channel image for a color image. But for now and for the sake of simplicity, let's use zero and one as our values. In this step, we define a filter. It can be any type of filter even a random filter, but let's use the Edge Detector filter for now. Now, let's slide it over the image and then calculate the convolved value for the first slide. Taking the values negative one and one on two adjacent pixels and zero everywhere else for the filter, results in the following image. Now, let's slide the kernel again. It will calculate the second element of our convolved image. We repeat this process to complete the convolved matrix. So in essence, the convolution function is a simple matrix multiplication and in this instance, the result shows us the edges of the image\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(963).png?raw=true)\r\n",
        "\r\n",
        ">Now, let's consider a handwritten digit that we want to recognize. We can apply a kernel to it. It is in fact the sum of element-wise multiplication of the kernel matrix and the image matrix as shown on the previous slide. So we can show the result as convolving a kernel on the image.\r\n",
        "\r\n",
        "\r\n",
        ">Applying multiple kernels on one image will result in multiple convolved images. Now you can understand that leveraging different kernels allows us to find different patterns in an image such as edges, curves, and so on. The output of the convolution process is called a feature map. At this point, you might be asking yourself, how do I choose or initialize the proper kernels? Well, typically you initialize the kernels with random values and during the training phase, the values will be updated with optimum values in such a way that the digit is recognized.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(964).png?raw=true)\r\n",
        "\r\n",
        ">For example, here is the result of applying eight different kernels on the digit 2. As you can see, each kernel will recognize a particular pattern in the digit. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(965).png?raw=true)\r\n",
        "\r\n",
        ">This is the first layer of convolutional deep learning. We can interpret this layer to traditional neural network terminology. The input image can be considered as a matrix that we feed into the neural network through input nodes. For example in this case, the input layer of the network has 28 by 28 nodes. The output of the coevolution process are eight of 28 by 28 neurons. We can assume those as hidden nodes. So what are the weights between the input and hidden nodes? Yes, the eight kernels which are five by five. Now we need to make a decision as to whether each hidden neuron should be fired or not. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(966).png?raw=true)\r\n",
        "\r\n",
        ">So we have to add activation functions to the neurons. We use ReLu which is short for rectified linear unit as the activation function on top of nodes in the convolution layer. ReLu is a non-linear activation function which is used to increase the non-linear properties of the decision function. So how can we apply it on a convolution layer? Well, we just go through all outputs of the convolution layer, convolved1, and wherever a negative number occurs we swap it out for a zero. This is called the ReLu activation function.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(967).png?raw=true)\r\n",
        "\r\n",
        ">Now, we have gone one step further. In this step, we have to downsample the output images from the ReLu function to reduce the dimensionality of the activate neurons. We use another layer which is called max pooling. Max pooling is an operation that finds the maximum values and simplifies the inputs. In other words, it reduces the number of parameters within the model. In a sense, it turns the low-level data into higher level information. For example, we can select a Window of size of two by two and then select the maximum value for this matrix. That is, if the image is a two-by-two matrix, it would result in one output pixel. Also in this step, we can define strides. A stride dictates the sliding behavior of the max pooling. For example, if we select stride equals 2, the Window will move two pixels every time. Thus, not overlapping. Now we should have our output matrices but with lower dimensions. For example, 14 by 14. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(968).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(969).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(970).png?raw=true)\r\n",
        "\r\n",
        ">The next layer is the fully connected layer. Fully connected layers take the high-level filtered images from the previous layer. That is, all eight matrices in our case and convert them into a vector. First, each previous layer matrix will be converted to a one-dimensional vector. Then it will be fully connected to the next hidden layer which usually has a low number of hidden units. We call it fully connected because each node from the previous layer is connected to all the nodes of this layer. It is connected through a weight matrix. We can use ReLu activation again here. Finally, we use softmax to find the class of each digit. Softmax is an activation function that is normally used in classification problems. It generates the probabilities for the output. For example, our model will not be a 100 percent sure that what digit is the number 3. Instead, the answer will be a distribution of probabilities where if the model is right, the three will be assigned the larger probability. That is, softmax can output a multi-class categorical probability distribution. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(971).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Now, let's put all of these layers together. This chart shows the main layers of a convolutional neural network. As you can see, the whole network generally is doing two tasks: the first part of this network is all about feature learning and extraction, and the second part revolves around classification.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(972).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">If we look at each operation as a building block, we can see that a typical convolutional neural network might look like this diagram. As you can imagine though, a typical CNN architecture for an image classification can be much more complicated. It can be a chain of repeating conv, ReLu, max pool operations. For example, here we see more than one convolutional layer followed by a few fully connected layers. Also, note the effect of each single conv, ReLu, and max pool operations pass through the image. It reduces height and width of the individual image and then it increases the depth of the images. For example, the output of the first convolutional layer is an image of Depth 8 and the output of the second layer is an image of Depth 32. This is very important because using multiple layers, CNN will be able to break down the complex patterns into a series of simpler patterns.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(973).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(974).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ">If we pass one of the digits through the network, you can see the outputs. In the first convolutional layer, we apply eight kernels of size five by five, then we apply ReLu and max pool. In the second convolutional layer, we use 32 kernels. Again, we apply ReLu and max pool. We connect the outputs to a fully connected layer. Also, we use a second fully connected layer with a lower number of nodes. Finally, we use softmax in the last layer.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(976).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Now you can imagine that if we use a CNN trained on human faces, the first layers will represent mostly primitive features. For example, the details of each part of the faces, the next layers represent more abstract patterns such as the nose and eyes, with the last layers representing more face-like patterns. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(977).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Now, let's take a closer look at this architecture and see what happens in the training phase of convolutional neural networks. As mentioned, a CNN is a type of feed forward neural network consisting of multiple layers of neurons. Each neuron in a layer receives some input, processes it, and optionally follows it with a non-linear output using an activation function. So what is the network going to learn through the training process? It learns the connection between the layers. These are the learnable weights and biases matrices. In fact, the whole network as about a series of dot products between the weight matrices and the input matrix. This means we must first initialize the weights of the network randomly, then we will keep feeding the network with a big data set of images, we then check the output of the network and depending on how far the output is from the expected one, we change or update the weights. We keep repeating this process until it reaches a high accuracy of prediction. Of course this is a very high level picture of the whole training process.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(978).png?raw=true)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IozYtec3YWGy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rRuiB4Whmt"
      },
      "source": [
        "### LAB: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw9Dg4e6WlBv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD2vbmfGW8bn"
      },
      "source": [
        "##Supervised Deep Learning Models: Recurrent Neural Networks(RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJjtYKRIXTyj"
      },
      "source": [
        "###The Sequential Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blyzov1rXWpD"
      },
      "source": [
        ">Hello and welcome. In this video, we'll provide an overview of sequential data and explain why it poses a problem for traditional neural networks. Whenever the points in a dataset are dependent on the other points, the data is said to be sequential. A common example of this is a time series, such as a stock price, or sensor data, where each data point represents an observation at a certain point in time. There are other examples of sequential data, like sentences, gene sequences, and weather data. But traditional neural networks typically can't handle this type of data. So let's see why we can't use feedforward neural networks to analyze sequential data.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(979).png?raw=true)\r\n",
        "\r\n",
        ">Let's consider a sequential problem to see how well-suited a basic neural network might be. Suppose we have a sequence of data that contains temperature and humidity values for every day. Our goal is to build a neural network that imports the temperature and humidity values of a given day as input and output; for instance, to predict if the weather for that day is sunny or rainy. This is a straightforward task for traditional feedforward neural networks. Using our dataset, we first feed a data point into the input layer. The data then flows to the hidden layer or layers where the weights and biases are applied. Then the output layer classifies the results from the hidden layer, which ultimately produces the output of sunny or cloudy. Of course, we can repeat this for the second day and get the result.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(980).png?raw=true)\r\n",
        "\r\n",
        ">However, it's important to note that the model does not remember the data that it just analyzed. All it does is accept input after input and produces individual classifications for every day. In fact, a traditional neural network assumes that the data is non-sequential, and that each data point is independent of the others. As a result, the inputs are analyzed in isolation, which can cause problems if there are dependencies in the data. To see how this can be a limitation, let's go back to the weather example again. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(981).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(982).png?raw=true)\r\n",
        "\r\n",
        ">As you can imagine when examining whether, there is often a strong correlation of the weather on one day having some influence on the weather in subsequent days. That is, if it was sunny on one day in the middle of summer, it's not unreasonable to presume that it'll also be sunny on the following day. A traditional neural network model does not use this information however, so we'd have to turn to a different type of model like a recurrent neural networks model. A recurrent neural network has a mechanism that can handle a sequential dataset. By now, you should have a good understanding of the problem that the recurrent neural network model is trying to address.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(983).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6NVdEJbZp0g"
      },
      "source": [
        "###Recurrent Neural Networks Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-VY4AO3abZo"
      },
      "source": [
        ">Hello, and welcome, in this video we'll provide an overview of recurrent neural networks and explain their architecture.A recurrent neural network, or RNN for short, is a great tool for modeling sequential data. The RNN is able to remember the analysis that was done up to a given point by maintaining a state or a context, so to speak. You can think of the state as the memory of RNN, which captures information about what's been previously calculated. This state recurs back into the net with each new input, which is where the network gets his name\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(984).png?raw=true)\r\n",
        "\r\n",
        ">let's take a closer at how this works. Imagine that the RNN were using has only one hidden layer, the first data point flows into the network as input data, denoted as X. As we mentioned before, the hidden units also received the previous state, or the context, denoted as H previous, along with the input. Then in the hidden layer, two values will be calculated, first the new or updated state, denoted as H new, is to be used for the next data point in the sequence. And second, the output of the network will be computed, which is denoted as y, the new state is a function of the previous state and the input data as shown here.\r\n",
        "If this is the first data point then some form of initial state is used, which will differ depending on the type of data being analyzed, but typically it is initialized to all zeros. Please notice that Wx in this equation is the weight matrix between the input and the hidden unit, and Wh are the weights that are multiplied by the previously hidden state in the equation.\r\n",
        "The output of the hidden unit is simply calculated by multiplication of the new hidden state and the output weight matrix. So after processing the first data point in addition to the output, a new context is generated that represents the most recent point. Then this context is fed back into the net with the next data point, and we repeat these steps until all the data is processed. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(985).png?raw=true)\r\n",
        "\r\n",
        ">**many-to-many network**  \r\n",
        "Recurrent neural networks are extremely versatile and are used in a wide range of applications that deal with sequential data. One of these applications is speech recognition, as you can see it is a type of a many-to-many network, that is, the goal is to consume a sequence of data and then produce another sequence. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(986).png?raw=true)\r\n",
        "\r\n",
        ">**one-to-many network**  \r\n",
        "Another application of RNN is image captioning, although it's not purely recurrent, you can create a model that's capable of understanding the elements in an image. Then using the RNN, you can string the elements as words together to form a caption that describes the scene. Typically, RNN has outputs at each time step, but it depends on the problem that RNN is addressing, for example, in this type of RNN for captioning there is one input as image and the output is a sequence of words. So it is sometimes called one-to-many, \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(987).png?raw=true)\r\n",
        "\r\n",
        ">**many-to-one**  \r\n",
        "RNN can also be many-to-one, that is, it consumes a sequence of data and produces just one output. For example, to predict the stock market price, where we might only be interested in the price of a particular stock in tomorrow's market, or for sentiment analysis, where we may only care about the final output, not the sentiment after each word. We've only covered a few applications, but variance of the recurrent models are continuing to solve increasingly complex problems, which are beyond the scope of this video.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(988).png?raw=true)\r\n",
        "\r\n",
        ">Despite all of its strengths, the recurrent neural network is not a perfect model, one issue is that the network needs to keep track of the states at any given time. There could be many units of data or many time steps, so this becomes computationally expensive, one compromise is to only store a portion of the recent states in a time window. Another issue is that recurrent neural networks are extremely sensitive to changes in their parameters, as a result gradient descent optimizers may struggle to train the net. Also, the net may suffer from the vanishing gradient problem, where the gradient drops to nearly zero and training slows to a halt. Finally, it may also suffer from the exploding gradient, where the gradient grows exponentially off to infinity, in either case the model's capacity to learn will be diminished. By now, you should have a good understanding of the main ideas behind the recurrent neural network model.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(989).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OfArIthgfJD"
      },
      "source": [
        "###The Long Short Term Memory (LSTM) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO0ESOQsiX8L"
      },
      "source": [
        ">The Long Short- Term Memory Model. The.Recurrent Neural Network, is a great tool for modeling sequential data, but there are a few issues that need to be addressed in order to use the model on a large scale. For example, recurrent nets are needed to keep track of states, which is computationally expensive. Also, there are issues with training, like the vanishing gradient and the exploding gradient. As a result, the RNN, or to be precise, the Vanilla RNN cannot learn long sequences very well. A popular method to solve these problems is a specific type of RNN, which is called the Long Short- Term Memory or LSTM for short. LSTM maintains a strong gradient over many time steps. This means you can train an LSTM with relatively long sequences.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(990).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">An LSTM unit in Recurrent Neural Networks is composed of four main elements: the memory cell and three logistic gates. The memory cell is responsible for holding data. The write, read, and forget gates define the flow of data inside the LSTM. The write gate is responsible for writing data into the memory cell. The read gate reads data from the memory cell and sends that data back to the recurrent network, and the forget gate, maintains or deletes data from the information cell, or in other words determines how much old information to forget. In fact, these gates are the operations in the LSTM that executes some function on a linear combination of the inputs to the network, the network's previous hidden state, and previous output. I don't want you to dive into the details of how these gates interact with each other, but what is important here is that by manipulating these gates, a recurrent network is able to remember what it needs in a sequence and simply forget what is no longer useful. Now, let's look at the data flow in LSTM recurrent networks. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(991).png?raw=true)\r\n",
        "\r\n",
        ">So let's see how data is passed through the network. In the first time step, the first element of the sequence is passed to the network. The LSTM unit uses the random initialize hidden state and output to produce the new hidden state and also the first step output. The LSTM then sends its output and hidden state to the net in the next time step, and the process continues for the next time steps. So as you can see, the LSTM unit keeps two pieces of information as it propagates through time. First a hidden state, which is in fact the memory the LSTM accumulates using its gates through time, and second, the previous time step output. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(992).png?raw=true)\r\n",
        "\r\n",
        ">The original LSTM model has only one single hidden LSTM layer, but as you know, in cases of simple Feedforward Neural Networks, we usually stack layers to create hierarchical feature representation of the input data. So does this also apply to LSTMs? What if we want to have an RNN with stacked LSTM. For example, a two layer LSTM. In this case, the output of the first layer will feed as the input to the second layer. Then, the second LSTM blends it with its own internal state to produce an output. Stacking an LSTM allows for greater model complexity. So the second LSTM can create a more complex feature representation of the current input. That is, stacking LSTM hidden layers makes the model deeper, and most probably leads to more accurate results.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(993).png?raw=true)\r\n",
        "\r\n",
        ">Now, let's see what happens during the training process. The network learns to determine how much old information to forget through the forget gate. So the weights denoted as WF, and biases denoted as BF, will be learned through the training procedure. We also determine how much new information called X, to incorporate through the input gate and it's weights. We also calculate the new cell state based on the current and the previous internal state so the network has to learn is corresponding weights and biases. Finally, we determine how much of our cell state we want to output an output gate. Basically, the network is learning the weights and biases used in each gate for each layer. By now you should have a good understanding of the main ideas behind the LSTM. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(994).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71nRrXEDmGt-"
      },
      "source": [
        "###Language Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2l9FD3Dmhl9"
      },
      "source": [
        ">We will be reviewing how to apply Recurrent Neural Networks to Language Modeling. Language Modeling is a gateway into many exciting deep learning applications like Speech Recognition, Machine Translation, and Image Captioning. At its simplest, Language Modeling is the process of assigning probabilities to sequences of words. So for example, a language model could analyze a sequence of words and predict which word is most likely to follow. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(995).png?raw=true)\r\n",
        "\r\n",
        "> So with the sequence, this is ''an'' which you see here. A language model might predict what the next word might be. Clearly, there are many options for what word could be used as the next one in the string. But a trained model might predict with an 80 percent probability of being correct that the word ''example'' is most likely to follow. This boils down to a sequential data analysis problem. The sequence of words forms the context and the most recent word is the input data. Using these two pieces of information, you need to output both a predicted word and a new context that contains the input word\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(996).png?raw=true)\r\n",
        "\r\n",
        ">Recurrent Neural Networks are a great fit for this type of problem. At the first time step, a recurrent net can receive a word as input along the initial contexts. It generates an output. The output word with a current sequence of words as the context will then be re-fed into the network in the second time step. A new word would be predicted and these steps are repeated until the sentence is complete. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(997).png?raw=true)\r\n",
        "\r\n",
        ">Now, let's take a closer look at an LSTM network for modeling the language. In this network, we will use an RNN network with two stacked LSTM units. For training such a network, we have to pass each word of the sentence to the network and let the network generate an output. For example, after passing the words, \"This'' and ''Is\", if we pass the word, \"an\" in the third time step, we expect the network to generate the word ''example'' as output. But notice that we cannot easily pass a word to the network. We have to convert it into a vector of numbers somehow. We can use Word Embedding for this purpose. Let's quickly examine what happens in Word Embedding. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(998).png?raw=true)\r\n",
        "\r\n",
        ">An interesting way to process words is through a structure known as a Word Embedding. A Word Embedding is an n-dimensional vector of real numbers for each word. The vector is typically large, for example 200 length. You can see what that might look like with the word ''example'' here. You think of Word Embedding as a type of encoding for text to numbers. Now, the question is how do we find the proper values for these vectors? In our RNN model, the vectors also known as the matrix for the vocabulary are initialized randomly for all the words that we are going to use for training. Then during the recurrent network's training, the vector values are updated based on the context into which the word is being inserted. So words that are used in similar contexts end up with similar positions in the vector space. This can be visualized by utilizing a dimensionality reduction algorithm\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(999).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1000).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        "> Take a look at the example shown here. After training the RNN, if we visualize the words based on their embedding vectors, the words are grouped together, either because they're synonyms or they're used in similar places within a sentence. For example, the words ''zero'' and ''none'' are close semantically, so it's natural for them to be placed close together. While Italy and Germany aren't synonyms, they can be interchanged in several sentences without distorting the grammar.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1001).png?raw=true)\r\n",
        "\r\n",
        ">Now let's look back at the RNN that we have been using. Imagine that the input data is a batch with only one sequence of words. Think of it as a batch that includes one sentence only, one that includes 20 words. Assume that the vocabulary size of the words is 10,000 words and the length of each embedding vector is 200. We have to look up those 20 words in the randomly initialized embedding matrix and then feed them into the first LSTM unit. Please notice that only one word in each time step is fed into the network and one word would be the output. But during 20 time steps, the output would be 20 words. In our network, we have two LSTM units with arbitrary hidden sizes of 256 and 128. So the output of the second LSTM unit would be a matrix of size 20 by 128. Now, we need a soft max layer to calculate the probability of the output words. It squashes that 128 dimensional vector of real values to a 10,000 dimensional vector, which is a vocabulary size. This means that the output of the network at each time step as a probability vector of length 10,000. So the output word is the one with maximum probability value in the vector. Now, we can compare the sequence of 20 output words with the ground truth words. Finally, calculate the discrepancy as a quantitative value, so-called loss value and back propagate the errors into the network\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1002).png?raw=true)\r\n",
        "\r\n",
        ">And of course we will not train the model using only one sequence. We will use a batch of sequences to train it and calculate the error. So instead of feeding one sequence, we can feed the network in many iterations, perhaps even a batch of 60 sentences for example. Now, the key question to be asked is, what does the network learn when the error is propagated back in each iteration? Well, as previously noted, the weights keep updating based on the error of the network in training. First, the embedding matrix will be updated in each iteration. Second, there are a bunch of weight matrices related to the gates in the LSTM units which will be changed. Finally, the weights related to the soft-max layer, which somehow plays the decoding role for the encoded words in the embedding layer. By now you should have a good understanding of how to use LSTM for language modeling. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1003).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDCh8CkNsPP-"
      },
      "source": [
        "###Lab: Basics of Long Short-Term Memory (LSTM) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce3JKz94sWCb"
      },
      "source": [
        "### Lab: Language Modelling with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhaGLq9hsprz"
      },
      "source": [
        "##Unsupervised Deep Learning Models: Restricted Boltzmann Machines(RBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue5VJpMQs0Fl"
      },
      "source": [
        "###Introduction to Restricted Boltzmann Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQcuUkC7s7lF"
      },
      "source": [
        ">Hello and welcome. In this video, we'll be providing an introduction to restricted Boltzmann machines. So let's get started. Imagine that we have access to a matrix of the viewer ratings of a certain number of movies on Netflix, where each row corresponds to a movie and each column corresponds to a user's rating. For the sake of simplicity, let's say we're only examining eight users and their ratings of seven movies. The value in each cell shows the score that the user has given to the movie after watching it and is based on a rating scale of one to five. Also, imagine that we have a type of neural network that has only two layers, the input layer and the hidden layer. Let's also assume that this network has learned in such a way that it can reconstruct the input vectors. For example, when you feed the first user vector into the network, it goes through the network and finally fires up some units in the hidden layer. Then the values of the hidden layer will be fed back into the network and a vector which is almost the same as the input vector is reconstructed as output. We can think of it as making guesses about the input data. You feed the second user's ratings, which are not very different from the first user and thus the same hidden units will be turned on and the network output would be the same as the first reconstructed vector. We can repeat it for the third user and for user number 4. Now, let's feed a user that has a completely different idea about these movies. As you can see, this particular user was not a fan of the first movie. When we feed the respective rating values into the network, different hidden units get turned on and a different vector is reconstructed which is almost the same as user number five's preferences. It is the same for user number 6, and the process can be repeated for the other users. Now, let's look at user number 8. He hasn't watched movie 6, but does have some preferences that are almost the same as users 5 and 6, right? Let's feed this vector into our network. It'll fire up the same hidden units as users 5 and 6, and feeding back these values will reconstruct a new vector. Look at the expected value for movie 6 in the reconstructed vector. Using this value is not hard to imagine that user number 8 might be interested in this movie even though he hasn't watched it yet. Maybe we could even recommended to him, yes. In fact, it is a way of solving collaborative filtering, which is a type of recommender system engine and the network that can make such a model is called a restricted Boltzmann machine.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1004).png?raw=true)\r\n",
        "\r\n",
        ">Restricted Boltzmann machines or RBMs for short, are shallow neural networks that only have two layers. They are an unsupervised method used to find patterns in data by reconstructing the input. The first layer of the RBM is called the visible layer and the second layer is the hidden layer. We say that they are restricted because neurons within the same layer are not connected. Feeding the input data, the network learns its weights. Then feeding an input image, the values that appear in the hidden layer can be considered as features learned automatically from the input data. As there are a smaller number of units in the hidden units of an RBM, we can tell that the values in the hidden units are a good representation of data, that are lower in dimensionality when compared to the original data.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1005).png?raw=true)\r\n",
        "\r\n",
        ">Restricted Boltzmann machines are useful in many applications, like dimensionality reduction, feature extraction, and collaborative filtering just to name a few. On top of that RBMs are used as the main block of another type of deep neural network which is called deep belief networks which we'll be talking about later. By now you should have basic knowledge about RBMs and their applications. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1006).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSc4iXATvney"
      },
      "source": [
        "###Restricted Boltzmann Machines (RBMs) Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBUXW8DuwBsH"
      },
      "source": [
        ">Hello and welcome. In this video, we will provide an overview of our RBMs with a focus on understanding how they work. Let's get started. Let's say that we provide an image as input to an RBM. The pixels are processed by the input layer, which is also known as the visible layer. RBMs learn patterns and extract important features in data by reconstructing the input. So the learning earning process consists of several forward and backward passes where the RBM tries to reconstruct the input data. The weights of the neural net are adjusted in such a way that the RBM can find the relationships among input features and then determines which features irrelevant. After training is complete, the net is able to reconstruct the input based on what it learned. The reconstructed image here is only a representation of what happens. What's important to take from this example though, is that the RBM can automatically extract meaningful features from a given input in the training process? In fact, a trained RBM can reveal which features are the most important ones when detecting patterns. It can also represent each image with some hidden values also referred to as latent values.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1007).png?raw=true)\r\n",
        "\r\n",
        ">Now, let's look at an RBM is trainings process in which three major steps are repeated. The first step is the forward pass. In the forward pass, the input image is converted to binary values and then the vector input is fed into the network where its values are multiplied by weights and an overall bias in each hidden unit. Then the result goes to an activation function such as the sigmoid function which represents the probability of turning each individual hidden unit on or in other words, the probability of the node activation. Then a sample is drawn from this probability distribution, and defines which neurons may or may not activate. This means it makes stochastic decisions about whether or not to transmit that hidden data. The intuition behind the sampling is that there are some random hidden variables and by sampling from the hidden layer, you can reproduce sample variance encountered during training. So as you can see the forward pass translates the inputs into a set of binary values that get represented in the hidden layer. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1008).png?raw=true)\r\n",
        "\r\n",
        ">Then we get to step two, the backward pass. In the backward pass the activated neurons in the hidden layer send the results back to the visible layer, where the input will be reconstructed. During this step, the data that is passed backwards is also combined with the same weights and overall bias that were used in the forward pass. So once the information gets to the visible layer, it is in the shape of the probability distribution of the input values given the hidden values. And sampling the distribution, the input is reconstructed. So as you can see the backward pass is about making guesses about the probability distribution of the original input. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1009).png?raw=true)\r\n",
        "\r\n",
        ">Now, let's look at step 3. Step 3 consists of assessing the quality of the reconstruction by comparing it to the original data. The RBM then calculates the error and adjust the weights and bias in order to minimize it. That is in each epoch, we compute the error as a sum of the squared difference between step 1 and the next step. These three steps are repeated until the error is deemed sufficiently low.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1010).png?raw=true)\r\n",
        "\r\n",
        ">Now, let's touch on a few reasons why RBMs are such a great tool. A big advantage of RBMs is that they excel when working with unlabeled data.\r\n",
        "Many important real world datasets are unlabeled like videos, photos and audio files. So RBMs provide a lot of benefit in these types of unsupervised learning problems. Another advantage is that during the learning process the RBM extracts features from the input data decides, which features are relevant and how to best combine them to form patterns. RBMs are also generally more efficient at dimensionality reduction when compared to principal component analysis, which is considered a popular alternative. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1011).png?raw=true)\r\n",
        "\r\n",
        ">Finally, RBMs learn from the data. They actually encode their own structure. This is why they're grouped into a larger family of models known as autoencoders. However, restricted boltzmann machines differ from autoencoders and that they use a stochastic approach rather than a deterministic approach.\r\n",
        "- [stochastic-vs-deterministic](https://blog.ev.uk/stochastic-vs-deterministic-understand-the-pros-and-cons)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1012).png?raw=true)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZcf4sjvzvuI"
      },
      "source": [
        "###Lab: Restricted Boltzmann Machines (RBMs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfwlYv6zsvw3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHfnel4az30g"
      },
      "source": [
        "##Unsupervised Deep Learning Models: Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ty4GcHj6rRF"
      },
      "source": [
        "###Introduction to Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGtpfUoq2tz4"
      },
      "source": [
        ">Hello, and welcome! In this video, well be covering the basic concepts and the motivation behind autoencoders, a type of neural network used in unsupervised machine learning. Lets say that you want to extract the feeling or emotion of a person in a photograph. The photograph that you see here is a small image thats just 256 by 256 pixels But, this means that there are over 65 thousand pixels in play in defining the dimension of the input! As we increase the dimensionality, the time to deal with data increases exponentially, in order to train and fit the raw data into a neural network that can detect the emotion. So, we need a way to extract the most important features of a face, and represent each image with those features which are of lower dimensions. An autoencoder works well for this type of problem. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1013).png?raw=true)\r\n",
        "\r\n",
        ">An autoencoder is a type of unsupervised learning algorithm that will find patterns in a dataset by detecting key features. It is a type of neural net that analyzes all of the images in your dataset and extracts some useful features automatically in such a way that it can distinguish images using those features.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1014).png?raw=true)\r\n",
        "\r\n",
        ">Generally speaking, autoencoders excel in tasks that involve feature learning or extraction, data compression, and learning generative models of data and dimensionality reduction.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1015).png?raw=true)\r\n",
        "\r\n",
        ">Let me talk more about Dimension Reduction. High-dimensional data is a significant problem for machine learning tasks. In fact, data scientists commonly refer to it as the CURSE OF DIMENSIONALITY. Many common problems that we want to target have a large number of dimensions. Even that 256 x 256 pixel image that we saw earlier, would have over 65 thousand dimensions; or in other words, one dimension for each pixel. A high-resolution image from a smart phone would be even larger. According to a study, the time to fit a model is, at best, the function you see here, which is a function of the number of points, dimensionality, and parameters. So, as we increase the dimensionality, our time to fit our model will increase exponentially. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1016).png?raw=true)\r\n",
        "\r\n",
        ">So, what happens when we increase or reduce the dimension of data? Well, if we have a huge number of dimensions, our data will start to get sparse, which results in an over-allocation of memory and slow training time. We run into additional problems when we try to reduce the dimensions. If we have a small number of dimensions, our data could overlap, resulting in a loss of data characteristics. You can see how that looks in one dimension and three dimensions. Overlap and sparsity make it difficult to determine the underlying patterns. However, with the proper number of dimensions, the patterns become much clearer\r\n",
        "- [deep-learning-curse-dimensionality](https://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html/3)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1017).png?raw=true)\r\n",
        "\r\n",
        "> Its important to know that an Autoencoder is not the only dimension reduction method in Machine Leaning. Principal Component Analysis (or PCA) has been around for a long time, and is a classic algorithm for dimensionality reduction. Take a look at the data separation outputs for the MNIST dataset of handwritten digits. These charts show the image data after reducing their dimensions to 2 dimensions. The left output is from PCA. The output on the right is from an autoencoder. As you can see, its easier to discern the data with the autoencoders output. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1018).png?raw=true)\r\n",
        "\r\n",
        ">Here is another comparison between PCA and an autoencoder, now applied to news stories. The separability of the autoencoder is far better than the PCA, in this case. Since separability is important for applying clustering algorithms, this difference in quality is significant. By now, you should understand that an autoencoder can extract key image features, improve training times of other networks, and improve the separability of reduced datasets when compared to other methods.For these reasons, the autoencoder was a breakthrough in the unsupervised learning research field.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1019).png?raw=true)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aMfFFmN6wDf"
      },
      "source": [
        "###Autoencoders architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYH-6V627D4h"
      },
      "source": [
        ">Hello, and welcome! In this video, well be examining the architecture of autoencoders and show you how autoencoders work. An autoencoder neural network is supposed to represent the images in a dataset with a low dimension feature set. For example, it extracts the most important features of faces, for an arbitrary task such as face recognition. Also, it is supposed to do it in an unsupervised manner, that is, feature extraction without provided labels for images. Now, lets see how Authoencoders actually work. An autoencoder is an artificial neural network thats designed to find important features by recreating the given input. Generally speaking, the main goal of Autoencoders is to take unlabeled inputs, encode them, and then try to reconstruct them afterwards, based on the most valuable features identified in the data. In fact, Autoencoders are based on Restricted Boltzmann Machines, or in other words, Restricted Boltzmann Machines are a type of Autoencoder.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1020).png?raw=true)\r\n",
        "\r\n",
        ">So, what is the difference between Autoencoders and Restricted Boltzmann Machines -- or RBMs, for short? Well, most autoencoders are shallow networks with an input layer, a few hidden layers, and an output layer. RBMs, on the other hand, are autoencoders with only two layers. Also, Autoencoders differ from Restricted Boltzmann Machines because they use a deterministic approach, rather than a stochastic approach.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1021).png?raw=true)\r\n",
        "\r\n",
        ">Ok, lets take a closer look at the architecture of autoencoders. An autoencoder can be divided into two parts, the encoder and the decoder. The encoder needs to compress the representation of an input. In this case, were going to compress the face of our actor, moving from 2000-dimensions of data to only 30 dimensions. The decoder is a reflection of the encoder's network. It works to recreate the input as accurately as it can. It has an important role during training, and that is to force the autoencoder to select the most important features in the compressed representation. As you can see, we really dont care about the reconstructed image in this network. What we do care about, though, is the code layer values, which represents the input layer. That is, if the network has learned enough that it can generate the replica of input images only based on the code-layer value, then, this vector of size 30, is good enough as the feature set to represent the input image. Therefore, after the training is complete, you can use the encoded data that has been dimensionally-reduced for the application of your choosing. This can include clustering, classification, or visualization of your data. So, as you can see, the Autoencoder can be considered as an unsupervised feature extraction technique for different machine learning algorithms. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1022).png?raw=true)\r\n",
        "\r\n",
        ">So, how do Autoencoders learn? Autoencoders use back-propagation in their learning process. The metric used to assess the quality of the network is loss, which is the amount of information lost in the reconstruction of the input. The squared error of reconstruction for a single sample is the average of the squared error of the reconstructed image for all dimensions. The goal is to minimize the loss, so that we have an output thats as close to the input as possible. At this point, you should have a better understanding of the structure and applications of autoencoders. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(1023).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxVZXxx8yLn"
      },
      "source": [
        "###Lab: Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo_EiLYTz_5i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hMzK7cR_o2K"
      },
      "source": [
        "##Scaling of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTRcW6Ih_lfl"
      },
      "source": [
        "**Learning Objectives**  \r\n",
        "In this module you will learn about:\r\n",
        "\r\n",
        "- How to train your machine learning model using [Watson Studio](https://studio.edx.org/container/dataplatform.cloud.ibm.com)\r\n",
        "- How to scale your model training using multi CPU or GPU runtimes\r\n",
        "\r\n",
        "In the previous sections you learned how to train various deep learn  models. This section you will learn how to use Watson Studio to scale  model training to multipe CPUs or GPUs. You don't need a credit card and  up to 100 hours of free runtimes are included, every month, never  expiring\r\n",
        "\r\n",
        "Please follow the [instructions](https://github.com/IBM/skillsnetwork/wiki/Watson-Studio-Setup) to setup your project in Watson Studio.\r\n",
        "\r\n",
        "Then,  please import the following notebook into Watson Studio and follow the  instructions within the notebook. Instructions on how to import can be  found [here](https://github.com/IBM/skillsnetwork/wiki/Watson-Studio-Notebook-Import).\r\n",
        "\r\n",
        "Raw link to notebook (to be used for Watson Studio import) https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week6/labs_DL0120EN_6.1_tensor_flow_scaling_cpu_gpu.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__21exN3APym"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}