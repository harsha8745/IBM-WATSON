{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GSx2icY1x8KN",
        "aY4xM5bu5hbk",
        "4QgbpN3u5lkr",
        "m5t_E08t934w",
        "PxM5ChSj_W8D",
        "S-qaf1QyCGNG",
        "HbFfkaJ8n-7W",
        "RUidctwlk6is",
        "gP1KVjUWl2RJ",
        "DQ5s8mg43YMk",
        "oFgL3tI499yQ",
        "YFbjyvCJT3Sv",
        "q8pGHHxBUqGr",
        "RuGXaoxNU3TE",
        "pLPMy5mlyHRo"
      ],
      "authorship_tag": "ABX9TyNefyUqyP8dLjGujPHlfP5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsha8745/IBM-WATSON/blob/master/AI_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSx2icY1x8KN"
      },
      "source": [
        "#Introduction to Deep Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY4xM5bu5hbk"
      },
      "source": [
        "##Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QgbpN3u5lkr"
      },
      "source": [
        "###Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFfGDhlm5nC4"
      },
      "source": [
        ">In this video, we will start talking about deep learning and how the recent advances in the field have led to amazing and mind-blowing applications. I am sure that you are aware that deep learning is one of the hottest subjects in data science, if not the hottest, especially with the tremendous amount of fascinating projects that are surfacing with the help of deep learning; projects which people deemed almost impossible with just a little over a decade ago. Therefore, there is a lot of excitement about deep learning. In this video, I will share with you some amazing and recent applications of deep learning that will hopefully inspire and motivate you even more about deep learning. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(847).png?raw=true)\r\n",
        "\r\n",
        ">The first amazing application is color restoration, where a given image in greyscale is automatically turned into a colored one. A group of researchers in Japan built a system using convolutional neural networks that can take a grayscale image, like these ones, and add life to them by turning them into colored ones. You can find many other awesome examples by following this link, which you can also find below the video. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(848).png?raw=true)\r\n",
        "\r\n",
        ">Another really cool but double-sworded application is speech enactment, where an audio clip is synthesized with a video, and the lip movements in the video are synced with the sounds and words in the audio clip. Many attempts have been made in the past to build such a system, but many of them produced results that looked uncanny. Recently, a group of researchers at the University of Washington built the first system that generates realistic results by training a recurrent neural network on a large corpus of video data of a single person. The subject of their case study was the former President of the United States, Barack Obama. Let's look at their example. So here is an audio clip from one of Obama's speeches. \"It's been less than a week since the deadliest mass shooting in American history.\" The audio clip was synthesized with a video of one of his other speeches, and his lip movements were synced with the words and sounds in the audio clip. Let's take a look. \"It's been less than a week since the deadliest mass shooting in American history.\" Anyone watching the video can't really tell that the video was synthesized. Not only that, but their system is also capable of extracting an audio from a video and syncing the lip movements in another video with the audio from the first video. Let's look at an example of this. \"Especially our friends who were lesbian, gay, bisexual, or transgender. I visited with the families of many of the victims on Thursday and one thing I told them is that they're not alone.\" Your jaw dropped yet? \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(849).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(850).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Another fascinating application of deep learning is automatic handwriting generation. Alex Graves at the University of Toronto used recurrent neural networks to design an algorithm that can rewrite a given message in highly realistic cursive handwriting in a wide variety of styles. So you can type some text in this field and you can either select the style of handwriting to be generated or let the algorithm randomly select it for you.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(851).png?raw=true)\r\n",
        "\r\n",
        ">There is a plethora of other applications such as automatic machine translation, where convolutional neural networks are used to translate text in an image on the fly. Another application is automatically adding sounds to silent movies, where a deep learning model uses a database of pre-recorded sounds to select a sound to play that best matches what is taking place in the scene. Not to leave out the popular applications of classifying objects in images and self-driving cars. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(852).png?raw=true)\r\n",
        ">For almost all of the aforementioned applications, you heard me say neural networks again and again. So you must be asking haven't neural networks been around for quite some time? How come all of a sudden they are taking off and becoming very popular with endless applications? In order to answer this question, let's start learning the specifics of neural networks and deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5t_E08t934w"
      },
      "source": [
        "###Neurons and Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3rXh1tv-sWD"
      },
      "source": [
        ">All the algorithms that are used in deep learning are largely inspired by the way neurons and neural networks function and process data in the brain. This image is one of the very first pictures of a neuron. It was drawn by Santiago Ramon y Cajal, back in 1899 based on what he saw after placing a pigeon's brain under the microscope. He is now known as the father of modern neuroscience, but based on his drawing, the neurons, one of them labeled A, have big bodies in the middle and long arms that stretch out and branch off to connect with other neurons. This other image here is that of a neural network and has a bunch or thousands of neurons in what looks like a brain tissue. Tt gives you a sense of how tightly they are packed together and how many of them are in a small brain tissue\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(853).png?raw=true)\r\n",
        "\r\n",
        ">Going back to the drawing of neurons by Ramon y Cajal, let's rotate it 90 degrees to the left. I bet this way it is starting to look a little familiar since it slightly resembles drawings of artificial neural networks that you must have seen. Here is a cartoon drawing of the neuron. The main body of the neuron is called the soma, which contains the nucleus of the neuron. The big network of arms sticking out of the body is called the dendrites, and then the long arm that sticks out of the soma in the other direction is called the axon. The whiskers at the end of the axon are called the terminal buttons or synapses. So the dendrites receive electrical impulses which carry information, or data, from sensors or terminal buttons of other adjoining neurons. The dendrites then carry the impulses or data to the soma. In the nucleus, electrical impulses, or the data, are processed by combining them together, and then they are passed on to the axon. The axon then carries the processed information to the terminal button or synapse, and the output of this neuron becomes the input to thousands of other neurons. Learning in the brain occurs by repeatedly activating certain neural connections over others, and this reinforces those connections. This makes them more likely to produce a desired outcome given a specified input. Once the desired outcome occurs, the neural connections causing that outcome become strengthened. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(854).png?raw=true)\r\n",
        "\r\n",
        ">An artificial neuron behaves in the same way as a biological neuron. So it consists of a soma, dendrites, and an axon to pass on the output of this neuron to other neurons. The end of the axon can branch off to connect to many other neurons, but for simplicity we are just showing one branch here. The learning process also very much resembles the way learning occurs in the brain as you will see in the next couple of videos. Now that we understand the different parts of an artificial neuron, let's learn how we formulate the way artificial neural networks process information.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(855).png?raw=true)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxM5ChSj_W8D"
      },
      "source": [
        "###Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWZ6UwwA-Po"
      },
      "source": [
        ">In this video, we will start learning about the mathematical formulation of neural networks. In the previous video, we established that the shape of an artificial neuron looks like this in order to resemble a real biological neuron. For a network of neurons, we normally divide it into different layers: the first layer that feeds the input into the network is obviously called the input layer. The set of nodes that provide the output of the network is called the output layer. And any sets of nodes in between the input and the output layers are called the hidden layers.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(855).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(856).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">When working with neural networks, the three main topics that we deal with are: forward propagation, backpropagation, and activation functions. The rest of this video will be mainly about forward propagation, and I will explain it through examples with numbers.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(857).png?raw=true)\r\n",
        "\r\n",
        ">Forward propagation is the process through which data passes through layers of neurons in a neural network from the input layer all the way to the output layer. Let's use one neuron and mathematically formulate the way information flows through it. As shown here, the data flows through each neuron by connections or the dendrites. Every connection has a specific weight by which the flow of data is regulated. Here x1 and x2 are the two inputs, they could be an integer or float. When these inputs pass through the connections, they're adjusted depending on the connection weights, w1 and w2. The neuron then processes this information by outputting a weighted sum of these inputs. It also adds a constant to the sum which is referred to as the bias. So, z here is the linear combination of the inputs and weights along with the bias, and a is the output of the network. For consistency, we will stick to these letters throughout the course, so z will always represent the linear combination of the inputs, and a will always represent the output of a neuron. However, simply outputting a weighted sum of the inputs limits the tasks that can be performed by the neural network. Therefore, a better processing of the data would be to map the weighted sum to a nonlinear space. A popular function is the sigmoid function, where if the weighted sum is a very large positive number, then the output of the neuron is close to 1, and if the weighted sum is a very large negative number, then the output of the neuron is close to zero. Non-linear transformations like the sigmoid function are called activation functions. Activation functions are another extremely important feature of artificial neural networks. They basically decide whether a neuron should be activated or not. In other words, whether the information that the neuron is receiving is relevant or should be ignored. The takeaway message here is that a neural network without an activation function is essentially just a linear regression model. The activation function performs non-linear transformation to the input enabling the neural network of learning and performing more complex tasks, such as image classifications and language translations. For further simplification, I am going to proceed with a neural network of one neuron and one input\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(858).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(859).png?raw=true)\r\n",
        "\r\n",
        ">Let's go over an example of how to compute the output. Let's say that the value of x1 is 0.1, and we want to predict the output for this input. The network has optimized weight and bias where w1 is 0.15 and b1 is 0.4. The first step is to calculate z, which is the dot product of the inputs and the corresponding weights plus the bias. So we find that z is 0.415. The neuron then uses the sigmoid function to apply non-linear transformation to z. Therefore, the output of the neuron is 0.6023. For a network with two neurons, the output from the first neuron will be the input to the second neuron. The rest is then exactly the same. The second neuron takes the input and computes the dot product of the input, which is a1 in this case, and the weight which is w2, and adds the bias which is b2. Using a sigmoid function as the activation function, the output of the network would be 0.7153. And this would be the predicted value for the input 0.1. This is in essence how a neural network predicts the output for any given input. No matter how complicated the network gets, it is the same exact process. To summarize, given a neural network with a set of weights and biases, you should be able to compute the output of the network for any given input. In the next video, we will start learning how to train a neural network and optimize the weights and the biases.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(860).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qaf1QyCGNG"
      },
      "source": [
        "###LAB: Artificial Neural Networks - Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "SJLTpplcB9Nh"
      },
      "source": [
        "#### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "mxc5yOBdB9Ni"
      },
      "source": [
        "In this lab, we will build a neural network from scratch and code how it performs predictions using forward propagation. Please note that all deep learning libraries have the entire training and prediction processes implemented, and so in practice you wouldn't really need to build a neural network from scratch. However, hopefully completing this lab will help you understand neural networks and how they work even better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5MxwS1oB9Ni"
      },
      "source": [
        "<h2>Artificial Neural Networks - Forward Propagation</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. Initalize a Network</h5>\n",
        "<h5> 2. Compute Weighted Sum at Each Node. </h5>\n",
        "<h5> 3. Compute Node Activation </h5>\n",
        "<h5> 4. Access your <b>Flask</b> app via a webpage anywhere using a custom link. </h5>     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "fp2zjvMxB9Ni"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>    \n",
        "\n",
        "1.  <a href=\"#item11\">Recap</a>\n",
        "2.  <a href=\"#item12\">Initalize a Network</a>  \n",
        "3.  <a href=\"#item13\">Compute Weighted Sum at Each Node</a>  \n",
        "4.  <a href=\"#item14\">Compute Node Activation</a>  \n",
        "5.  <a href=\"#item15\">Forward Propagation</a>\n",
        "\n",
        "</font>\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "bgwtvOtlB9Nn"
      },
      "source": [
        "#### Recap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "HkNBEuFHB9No"
      },
      "source": [
        "From the videos, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "tFuHZoWkB9Np"
      },
      "source": [
        "<img src=\"http://cocl.us/neural_network_example\" alt=\"Neural Network Example\" width=600px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "ZPnM0vfLB9Nq"
      },
      "source": [
        "Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoIOoQPV5VMO"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "w = weights = np.around(np.random.uniform(size=6),decimals=2) # initialize the weights\r\n",
        "b = biases = np.around(np.random.uniform(size=3),decimals=2) # initialize the weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4ZyzWeErB9Ns"
      },
      "source": [
        "Let's print the weights and biases for sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "FbwM3GmZB9Ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e6102b-07ed-4869-d1bb-10f6388e8a99"
      },
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.55 0.72 0.6  0.54 0.42 0.65]\n",
            "[0.44 0.89 0.96]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "eQKai5rgB9Nt"
      },
      "source": [
        "Now that we have the weights and the biases defined for the network, let's compute the output for a given input, $x_1$ and $x_2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBNHuyCiE3f_"
      },
      "source": [
        "x1 = 0.5\r\n",
        "x2 = 0.85"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "RnWKwhccB9Nu"
      },
      "source": [
        "Let's start by computing the wighted sum of the inputs, $z\\_{1, 1}$, at the first node of the hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev9sW7eeFIo4",
        "outputId": "2d246758-3730-48f7-b58d-7f98d3a95d32"
      },
      "source": [
        "z11 = x1*w[0] + x2*w[1] + b[0]\r\n",
        "z11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "YilmuusAB9Nv"
      },
      "source": [
        "Next, let's compute the weighted sum of the inputs, $z\\_{1, 2}$, at the second node of the hidden layer. Assign the value to **z_12**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgLkvnmDGfRb",
        "outputId": "d45490e7-e735-4847-9cf7-b41d1c9f6047"
      },
      "source": [
        "z12 = x1*w[2] + x2*w[3] + b[1]\r\n",
        "z12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.649"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Xl1gbrlRB9Ny"
      },
      "source": [
        "Next, assuming a sigmoid activation function, let's compute the activation of the first node, $a\\_{1, 1}$, in the hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92PdcGm_HbGY"
      },
      "source": [
        "def sigmoid(z):\r\n",
        "  y = 1/(1+np.exp(-z))\r\n",
        "  return np.round(y,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djWtZTUBNR0K",
        "outputId": "0946af8a-5b65-448f-ef04-65220a9f4222"
      },
      "source": [
        "a11 = sigmoid(z11)\r\n",
        "a11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "IhDuBU4kB9Nz"
      },
      "source": [
        "Let's also compute the activation of the second node, $a\\_{1, 2}$, in the hidden layer. Assign the value to **a_12**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhwRAHUtPsw4",
        "outputId": "9a40c7ba-c476-498a-db54-b26017335182"
      },
      "source": [
        "a12 = sigmoid(z12)\r\n",
        "a12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8388"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "NpfGUeqpB9N0"
      },
      "source": [
        "Now these activations will serve as the inputs to the output layer. So, let's compute the weighted sum of these inputs to the node in the output layer. Assign the value to **z_2**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UnlgdkXP0S7",
        "outputId": "2fd08d3f-7da0-42b0-d833-2bc9569f319c"
      },
      "source": [
        "z2 = a11*w[4] + a12*w[5] +b[2]\r\n",
        "z2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.837146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyMn_RaFQk_Q",
        "outputId": "0c1d18d4-477d-43cc-a652-9ad84dfc19d4"
      },
      "source": [
        "a2 = sigmoid(z2)\r\n",
        "a2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMDAKK3XRlDH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "MyUjfV7oB9N3"
      },
      "source": [
        "Obviously, neural networks for real problems are composed of many hidden layers and many more nodes in each layer. So, we can't continue making predictions using this very inefficient approach of computing the weighted sum at each node and the activation of each node manually. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WHl5YoNcB9N3"
      },
      "source": [
        "In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "x0RObS7SB9N3"
      },
      "source": [
        "<img src=\"http://cocl.us/general_neural_network\" alt=\"Neural Network General\" width=600px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "700NMpS1B9N5"
      },
      "source": [
        "#### Initialize a Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "ZFV6NdItB9N5"
      },
      "source": [
        "Let's start by formally defining the structure of the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPkw4WNfSIn-"
      },
      "source": [
        "n = 2 # number of inputs\r\n",
        "num_hidden_layers = 2 # number of hidden layers\r\n",
        "m = [2, 2] # number of nodes in each hidden layer\r\n",
        "num_nodes_output = 1 # number of nodes in the output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "LuAHPFKoB9N5"
      },
      "source": [
        "Now that we defined the structure of the network, let's go ahead and inititailize the weights and the biases in the network to random numbers. In order to be able to initialize the weights and the biases to random numbers, we will need to import the **Numpy** library.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvqGernBS6qr",
        "outputId": "35153074-2edf-430d-b641-799c7ba128f2"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "num_nodes_prev = n # number of nodes in the previous layer\r\n",
        "\r\n",
        "network = {} # initialize network an an empty dictionary\r\n",
        "\r\n",
        "# loop through each layer and randomly initialize the weights and biases associated with each node\r\n",
        "# notice how we are adding 1 to the number of hidden layers in order to include the output layer\r\n",
        "for layer in range(num_hidden_layers+1):\r\n",
        "\r\n",
        "  # determine name of layer\r\n",
        "  if layer == num_hidden_layers:\r\n",
        "    layer_name = \"output\"\r\n",
        "    num_nodes = num_nodes_output\r\n",
        "  else:\r\n",
        "    layer_name = \"layer_{}\".format(layer+1)\r\n",
        "    num_nodes = m[layer]\r\n",
        "  \r\n",
        "  # initialize weights and biases associated with each node in the current layer\r\n",
        "  network[layer_name] = {}\r\n",
        "  for node in range(num_nodes):\r\n",
        "    node_name = \"node_{}\".format(node+1)\r\n",
        "    network[layer_name][node_name] = {\"weights\":np.round(np.random.uniform(size=num_nodes_prev),2),\r\n",
        "                                      \"bias\":np.round(np.random.uniform(size=1),2)}\r\n",
        "\r\n",
        "    num_nodes_prev = num_nodes\r\n",
        "\r\n",
        "print(network)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'layer_1': {'node_1': {'weights': array([0.55, 0.72]), 'bias': array([0.6])}, 'node_2': {'weights': array([0.54, 0.42]), 'bias': array([0.65])}}, 'layer_2': {'node_1': {'weights': array([0.44, 0.89]), 'bias': array([0.96])}, 'node_2': {'weights': array([0.38, 0.79]), 'bias': array([0.53])}}, 'output': {'node_1': {'weights': array([0.57, 0.93]), 'bias': array([0.07])}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Q-jOwsipB9N6"
      },
      "source": [
        "Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zc_NsknF2wQ"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWbSx4yBajl2"
      },
      "source": [
        "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\r\n",
        "  num_nodes_previous = num_inputs # number of nodes in the previous layer\r\n",
        "  network={}\r\n",
        "\r\n",
        "# loop through each layer and randomly initialize the weights and biases associated with each layer\r\n",
        "  for layer in range(num_hidden_layers+1):\r\n",
        "    \r\n",
        "    if layer == num_hidden_layers:\r\n",
        "      layer_name = \"output\" # name last layer in the network output\r\n",
        "      num_nodes = num_nodes_output\r\n",
        "    else:\r\n",
        "      layer_name = \"layer_{}\".format(layer+1)  # otherwise give the layer a number\r\n",
        "      num_nodes = num_nodes_hidden[layer]\r\n",
        "\r\n",
        "     #initialize weights and bias for each node\r\n",
        "    network[layer_name] = {}\r\n",
        "    for node in range(num_nodes):\r\n",
        "        node_name = \"node_{}\".format(node+1)\r\n",
        "        network[layer_name][node_name] = {\"weights\":np.round(np.random.uniform(size=num_nodes_previous),2),\r\n",
        "                                          \"bias\":np.round(np.random.uniform(size=1),2)} \r\n",
        "                                        \r\n",
        "    num_nodes_previous = num_nodes\r\n",
        "\r\n",
        "  return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "etq-MmnCB9N7"
      },
      "source": [
        "##### Use the _initialize_network_ function to create a network that:\n",
        "\n",
        "1.  takes 5 inputs\n",
        "2.  has three hidden layers\n",
        "3.  has 3 nodes in the first layer, 2 nodes in the second layer, and 3 nodes in the third layer\n",
        "4.  has 1 node in the output layer\n",
        "\n",
        "Call the network **small_network**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUAglMnKFEOD",
        "outputId": "eb0af48b-796b-4a55-921b-85612c0a260a"
      },
      "source": [
        "small_network = initialize_network(5,3,[3,2,3],1)\r\n",
        "small_network"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layer_1': {'node_1': {'bias': array([0.65]),\n",
              "   'weights': array([0.55, 0.72, 0.6 , 0.54, 0.42])},\n",
              "  'node_2': {'bias': array([0.53]),\n",
              "   'weights': array([0.44, 0.89, 0.96, 0.38, 0.79])},\n",
              "  'node_3': {'bias': array([0.83]),\n",
              "   'weights': array([0.57, 0.93, 0.07, 0.09, 0.02])}},\n",
              " 'layer_2': {'node_1': {'bias': array([0.8]),\n",
              "   'weights': array([0.78, 0.87, 0.98])},\n",
              "  'node_2': {'bias': array([0.64]), 'weights': array([0.46, 0.78, 0.12])}},\n",
              " 'layer_3': {'node_1': {'bias': array([0.52]), 'weights': array([0.14, 0.94])},\n",
              "  'node_2': {'bias': array([0.77]), 'weights': array([0.41, 0.26])},\n",
              "  'node_3': {'bias': array([0.02]), 'weights': array([0.46, 0.57])}},\n",
              " 'output': {'node_1': {'bias': array([0.94]),\n",
              "   'weights': array([0.62, 0.61, 0.62])}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "p7MoieHEB9N8"
      },
      "source": [
        "#### Compute Weighted Sum at Each Node\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "qGPf2C9uB9N9"
      },
      "source": [
        "The weighted sum at each node is computed as the dot product of the inputs and the weights plus the bias. So let's create a function called _compute_weighted_sum_ that does just that.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXkd01iqItp5"
      },
      "source": [
        "def weighted_sum(inputs,weights,bias):\r\n",
        "  return np.sum(inputs*weights)+bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "ajeKPipiB9N9"
      },
      "source": [
        "Let's generate 5 inputs that we can feed to **small_network**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-561alCN2f1",
        "outputId": "b43b64a3-96e1-4b0b-cba1-ce5e33b4a4fb"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy.random import seed\r\n",
        "\r\n",
        "seed(12)\r\n",
        "inputs = np.round(np.random.uniform(size=5),2)\r\n",
        "inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.15, 0.74, 0.26, 0.53, 0.01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "8dQTMLcAB9N-"
      },
      "source": [
        "##### Use the _compute_weighted_sum_ function to compute the weighted sum at the first node in the first hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAlcqzcsOkfD",
        "outputId": "b9b165a3-76f4-4586-b282-e2b174d47a00"
      },
      "source": [
        "node_weights = small_network[\"layer_1\"][\"node_1\"][\"weights\"]\r\n",
        "node_bias = small_network[\"layer_1\"][\"node_1\"][\"bias\"]\r\n",
        "\r\n",
        "weighted_sum_11 = weighted_sum(inputs,node_weights,node_bias)\r\n",
        "weighted_sum_11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.7117])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "XmQnZuwBB9N_"
      },
      "source": [
        "#### Compute Node Activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "LixYhvqrB9N_"
      },
      "source": [
        "Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XukcmkvxQIyB"
      },
      "source": [
        "def node_activation(z):\r\n",
        "  return (1/(1+np.exp(-z)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "_kZnhys3B9OA"
      },
      "source": [
        "##### Use the _node_activation_ function to compute the output of the first node in the first hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yReCrw8FQbbZ",
        "outputId": "49856cd1-1f01-4ef5-a29c-e18be0a91012"
      },
      "source": [
        "a11 = np.round(node_activation(weighted_sum_11),4)\r\n",
        "a11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8471])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auFYfewsRM2j",
        "outputId": "1ffd107c-0bb5-44fe-88fd-4787dd5e8c4f"
      },
      "source": [
        "#generalised output\r\n",
        "node_output = np.round(node_activation(weighted_sum(inputs,node_weights,node_bias)),4)\r\n",
        "node_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8471])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "sJkamyK8B9OC"
      },
      "source": [
        "#### Forward Propagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4yCeKMFHB9OC"
      },
      "source": [
        "The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the _compute_weighted_sum_ and _node_activation_ functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "5gMrvYIDB9OD"
      },
      "source": [
        "The way we are going to accomplish this is through the following procedure:\n",
        "\n",
        "1.  Start with the input layer as the input to the first hidden layer.\n",
        "2.  Compute the weighted sum at the nodes of the current layer.\n",
        "3.  Compute the output of the nodes of the current layer.\n",
        "4.  Set the output of the current layer to be the input to the next layer.\n",
        "5.  Move to the next layer in the network.\n",
        "6.  Repeat steps 2 - 4 until we compute the output of the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDB4ShtgS3w9",
        "outputId": "a072cfc8-e087-471a-d1ad-927f7d0cffa9"
      },
      "source": [
        "small_network"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layer_1': {'node_1': {'bias': array([0.65]),\n",
              "   'weights': array([0.55, 0.72, 0.6 , 0.54, 0.42])},\n",
              "  'node_2': {'bias': array([0.53]),\n",
              "   'weights': array([0.44, 0.89, 0.96, 0.38, 0.79])},\n",
              "  'node_3': {'bias': array([0.83]),\n",
              "   'weights': array([0.57, 0.93, 0.07, 0.09, 0.02])}},\n",
              " 'layer_2': {'node_1': {'bias': array([0.8]),\n",
              "   'weights': array([0.78, 0.87, 0.98])},\n",
              "  'node_2': {'bias': array([0.64]), 'weights': array([0.46, 0.78, 0.12])}},\n",
              " 'layer_3': {'node_1': {'bias': array([0.52]), 'weights': array([0.14, 0.94])},\n",
              "  'node_2': {'bias': array([0.77]), 'weights': array([0.41, 0.26])},\n",
              "  'node_3': {'bias': array([0.02]), 'weights': array([0.46, 0.57])}},\n",
              " 'output': {'node_1': {'bias': array([0.94]),\n",
              "   'weights': array([0.62, 0.61, 0.62])}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UiESnwSAfB"
      },
      "source": [
        "def forward_propagate(network,inputs):\r\n",
        "  layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\r\n",
        "    \r\n",
        "  for layer in network:\r\n",
        "      layer_data = network[layer]\r\n",
        "      \r\n",
        "      layer_outputs = []\r\n",
        "      for node in layer_data:\r\n",
        "        node_data = layer_data[node]\r\n",
        "\r\n",
        "        #compute the weighted sum and the output of each node at same time\r\n",
        "        node_output = node_activation(weighted_sum(layer_inputs,node_data[\"weights\"],node_data[\"bias\"]))\r\n",
        "        layer_outputs.append(np.round(node_output[0],4))\r\n",
        "\r\n",
        "      if layer != \"output\":\r\n",
        "        print(\"the outputs of the nodes in hidden layer number {} is {}\".format(layer.split(\"_\")[1],layer_outputs))\r\n",
        "\r\n",
        "      layer_inputs = layer_outputs ## set the output of this layer to be the input to next layer\r\n",
        "\r\n",
        "  network_predictions = layer_outputs\r\n",
        "  return network_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "QE9Sh0QfB9OD"
      },
      "source": [
        "##### Use the _forward_propagate_ function to compute the prediction of our small network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4u3DwfXUrM",
        "outputId": "21b9a99c-21c4-421e-f089-4cb7edf1eb7e"
      },
      "source": [
        "forward_propagate(small_network,inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the outputs of the nodes in hidden layer number 1 is [0.8471, 0.8473, 0.8415]\n",
            "the outputs of the nodes in hidden layer number 2 is [0.9536, 0.8571]\n",
            "the outputs of the nodes in hidden layer number 3 is [0.8114, 0.7996, 0.7206]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9151]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "kzLeDHtIB9OE"
      },
      "source": [
        "So we built the code to define a neural network. We can specify the number of inputs that a neural network can take, the number of hidden layers as well as the number of nodes in each hidden layer, and the number of nodes in the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "6ENEkXU7B9OE"
      },
      "source": [
        "We first use the _initialize_network_ to create our neural network and define its weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "TP1RtZ6TB9OE"
      },
      "source": [
        "my_network = initialize_network(5, 3, [2, 3, 2], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "hYGX0wMwB9OE"
      },
      "source": [
        "Then, for a given input,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4bJK3lOEB9OE"
      },
      "source": [
        "inputs = np.around(np.random.uniform(size=5), decimals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "QM97me_lB9OE"
      },
      "source": [
        "we compute the network predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "collapsed": true,
        "deletable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "BrwstO45B9OF"
      },
      "source": [
        "predictions = forward_propagate(my_network, inputs)\n",
        "print('The predicted values by the network for the given input are {}'.format(predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "3H4Lt3t6B9OF"
      },
      "source": [
        "Feel free to play around with the code by creating different networks of different structures and enjoy making predictions using the _forward_propagate_ function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbFfkaJ8n-7W"
      },
      "source": [
        "##Training a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvhetLWDoFNF"
      },
      "source": [
        "###Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMcd1KpWoJM2"
      },
      "source": [
        ">Let's say that we have some data, and here's a scatter plot of the data. For simplicity, I have generated data where z is twice x. And say we want to find the value of w that would generate a line that best fits this data. To do that, we define a cost or a loss function. One common cost or loss function is the one shown here as J, where we take the difference between the z values and the product of w and x's. We square that and we sum the squared difference across all values of z and x. The best value of w would then be the value that results in the minimum value of this cost or loss function. Let's take a look at how this cost function looks like. What makes this loss or cost function attractive is that it is a parabola, and has one global minimum or one unique solution. For the given data, the value of w that makes this cost function minimum, is w equals 2, meaning z equals 2x, which would result in a line that fits the points perfectly. This is a very simplified example as in real world datasets, the target variable z would be dependent on more than one variable and we can't just simply plot the cost function and visually determine the best value of the weights. So how do we determine the best value of w. The best value of w, or w's in case you have many weights to optimize, is determined through an algorithm called gradient descent. Gradient descent is an iterative optimization algorithm for finding the minimum of a function. To find the minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. What does that mean? We start at a random initial value of w. Let's call it w0, and say it's equal to 0.2, and we start taking steps towards the green dot which is w = 2. To determine in which direction to move, we compute the gradient of the loss function at the current value of w, which is 0.2. The gradient is given by the slope of the tangent at w = 0.2, and then the magnitude of the step is controlled by a parameter called the learning rate. The larger the learning rate, the bigger the step we take, and the smaller the learning rate, the smaller the step we take. Then we take the step and we move to w1. w1 is essentially computed as w0 minus the learning rate times the gradient at w0. This represents the first iteration of the algorithm. At w1, we repeat the same process of computing the gradient at w1 and using the same learning rate to control the magnitude of the step towards the minimum. We keep repeating this step again and again until we hit the minimum or a value of the cost function that is very close to the minimum, within a very small predefined threshold. Now when choosing the learning rate, we have to be very careful as a large learning rate can lead to big steps and eventually missing the minimum. On the other hand, a small learning rate can result in very small steps and therefore causing the algorithm to take a long time to find the minimum point. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(861).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29gMjhpGpIEy"
      },
      "source": [
        ">Now let's see how each iteration with a learning rate of 0.4 affects the way the resulting line fits the data on the left. We initialize w to 0, which means z equals 0. It is a horizontal line and therefore the cost is high and the line as you can see is a bad fit. After the 1st iteration, w moves closer to 2 and because the slope is very steep at w=0, the new value of w results in a big drop in the loss function. The resulting line fits better than the initial one but there is still room for improvement. After the 2nd iteration, w continues moving toward w = 2. Because the slope is not as steep as before, the step is not as big but the cost function still drops in value and the resulting line is moving closer to the ideal best fit line. The same observation is noted with the 3rd iteration, and the 4th iteration. After 4 iterations, you can see how we are almost there at w = 2, and the resulting line almost fits the scatterplot perfectly. With each iteration, the weight is updated in a way that's proportional to the negative of the gradient of the function at the current point. Therefore, if you initialize the weight to a value that is to the right of the minimum, then the positive gradient will result in w moving to the left towards the minimum. Now that we understand how to optimize a parameter that a function depends on, we are now ready to start learning about backpropagation and how neural networks learn and optimize their weights and biases.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(862).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQoYJmHfsNYo"
      },
      "source": [
        "###Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOobhKwfsas2"
      },
      "source": [
        ">In the previous videos, when discussing how neural networks make predictions using forward propagation, we assumed that the network had optimized weights and biases. But how do neural networks train and optimize their weights and biases for a given problem and data set? Training is done in a supervised learning setting, where each data point has a corresponding label or ground truth. And training is needed when the predicted value by the neural network obviously does not match the ground truth for a given input. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(863).png?raw=true)\r\n",
        "\r\n",
        ">The training process starts by calculating the error E between the predicted values and the ground truth labels. This error now represents the cost or the loss function that we discussed in the gradient descent video. Therefore, the next step would be to propagate this error back into the network and use it to perform gradient descent on the different weights and biases in the network, to optimize them using the same equations that we saw in the gradient descent video.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(864).png?raw=true) \r\n",
        "\r\n",
        "\r\n",
        ">So for our simple network with only two neurons and that takes one input, we calculate the squared error between the ground truth T and the predicted value a2. This represents our cost or loss function. Of course normally you don't train your network using one data point but thousands and thousands of data points. Therefore, the error is computed as the mean squared error calculated as follows. Then, we use the error to update w2, b2, w1, and b1 by propagating the error back into the network.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(867).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">For updating w2, we know from our gradient descent video that we need to use this equation to do that, but since the error is not explicitly a function of w2, then we will need to use the chain rule to establish the derivative of the error with respect to w2. So we know that E is a function of a2, and we know that a2 is a function of z2, and z2 is a function of w2. Therefore, we can take the derivative of E with respect to a2, and take the derivative of a2 with respect to z2, and take the derivative of z2 with respect to w2. Then the derivative of the error with respect to w2 would be simply the product of these individual derivatives, like so. The derivative of E with respect to a2 is -(T - a2) and the derivative of a2 with respect to z2 is a2(1- a2) and the derivative of z2 with respect to w2 is just the input a1. Therefore, w2 is updated as per the following equation. I'm assuming that you a're familiar with differentiation, so I am going to skip showing you how to find the derivatives, but below the video you will find a document that I prepared showing you in details how to find each of the derivatives shown here. Updating b2 is exactly the same. The only difference is the derivative of z2 with respect to b2 is 1 and not the input a1. Similarly, we use this expression to update w1, but the error is not explicitly related to w1. So we again use the chain rule to establish the derivative of the error with respect to w1. We know that E is a function of a2, and we know that a2 is a function of z2 and z2 is a function of a1, and a1 is a function of z1, and z1 is a function of w1. Therefore, we can take the derivative of E with respect to a2, and take the derivative of a2 with respect to z2, and take the derivative of z2 with respect to a1, and take the derivative of a1 with respect to z1, and then finally take the derivative of z1 with respect to w1. Then the derivative of the error with respect to w1 would simply be the product of these individual derivatives, like so. The derivative of E with respect to a2 is -(T - a2). The derivative of a2 with respect to z2 is a2(1 - a2), and the derivative of z2 with respect to a1 is w2, and the derivative of a1 with respect to z1 is a1(1 - a1), and the derivative of z1 with respect to w1 is the input x1. Therefore, w1 is updated as per the following equation. Using the same approach we figure out the expression to update b1.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_001.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_002.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_003.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Partial_Derivatives_Backpropagation_004.png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(868).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(869).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(870).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(871).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(872).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(873).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Now let's apply back propagation to our example from the forward propagation video. Recall that we have a network of two neurons, with the initial values of the weights and the biases as shown. We apply forward propagation and we calculate the value of z1, which we found to be 0.415, and the value of a1, which we found to be 0.6023, the value of z2 which we found to be 0.9210, and the value of a2 which we found to be 0.7153. This is the predicted value by the network for an input of 0.1. Now let's assume that the ground truth is 0.25. We first compute the error between the ground truth and the predicted value. Next, we will start updating the weights and biases for either a predefined number of iterations or epochs, like 1,000 for example, or until the error is within a predefined threshold of 0.001 for example. Using a learning rate of 0.4, let's see how the weights and the biases change after the first iteration.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(874.png?raw=true)\r\n",
        "\r\n",
        ">We already know the expression to update w2, so we simply plug in the values of T, a2, and a1 to calculate the gradient. The gradient of the error with respect to w2 is 0.05706, and therefore, w2 gets updated to 0.427 We repeat the same thing for b2. We plug in the values of T and a 2 to calculate the gradient. The gradient of the error with respect to b2 is 0.0948, and therefore, b2 gets updated to 0.612. Similarly, for w1, we plug in the values of T, a2, and w2, a1, and x1 to calculate the gradient. The gradient of the error with respect to w1 is 0.001021, and therefore, w1 gets updated to 0.1496. As for b1, the gradient of the error with respect to b1 is 0.01021, and therefore b1 gets updated to 0.3959. This completes the first iteration or epoch of the training process. With the updated weights and biases, we do another round of forward propagation calculate the predicted value and compare it to the ground truth. Calculate the error and do another round of backpropagation \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(875).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(876).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(877).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(878).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Therefore, to summarize the training algorithm: first, we initialize the weights and biases to random values. Then, we iteratively repeat the following steps: 1. Calculate the network output using forward propagation. 2. Calculate the error between the ground truth and the estimated or predicted output of the network. 3. Update the weights and the biases through backpropagation. Repeat the above three steps until the number of iterations or epochs is reached or the error between the ground truth and the predicted output is below a predefined threshold. In the next video, we will continue our discussion of the backpropagation algorithm and point out a serious shortcoming of the sigmoid function when used as the activation function in the hidden layers of deep networks.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(879).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6a1ILF1tqPS"
      },
      "source": [
        "###Vanishing Gradient\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4trvFuCtsiR"
      },
      "source": [
        ">In this video, we will discuss a problem with the sigmoid activation function that prevented neural networks from booming sooner. This problem is the vanishing gradient problem. Recall from the previous video, with a very simple network of two neurons only, the derivatives of the error with respect to the weights were as follows. See how small the gradients are, but more importantly, how small the gradient of the error with respect to w1. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(880).png?raw=true)\r\n",
        "\r\n",
        ">Well it turns out that because we are using the sigmoid function as the activation function, then all the intermediate values in the network are between 0 and 1. So when we do backpropagation, we keep multiplying factors that are less than 1 by each other, and so their gradients tend to get smaller and smaller as we keep on moving backward in the network. This means that the neurons in the earlier layers learn very slowly as compared to the neurons in the later layers in the network. The earlier layers in the network are the slowest to train. The result is a training process that takes too long and a prediction accuracy that is compromised. Accordingly, this is the reason why we do not use the sigmoid function or similar functions as activation functions, since they are prone to the vanishing gradient problem. Therefore, in the next video we will learn about other activation functions that became so popular and are now the activation functions that get used almost all the time in the hidden layers, since they help in overcoming the vanishing gradient problem.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(881).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPRMXjMUu2Kl"
      },
      "source": [
        "###Activation Functions\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbhK1rNvUka"
      },
      "source": [
        ">As we discussed earlier, activation functions play a major role in the learning process of a neural network. So far, we have used only the sigmoid function as the activation function in our networks, but we saw how the sigmoid function has its shortcomings since it can lead to the vanishing gradient problem for the earlier layers. In this video, we will discuss other activation functions; ones that are more efficient to use and are more applicable to deep learning applications. There are seven types of activation functions that you can use when building a neural network. There is the binary step function, the linear or identity function, there is our old friend the sigmoid or logistic function, there is the hyperbolic tangent, or tanh, function, the rectified linear unit (ReLU) function, the leaky ReLU function, and the softmax function. In this video, we will discuss the popular ones, which are the sigmoid, the hyperbolic tangent, ReLU, and the softmax functions.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(882).png?raw=true)\r\n",
        "\r\n",
        ">This is the sigmoid function. At z = 0, a is equal to 0.5 and when z is a very large positive number, a is close to 1, and when z is a very large negative number, a is close to zero. Sigmoid functions used to be widely used as activation functions in the hidden layers of a neural network. However, as you can see, the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region, the gradients become very small. This results in the vanishing gradient problem that we discussed, and as the gradients approach 0, the network doesn't really learn. Another problem with the sigmoid function is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin. The values received are all positive. Well, not all the times would we desire that values going to the next neuron be all of the same sign. This can be addressed by scaling the sigmoid function, and this brings us to the next activation function: the hyperbolic tangent function. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(883).png?raw=true)\r\n",
        "\r\n",
        ">This is the hyperbolic tangent, or tanh, function. It is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function, but unlike the sigmoid function, it's symmetric over the origin. It ranges from -1 to +1. However, although it overcomes the lack of symmetry of the sigmoid function, it also leads to the vanishing gradient problem in very deep neural networks.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(884).png?raw=true)\r\n",
        "\r\n",
        ">The rectified linear unit, or ReLU, function is the most widely used activation function when designing networks today. In addition to it being nonlinear, the main advantage of using the ReLU, function over the other activation functions is that it does not activate all the neurons at the same time. According to the plot here, if the input is negative it will be converted to 0, and the neuron does not get activated. This means that at a time, only a few neurons are activated, making the network sparse and very efficient. Also, the ReLU function was one of the main advancements in the field of deep learning that led to overcoming the vanishing gradient problem. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(885).png?raw=true)\r\n",
        "\r\n",
        ">One last activation function that we will discuss here is the softmax function. The softmax function is also a type of a sigmoid function, but it is handy when we are trying to handle classification problems. The softmax function is ideally used in the output layer of the classifier where we are actually trying to get the probabilities to define the class of each input. So, if a network with 3 neurons in the output layer outputs [1.6, 0.55, 0.98] then with a softmax activation function, the outputs get converted to [0.51, 0.18, 0.31]. This way, it is easier for us to classify a given data point and determine to which category it belongs.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(886).png?raw=true)\r\n",
        "\r\n",
        ">In conclusion, the sigmoid and the tanh functions are avoided in many applications nowadays since they can lead to the vanishing gradient problem. The ReLU function is the function that's widely used nowadays, and it's important to note that it is only used in the hidden layers. Finally, when building a model, you can begin with using the ReLU function and then you can switch to other activation functions if the ReLU function does not yield a good performance. And this concludes this video on activation functions. I'll see you in the next video.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(887).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtBrlmewfdhy"
      },
      "source": [
        "##Keras and Deep Learning Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUidctwlk6is"
      },
      "source": [
        "###Deep Learning Libraries\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-T9t7QGk6-h"
      },
      "source": [
        ">Before we can start building deep learning networks, we will spend some time learning about the different deep learning libraries and frameworks that are out there. In this video, I will briefly cover the libraries that we'll be teaching in this specialization. The most popular library is in descending order are TensorFlow, Keras, and PyTorch. There is also Theano, a library developed by the Montreal Institute for Learning Algorithms, and was the major library for deep learning development even before TensorFlow and PyTorch. However, the founders can't afford to continuously support it and maintain it, and therefore, the library lost its popularity. Because of that, in this specialization, we will focus on the three other popular libraries.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(888).png?raw=true)\r\n",
        "\r\n",
        ">Among the three libraries. TensorFlow is the most popular one. It is the library that is mostly used in production of deep learning models. It has a very large community. Just a quick look at the number of forks on the library's Github repository as well as the number of commits and pull requests should suffice in giving you an idea of how popular the library is. Tensorflow was developed by Google and released to the public in 2015, and is still being actively used at Google for both research and production needs. PyTorch on the other hand, is the cousin of the Torch framework, which is in Lua, and supports machine learning algorithms running on GPUs in particular. However being derived from the Torch framework, PyTorch isn't just a set of wrappers to support a popular language like Python. Tt was actually rewritten and tailored to be fast and feel native. PyTorch was released in 2016 and has gained immense interest lately and is becoming the preferred language over TensorFlow, especially in academic research settings and applications of deep learning requiring optimizing custom expressions. PyTorch is supported and being actively used at Facebook. However, despite their popularity, both PyTorch and TensorFlow are not easy to use, and have a steep learning curve. So for people who are just starting to learn deep learning, there is no better library to use other than the Keras library. Keras is a high level API for building deep learning models. It has gained favor for its ease of use and syntactic simplicity facilitating fast development. As you'll see in the next couple of videos, building a very complex deep learning network can be achieved with Keras with only few lines of code. Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Kares library, you will have to install TensorFlow first, and when you import Keras, it will be explicitly displayed what backend was used to install the Keras library. Keras is also supported by Google. I won't go into more details about the different libraries, but the take home message here is if you're interested in building something quickly go with the Keras library; you won't be disappointed. However, if you want to have more control over the different nodes and layers in the network, and want to watch closely what happens with the network over time, then PyTorch or TensorFlow would be the right library. It will really boil down to your personal preference. With that, in the next videos, we will start learning how to use the Keras library to build models for regression and classification problems.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(889).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1KVjUWl2RJ"
      },
      "source": [
        "###Regression Models with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4PRFRft0cxl"
      },
      "source": [
        ">Let's take a look at a regression example. Here is a data set of the compressive strength of different samples of concrete based on the volumes of the different materials that were used to make them. So the first concrete sample has 540 cubic meter of cement, 0 cubic meter of blast furnace slag, 0 cubic meter of fly ash, 162 cubic meter of water, 2.5 cubic meter of superplasticizer, 1040 cubic meter of coarse aggregate, and 676 cubic meter of fine aggregate. Such a concrete mix which is 28 days old has a compressive strength of 79.99 MPa. The data is in a pandas dataframe and named concrete_data. So let's say we would like to use the Keras library to quickly build a deep neural network to model this dataset, and so we can automatically determine the compressive strength of a given concrete sample based on its ingredients.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(890).png?raw=true)\r\n",
        "\r\n",
        ">So let's say that the deep neural network that we would like to create takes all the eight features as input, feeds them into a hidden layer of five nodes, which is connected to another hidden layer of five nodes, which is then connected to the output layer with one node that is supposed to output the compressive strength of a given concrete sample. Note that usually you would go with a much larger number of neurons in each hidden layer like 50 or even 100, but we're just using a small network for simplicity. Notice how all the nodes in one layer are connected to all the other nodes in the next layer. Such a network is called a dense network.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(891).png?raw=true)\r\n",
        "\r\n",
        ">Before we begin using the Keras library, let's prepare our data and have it in the right format. The only thing we would need to do is to split the dataframe into two dataframes, one that has the predictors columns and another one that has the target column. We will also name them predictors and target. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(893).png?raw=true)\r\n",
        "\r\n",
        ">Now prepare to see the magic of Keras and how building such a network and training it and using it to predict new samples can be achieved with only few lines of code. The first thing you will need to do is import Keras and the Sequential model from \"keras.models\". Because our network consists of a linear stack of layers, then the Sequential model is what you would want to use. This is the case most of the time unless you are building something out of the ordinary. There are two models in the Keras library. One of them is the Sequential model and the other one is the model class used with the functional API. So to create your model, you simply call the Sequential constructor. Now building your layers is pretty straightforward as well. For that, we would need to import the \"Dense\" type of layers from \"keras.layers\". Then we use the add method to add each dense layer. We specify the number of neurons in each layer and the activation function that we want to use. As per our discussion in the video on activation functions, ReLU is one of the recommended activation functions for hidden layers, so we will use that. And for the first hidden layer we need to pass in the \"input_shape\" parameter, which is the number of columns or predictors in our dataset. Then we repeat the same thing for the other hidden layer, of course without the input_shape parameter, and we create our output layer with one node. Now for training, we need to define an optimizer and the error metric. In the previous module, we used gradient descent as our minimization or optimization algorithm, and the mean squared error as our loss measure between the predicted value and the ground truth. So we will stick with that and use the mean squared error as our loss measure. As for the minimization algorithm, there are actually other more efficient algorithms than the gradient descent for deep learning applications. One of them is \"adam\". One of the main advantages of the \"adam\" optimizer is that you don't need to specify the learning rate that we saw in the gradient descent video. So this saves us the task of optimizing the learning rate for our model. Then we use the fit method to train our model. Once training is complete, we can start making predictions using the predict method.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(894).png?raw=true)\r\n",
        "\r\n",
        "- [Keras Activation Functions:](https://keras.io/activations/)\r\n",
        "\r\n",
        "- [Keras Models:](https://keras.io/models/about-keras-models/#about-keras-models)\r\n",
        "\r\n",
        "- [Keras Optimizers:](https://keras.io/optimizers/)\r\n",
        "\r\n",
        "- [Keras Metrics:](https://keras.io/metrics/)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ5s8mg43YMk"
      },
      "source": [
        "###Classification Models with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVbzwzrA39GW"
      },
      "source": [
        ">Let's say that we would like to build a model that would inform someone whether purchasing a certain car would be a good choice based on the price of the car, the cost to maintain it, and whether it can accommodate two or more people. So, here is a dataset that we are calling \"car_data\". I already cleaned the data, as you can see, where I used one-hot encoding to transform each category of price, maintenance, and how many people the car can accommodate, into separate columns. So the price of the car can be either high, medium, or low. Similarly, the cost of maintaining the car can also be high, medium, or low, and the car can either fit two people or more. If you take the first car in the dataset, it is considered an expensive car, has high maintenance cost, and can fit only two people. The decision is 0, meaning that buying this car would be a bad choice. A decision of 1 means that buying the car is acceptable, a decision of 2 means that buying the car would be a good decision, and a decision of 3 means that buying the car would be a very good decision.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(895).png?raw=true)\r\n",
        "\r\n",
        ">Let's use the same neural network as the one we used for the regression problem that we discussed in the previous video. So a network that still takes eight inputs or predictors, consists of two hidden layers, each of five neurons, and an output layer.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(896).png?raw=true)\r\n",
        "\r\n",
        ">Next, let's divide our dataset into predictors and target. However, with Keras, for classification problems, we can't use the target column as is; we actually need to transform the column into an array with binary values similar to one-hot encoding like the output shown here. We easily achieve that using the \"to_categorical\" function from the Keras utilities package. In other words, our model instead of having just one neuron in the output layer, it would have four neurons, since our target variable consists of four categories. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(897).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(898).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(899).png?raw=true)\r\n",
        "\r\n",
        ">In terms of code, the structure of our code is pretty similar to the one we use to build the model for our regression problem. We start by importing the Keras library and the Sequential model and we use it to construct our model. We also import the \"Dense\" layer since we will be using it to build our network. The additional import statement here is the \"to_categorical\" function in order to transform our target column into an array of binary numbers for classification. Then, we proceed to constructing our layers. We use the add method to create two hidden layers, each with five neurons and the neurons are activated using the ReLU activation function. Notice how here we also specify the softmax function as the activation function for the output layer, so that the sum of the predicted values from all the neurons in the output layer sum nicely to 1. Then in defining our compiler, here we will use the categorical cross-entropy as our loss measure instead of the mean squared error that we use for regression, and we will specify the evaluation metric to be \"accuracy\". \"accuracy\" is a built-in evaluation metric in Keras but you can actually define your own evaluation metric and pass it in the metrics parameter. Then we fit the model. Notice how this time we're specifying the number of epochs for training the model. Although we didn't specify the number of epochs when we built a regression model, but we could have done that. Finally, we use the predict method to make predictions. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(900).png?raw=true)\r\n",
        "\r\n",
        ">Now the output of the Keras predict method would be something like what's shown here. For each data point, the output is the probability that the decision of purchasing a given car belongs to one of the four classes. For each data point, the probabilities should sum to 1, and the higher the probability the more confident is the algorithm that a datapoint belongs to the respective class. So for the first data point or the first car in the test set, the decision would be 0 meaning not acceptable, since the first probability is the highest, with a value of 0.99 or close to 1, in this case. Similarly, for the second datapoint, the decision is also 0 or not acceptable, since the probability for this class is the highest, again with a value of 0.99 or almost 1. For the first three datapoints, the model is very confident that purchasing these cars is not acceptable. As for the last three datapoints, the decision would be 1 or acceptable, since the probabilities for the second class are higher than the rest of the classes. But notice how the probabilities for decision 0 and decision 1 are very close. Therefore, the model is not very confident but it would lean towards accepting purchasing these cars.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(901).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFgL3tI499yQ"
      },
      "source": [
        "###LAB: Regression with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "yDkDCkVj90gQ"
      },
      "source": [
        "#### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "43Z_lsim90gS"
      },
      "source": [
        "As we discussed in the videos, despite the popularity of more powerful libraries such as PyToch and TensorFlow, they are not easy to use and have a steep learning curve. So, for people who are just starting to learn deep learning, there is no better library to use other than the Keras library. \n",
        "\n",
        "Keras is a high-level API for building deep learning models. It has gained favor for its ease of use and syntactic simplicity facilitating fast development. As you will see in this lab and the other labs in this course, building a very complex deep learning network can be achieved with Keras with only few lines of code. You will appreciate Keras even more, once you learn how to build deep models using PyTorch and TensorFlow in the other courses.\n",
        "\n",
        "So, in this lab, you will learn how to use the Keras library to build a regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abyLLOKm90gT"
      },
      "source": [
        "<h2>Regression Models with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. How to use the Keras library to build a regression model.</h5>\n",
        "<h5> 2. Download and Clean dataset </h5>\n",
        "<h5> 3. Build a Neural Network </h5>\n",
        "<h5> 4. Train and Test the Network. </h5>     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "10Mgly6890gT"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item31\">Download and Clean Dataset</a>  \n",
        "2. <a href=\"#item32\">Import Keras</a>  \n",
        "3. <a href=\"#item33\">Build a Neural Network</a>  \n",
        "4. <a href=\"#item34\">Train and Test the Network</a>  \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "h192U57P90gU"
      },
      "source": [
        "<a id=\"item31\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "tvKThLtP90gU"
      },
      "source": [
        "#### Download and Clean Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "qX23BDac90gU"
      },
      "source": [
        "Let's start by importing the <em>pandas</em> and the Numpy libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "FJYAWVaH90gV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "HH4M4nmJ90gV"
      },
      "source": [
        "We will be playing around with the same dataset that we used in the videos.\n",
        "\n",
        "<strong>The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:</strong>\n",
        "\n",
        "<strong>1. Cement</strong>\n",
        "\n",
        "<strong>2. Blast Furnace Slag</strong>\n",
        "\n",
        "<strong>3. Fly Ash</strong>\n",
        "\n",
        "<strong>4. Water</strong>\n",
        "\n",
        "<strong>5. Superplasticizer</strong>\n",
        "\n",
        "<strong>6. Coarse Aggregate</strong>\n",
        "\n",
        "<strong>7. Fine Aggregate</strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Cf7zBah490gW"
      },
      "source": [
        "Let's download the data and read it into a <em>pandas</em> dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYFOrCgD99h5"
      },
      "source": [
        "!wget -q https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Gdn4JmwPmEv6",
        "outputId": "5577fa8c-b647-4a22-992b-b0afc9b4a01c"
      },
      "source": [
        "df = pd.read_csv(\"concrete_data.csv\")\r\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQD40BhD_SvA",
        "outputId": "0fc7b7d4-b2d5-4f1d-9765-cb4809f53a85"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1030 entries, 0 to 1029\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Cement              1030 non-null   float64\n",
            " 1   Blast Furnace Slag  1030 non-null   float64\n",
            " 2   Fly Ash             1030 non-null   float64\n",
            " 3   Water               1030 non-null   float64\n",
            " 4   Superplasticizer    1030 non-null   float64\n",
            " 5   Coarse Aggregate    1030 non-null   float64\n",
            " 6   Fine Aggregate      1030 non-null   float64\n",
            " 7   Age                 1030 non-null   int64  \n",
            " 8   Strength            1030 non-null   float64\n",
            "dtypes: float64(8), int64(1)\n",
            "memory usage: 72.5 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFngEcP1_UYg",
        "outputId": "d9a774af-406d-411b-a55e-b1ca7b26bee5"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cement                0\n",
              "Blast Furnace Slag    0\n",
              "Fly Ash               0\n",
              "Water                 0\n",
              "Superplasticizer      0\n",
              "Coarse Aggregate      0\n",
              "Fine Aggregate        0\n",
              "Age                   0\n",
              "Strength              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "L4zkY8Me_i3-",
        "outputId": "3206a887-7391-4bae-c30c-aee41f8cbcda"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Y7xLOP5J90gY"
      },
      "source": [
        "The data looks very clean and is ready to be used to build our model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "_nTonJIl90gY"
      },
      "source": [
        "##### Split data into predictors and target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAAKBqXW90gZ"
      },
      "source": [
        "The target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQYPjBIv_wKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c562f38e-bb2e-4874-b188-74ef9f5074c7"
      },
      "source": [
        "df_columns = df.columns\r\n",
        "df_columns"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',\n",
              "       'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "AdBgD4xPIC8v",
        "outputId": "63ba0664-3261-4e32-9d37-a4ef0e77fb8a"
      },
      "source": [
        "x = df.loc[:,df.columns!='Strength']\r\n",
        "x"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>276.4</td>\n",
              "      <td>116.0</td>\n",
              "      <td>90.3</td>\n",
              "      <td>179.6</td>\n",
              "      <td>8.9</td>\n",
              "      <td>870.1</td>\n",
              "      <td>768.3</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>322.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.6</td>\n",
              "      <td>196.0</td>\n",
              "      <td>10.4</td>\n",
              "      <td>817.9</td>\n",
              "      <td>813.4</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>148.5</td>\n",
              "      <td>139.4</td>\n",
              "      <td>108.6</td>\n",
              "      <td>192.7</td>\n",
              "      <td>6.1</td>\n",
              "      <td>892.4</td>\n",
              "      <td>780.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>159.1</td>\n",
              "      <td>186.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>175.6</td>\n",
              "      <td>11.3</td>\n",
              "      <td>989.6</td>\n",
              "      <td>788.9</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>260.9</td>\n",
              "      <td>100.5</td>\n",
              "      <td>78.3</td>\n",
              "      <td>200.6</td>\n",
              "      <td>8.6</td>\n",
              "      <td>864.5</td>\n",
              "      <td>761.5</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Cement  Blast Furnace Slag  ...  Fine Aggregate  Age\n",
              "0      540.0                 0.0  ...           676.0   28\n",
              "1      540.0                 0.0  ...           676.0   28\n",
              "2      332.5               142.5  ...           594.0  270\n",
              "3      332.5               142.5  ...           594.0  365\n",
              "4      198.6               132.4  ...           825.5  360\n",
              "...      ...                 ...  ...             ...  ...\n",
              "1025   276.4               116.0  ...           768.3   28\n",
              "1026   322.2                 0.0  ...           813.4   28\n",
              "1027   148.5               139.4  ...           780.0   28\n",
              "1028   159.1               186.7  ...           788.9   28\n",
              "1029   260.9               100.5  ...           761.5   28\n",
              "\n",
              "[1030 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skGhXmuDIp4B",
        "outputId": "2a1698c4-2cf8-4af4-f328-f5782ea73708"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1030, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t_Sta-8Ieoa"
      },
      "source": [
        "y = df['Strength']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "7bD08RvK90ga"
      },
      "source": [
        "Finally, the last step is to normalize the data by substracting the mean and dividing by the standard deviation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Q9rMo1ugIj4M",
        "outputId": "108f3c1f-ad13-4291-b1d2-49c7c7bcf0e3"
      },
      "source": [
        "x_norm = (x-x.mean())/x.std()\r\n",
        "x_norm.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>0.862735</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>-0.279597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>1.055651</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>-0.279597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>3.551340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>5.055221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.790075</td>\n",
              "      <td>0.678079</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>0.488555</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>0.070492</td>\n",
              "      <td>0.647569</td>\n",
              "      <td>4.976069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Cement  Blast Furnace Slag  ...  Fine Aggregate       Age\n",
              "0  2.476712           -0.856472  ...       -1.217079 -0.279597\n",
              "1  2.476712           -0.856472  ...       -1.217079 -0.279597\n",
              "2  0.491187            0.795140  ...       -2.239829  3.551340\n",
              "3  0.491187            0.795140  ...       -2.239829  5.055221\n",
              "4 -0.790075            0.678079  ...        0.647569  4.976069\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPAGAoSWJEGB",
        "outputId": "47d05be5-05d6-4c1c-f255-252fbab8b4e8"
      },
      "source": [
        "#no of predictors\r\n",
        "n_cols = len(x_norm.columns)\r\n",
        "n_cols"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WL6MFCTM90gb"
      },
      "source": [
        "#### Import Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "qdeqJRT_90gc"
      },
      "source": [
        "Recall from the videos that Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Keras library, you will have to install TensorFlow first and when you import the Keras library, it will be explicitly displayed what backend was used to install the Keras library. In CC Labs, we used TensorFlow as the backend to install Keras, so it should clearly print that when we import Keras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "jNYd7q-190gc"
      },
      "source": [
        "##### Let's go ahead and import the Keras library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GNyL-mnJuzq"
      },
      "source": [
        "import keras"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "gpbtoG3990gc"
      },
      "source": [
        "As you can see, the TensorFlow backend was used to install the Keras library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "9PXaJr6390gc"
      },
      "source": [
        "Let's import the rest of the packages from the Keras library that we will need to build our regressoin model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBMyBRvJ619"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "lEQz14yo90gd"
      },
      "source": [
        "#### Build a Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "nR37vsKJ90gd"
      },
      "source": [
        "Let's define a function that defines our regression model for us so that we can conveniently call it to create our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg1DVA8WKLfa"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  #create model\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Dense(50,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(50,activation=\"relu\"))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model...specify optimisation function and cost function\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDVzIw5F90ge"
      },
      "source": [
        "The above function create a model that has two hidden layers, each of 50 hidden units.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "C2DPyH0k90gf"
      },
      "source": [
        "#### Train and Test the Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqsKW2kQ90gg"
      },
      "source": [
        "Let's call the function now to create our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nSbXpEXLj9m"
      },
      "source": [
        "model = regression_model()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoH2K4pH90gg"
      },
      "source": [
        "Next, we will train and test the model at the same time using the _fit_ method. We will leave out 30% of the data for validation and we will train the model for 100 epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tq_3dNzOcRE"
      },
      "source": [
        ">**Verbose:**   \r\n",
        "By default verbose = 1,\r\n",
        "\r\n",
        "- verbose = 1, which includes both progress bar and one line per epoch\r\n",
        "\r\n",
        "- verbose = 0, means silent\r\n",
        "\r\n",
        "- verbose = 2, one line per epoch i.e. epoch no./total no. of epochs\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ">validation_split:  \r\n",
        " Float between 0 and 1. The fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling.\r\n",
        "Cross-validation data is used to investigate whether your model over-fits the data or does not. This is what we can understand whether our model has generalization capability or not.\r\n",
        "\r\n",
        ">validation_data:   \r\n",
        " tuple (x_val, y_val) or tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This will override validation_split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DwYwMIWLq2-",
        "outputId": "8eae83f0-4613-448e-df5f-b579dbd4b6c3"
      },
      "source": [
        "#fit the model\r\n",
        "model.fit(x_norm,y,validation_split=0.3,epochs=100,verbose=2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/23 - 0s - loss: 17.1790 - val_loss: 117.1033\n",
            "Epoch 2/100\n",
            "23/23 - 0s - loss: 17.0617 - val_loss: 123.3737\n",
            "Epoch 3/100\n",
            "23/23 - 0s - loss: 16.6405 - val_loss: 124.2634\n",
            "Epoch 4/100\n",
            "23/23 - 0s - loss: 16.6252 - val_loss: 128.3106\n",
            "Epoch 5/100\n",
            "23/23 - 0s - loss: 16.5083 - val_loss: 118.9804\n",
            "Epoch 6/100\n",
            "23/23 - 0s - loss: 16.3251 - val_loss: 123.0360\n",
            "Epoch 7/100\n",
            "23/23 - 0s - loss: 16.3410 - val_loss: 121.8743\n",
            "Epoch 8/100\n",
            "23/23 - 0s - loss: 16.4991 - val_loss: 121.0805\n",
            "Epoch 9/100\n",
            "23/23 - 0s - loss: 16.7510 - val_loss: 121.8111\n",
            "Epoch 10/100\n",
            "23/23 - 0s - loss: 16.5290 - val_loss: 125.5427\n",
            "Epoch 11/100\n",
            "23/23 - 0s - loss: 16.1788 - val_loss: 127.1908\n",
            "Epoch 12/100\n",
            "23/23 - 0s - loss: 16.4590 - val_loss: 120.8689\n",
            "Epoch 13/100\n",
            "23/23 - 0s - loss: 16.1648 - val_loss: 119.2210\n",
            "Epoch 14/100\n",
            "23/23 - 0s - loss: 15.9418 - val_loss: 116.8745\n",
            "Epoch 15/100\n",
            "23/23 - 0s - loss: 15.8657 - val_loss: 115.3904\n",
            "Epoch 16/100\n",
            "23/23 - 0s - loss: 16.0210 - val_loss: 130.2403\n",
            "Epoch 17/100\n",
            "23/23 - 0s - loss: 16.2172 - val_loss: 127.5238\n",
            "Epoch 18/100\n",
            "23/23 - 0s - loss: 16.4940 - val_loss: 130.9841\n",
            "Epoch 19/100\n",
            "23/23 - 0s - loss: 15.9912 - val_loss: 125.9242\n",
            "Epoch 20/100\n",
            "23/23 - 0s - loss: 16.3901 - val_loss: 128.6080\n",
            "Epoch 21/100\n",
            "23/23 - 0s - loss: 15.8496 - val_loss: 125.3561\n",
            "Epoch 22/100\n",
            "23/23 - 0s - loss: 15.8237 - val_loss: 116.0531\n",
            "Epoch 23/100\n",
            "23/23 - 0s - loss: 15.5758 - val_loss: 120.9095\n",
            "Epoch 24/100\n",
            "23/23 - 0s - loss: 15.2481 - val_loss: 116.7772\n",
            "Epoch 25/100\n",
            "23/23 - 0s - loss: 15.6061 - val_loss: 124.6870\n",
            "Epoch 26/100\n",
            "23/23 - 0s - loss: 15.7026 - val_loss: 116.8156\n",
            "Epoch 27/100\n",
            "23/23 - 0s - loss: 15.7084 - val_loss: 111.6947\n",
            "Epoch 28/100\n",
            "23/23 - 0s - loss: 15.2827 - val_loss: 111.2006\n",
            "Epoch 29/100\n",
            "23/23 - 0s - loss: 15.1580 - val_loss: 113.4407\n",
            "Epoch 30/100\n",
            "23/23 - 0s - loss: 15.4221 - val_loss: 112.4613\n",
            "Epoch 31/100\n",
            "23/23 - 0s - loss: 14.9935 - val_loss: 120.9651\n",
            "Epoch 32/100\n",
            "23/23 - 0s - loss: 15.0817 - val_loss: 120.4396\n",
            "Epoch 33/100\n",
            "23/23 - 0s - loss: 14.9941 - val_loss: 108.4624\n",
            "Epoch 34/100\n",
            "23/23 - 0s - loss: 15.3270 - val_loss: 113.4323\n",
            "Epoch 35/100\n",
            "23/23 - 0s - loss: 16.2248 - val_loss: 117.2679\n",
            "Epoch 36/100\n",
            "23/23 - 0s - loss: 14.9389 - val_loss: 111.4484\n",
            "Epoch 37/100\n",
            "23/23 - 0s - loss: 14.7215 - val_loss: 111.6582\n",
            "Epoch 38/100\n",
            "23/23 - 0s - loss: 14.5297 - val_loss: 108.8954\n",
            "Epoch 39/100\n",
            "23/23 - 0s - loss: 14.8633 - val_loss: 111.2179\n",
            "Epoch 40/100\n",
            "23/23 - 0s - loss: 14.6935 - val_loss: 108.4619\n",
            "Epoch 41/100\n",
            "23/23 - 0s - loss: 14.8073 - val_loss: 99.6928\n",
            "Epoch 42/100\n",
            "23/23 - 0s - loss: 14.8939 - val_loss: 102.2421\n",
            "Epoch 43/100\n",
            "23/23 - 0s - loss: 14.7514 - val_loss: 105.9978\n",
            "Epoch 44/100\n",
            "23/23 - 0s - loss: 14.8169 - val_loss: 107.3738\n",
            "Epoch 45/100\n",
            "23/23 - 0s - loss: 14.5588 - val_loss: 107.3471\n",
            "Epoch 46/100\n",
            "23/23 - 0s - loss: 14.3578 - val_loss: 107.7220\n",
            "Epoch 47/100\n",
            "23/23 - 0s - loss: 14.5230 - val_loss: 108.7005\n",
            "Epoch 48/100\n",
            "23/23 - 0s - loss: 14.6552 - val_loss: 109.0493\n",
            "Epoch 49/100\n",
            "23/23 - 0s - loss: 14.5409 - val_loss: 102.6309\n",
            "Epoch 50/100\n",
            "23/23 - 0s - loss: 14.5692 - val_loss: 104.9035\n",
            "Epoch 51/100\n",
            "23/23 - 0s - loss: 14.8700 - val_loss: 112.3632\n",
            "Epoch 52/100\n",
            "23/23 - 0s - loss: 14.5734 - val_loss: 104.3658\n",
            "Epoch 53/100\n",
            "23/23 - 0s - loss: 14.6500 - val_loss: 111.6282\n",
            "Epoch 54/100\n",
            "23/23 - 0s - loss: 14.9577 - val_loss: 106.9401\n",
            "Epoch 55/100\n",
            "23/23 - 0s - loss: 14.4394 - val_loss: 106.2872\n",
            "Epoch 56/100\n",
            "23/23 - 0s - loss: 14.1220 - val_loss: 106.7646\n",
            "Epoch 57/100\n",
            "23/23 - 0s - loss: 14.2255 - val_loss: 104.2864\n",
            "Epoch 58/100\n",
            "23/23 - 0s - loss: 14.2379 - val_loss: 102.4789\n",
            "Epoch 59/100\n",
            "23/23 - 0s - loss: 14.0016 - val_loss: 111.0648\n",
            "Epoch 60/100\n",
            "23/23 - 0s - loss: 13.9787 - val_loss: 102.7890\n",
            "Epoch 61/100\n",
            "23/23 - 0s - loss: 14.0723 - val_loss: 109.4303\n",
            "Epoch 62/100\n",
            "23/23 - 0s - loss: 14.0656 - val_loss: 102.0480\n",
            "Epoch 63/100\n",
            "23/23 - 0s - loss: 14.4911 - val_loss: 98.3831\n",
            "Epoch 64/100\n",
            "23/23 - 0s - loss: 14.2595 - val_loss: 102.0425\n",
            "Epoch 65/100\n",
            "23/23 - 0s - loss: 13.8506 - val_loss: 104.2471\n",
            "Epoch 66/100\n",
            "23/23 - 0s - loss: 13.9719 - val_loss: 112.1980\n",
            "Epoch 67/100\n",
            "23/23 - 0s - loss: 14.4314 - val_loss: 98.9371\n",
            "Epoch 68/100\n",
            "23/23 - 0s - loss: 14.4488 - val_loss: 101.5803\n",
            "Epoch 69/100\n",
            "23/23 - 0s - loss: 13.6752 - val_loss: 105.4426\n",
            "Epoch 70/100\n",
            "23/23 - 0s - loss: 13.7898 - val_loss: 102.2314\n",
            "Epoch 71/100\n",
            "23/23 - 0s - loss: 13.7958 - val_loss: 110.8073\n",
            "Epoch 72/100\n",
            "23/23 - 0s - loss: 13.7249 - val_loss: 107.8474\n",
            "Epoch 73/100\n",
            "23/23 - 0s - loss: 13.7167 - val_loss: 101.1368\n",
            "Epoch 74/100\n",
            "23/23 - 0s - loss: 13.7036 - val_loss: 102.1868\n",
            "Epoch 75/100\n",
            "23/23 - 0s - loss: 13.9018 - val_loss: 109.7909\n",
            "Epoch 76/100\n",
            "23/23 - 0s - loss: 13.4595 - val_loss: 114.2139\n",
            "Epoch 77/100\n",
            "23/23 - 0s - loss: 14.0384 - val_loss: 97.9043\n",
            "Epoch 78/100\n",
            "23/23 - 0s - loss: 13.7346 - val_loss: 102.2385\n",
            "Epoch 79/100\n",
            "23/23 - 0s - loss: 13.3726 - val_loss: 111.4213\n",
            "Epoch 80/100\n",
            "23/23 - 0s - loss: 13.3669 - val_loss: 99.2604\n",
            "Epoch 81/100\n",
            "23/23 - 0s - loss: 13.3021 - val_loss: 100.2064\n",
            "Epoch 82/100\n",
            "23/23 - 0s - loss: 13.5376 - val_loss: 105.6957\n",
            "Epoch 83/100\n",
            "23/23 - 0s - loss: 13.5340 - val_loss: 112.5290\n",
            "Epoch 84/100\n",
            "23/23 - 0s - loss: 13.7215 - val_loss: 104.8863\n",
            "Epoch 85/100\n",
            "23/23 - 0s - loss: 13.0501 - val_loss: 96.1600\n",
            "Epoch 86/100\n",
            "23/23 - 0s - loss: 13.3682 - val_loss: 105.2716\n",
            "Epoch 87/100\n",
            "23/23 - 0s - loss: 13.0921 - val_loss: 107.5444\n",
            "Epoch 88/100\n",
            "23/23 - 0s - loss: 13.6332 - val_loss: 104.3077\n",
            "Epoch 89/100\n",
            "23/23 - 0s - loss: 13.2197 - val_loss: 102.3261\n",
            "Epoch 90/100\n",
            "23/23 - 0s - loss: 13.3651 - val_loss: 106.0234\n",
            "Epoch 91/100\n",
            "23/23 - 0s - loss: 13.1311 - val_loss: 105.2761\n",
            "Epoch 92/100\n",
            "23/23 - 0s - loss: 13.2952 - val_loss: 114.1159\n",
            "Epoch 93/100\n",
            "23/23 - 0s - loss: 13.8320 - val_loss: 101.8911\n",
            "Epoch 94/100\n",
            "23/23 - 0s - loss: 12.9144 - val_loss: 95.0624\n",
            "Epoch 95/100\n",
            "23/23 - 0s - loss: 12.9908 - val_loss: 100.4465\n",
            "Epoch 96/100\n",
            "23/23 - 0s - loss: 13.1125 - val_loss: 100.9772\n",
            "Epoch 97/100\n",
            "23/23 - 0s - loss: 12.7766 - val_loss: 101.8151\n",
            "Epoch 98/100\n",
            "23/23 - 0s - loss: 12.7020 - val_loss: 97.6959\n",
            "Epoch 99/100\n",
            "23/23 - 0s - loss: 12.8936 - val_loss: 103.4992\n",
            "Epoch 100/100\n",
            "23/23 - 0s - loss: 12.6581 - val_loss: 98.2987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fa6b74400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COkyNtxPTHq5"
      },
      "source": [
        "#prediction\r\n",
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "UzwluhU990gi"
      },
      "source": [
        "<strong>You can refer to this [link](https://keras.io/models/sequential?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ) to learn about other functions that you can use for prediction or evaluation.</strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "kBVFrIPh90gi"
      },
      "source": [
        "Feel free to vary the following and note what impact each change has on the model's performance:\n",
        "\n",
        "1.  Increase or decreate number of neurons in hidden layers\n",
        "2.  Add more hidden layers\n",
        "3.  Increase number of epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aD12D4yP8me"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H55nPJQAP1PW"
      },
      "source": [
        "def model1():\r\n",
        "  model =Sequential()\r\n",
        "  model.add(Dense(100,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux5PN1fAR6gV",
        "outputId": "f249c411-eff1-4cbe-b6bb-b13b44ba09f7"
      },
      "source": [
        "#fit the model\r\n",
        "model1.fit(x_norm,y,validation_split=0.3,epochs=150,verbose=1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.3153 - val_loss: 130.2595\n",
            "Epoch 2/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 6.1643 - val_loss: 140.7603\n",
            "Epoch 3/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.6716 - val_loss: 130.8577\n",
            "Epoch 4/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.7173 - val_loss: 107.9076\n",
            "Epoch 5/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.8055 - val_loss: 141.0662\n",
            "Epoch 6/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.3645 - val_loss: 118.2120\n",
            "Epoch 7/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7443 - val_loss: 133.5740\n",
            "Epoch 8/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2848 - val_loss: 124.8207\n",
            "Epoch 9/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3678 - val_loss: 133.8175\n",
            "Epoch 10/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1305 - val_loss: 126.8651\n",
            "Epoch 11/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3748 - val_loss: 120.5652\n",
            "Epoch 12/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8116 - val_loss: 135.8880\n",
            "Epoch 13/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6967 - val_loss: 146.5878\n",
            "Epoch 14/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.0459 - val_loss: 116.4828\n",
            "Epoch 15/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.0329 - val_loss: 131.2587\n",
            "Epoch 16/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4325 - val_loss: 130.2632\n",
            "Epoch 17/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4131 - val_loss: 128.2400\n",
            "Epoch 18/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.6848 - val_loss: 130.7573\n",
            "Epoch 19/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.0672 - val_loss: 135.5246\n",
            "Epoch 20/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.2864 - val_loss: 129.5764\n",
            "Epoch 21/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3669 - val_loss: 124.2697\n",
            "Epoch 22/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6065 - val_loss: 148.9418\n",
            "Epoch 23/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2939 - val_loss: 128.2308\n",
            "Epoch 24/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1936 - val_loss: 122.0079\n",
            "Epoch 25/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.2415 - val_loss: 123.6696\n",
            "Epoch 26/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8225 - val_loss: 116.1043\n",
            "Epoch 27/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.9341 - val_loss: 148.0475\n",
            "Epoch 28/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6545 - val_loss: 103.5297\n",
            "Epoch 29/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.8916 - val_loss: 134.5279\n",
            "Epoch 30/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.5276 - val_loss: 132.8243\n",
            "Epoch 31/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7352 - val_loss: 126.6457\n",
            "Epoch 32/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8960 - val_loss: 118.1688\n",
            "Epoch 33/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3125 - val_loss: 126.2022\n",
            "Epoch 34/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2471 - val_loss: 126.3972\n",
            "Epoch 35/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.2976 - val_loss: 136.5154\n",
            "Epoch 36/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8764 - val_loss: 114.6979\n",
            "Epoch 37/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.4574 - val_loss: 132.2731\n",
            "Epoch 38/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.7071 - val_loss: 119.0576\n",
            "Epoch 39/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4239 - val_loss: 127.6899\n",
            "Epoch 40/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3566 - val_loss: 132.0519\n",
            "Epoch 41/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1197 - val_loss: 128.3598\n",
            "Epoch 42/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2663 - val_loss: 125.3497\n",
            "Epoch 43/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1038 - val_loss: 122.1437\n",
            "Epoch 44/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7358 - val_loss: 127.4946\n",
            "Epoch 45/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1528 - val_loss: 125.6672\n",
            "Epoch 46/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 6.9351 - val_loss: 117.9809\n",
            "Epoch 47/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8752 - val_loss: 125.0417\n",
            "Epoch 48/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7404 - val_loss: 120.3708\n",
            "Epoch 49/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0120 - val_loss: 127.0598\n",
            "Epoch 50/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7205 - val_loss: 121.6878\n",
            "Epoch 51/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.9282 - val_loss: 109.1075\n",
            "Epoch 52/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.3986 - val_loss: 135.4813\n",
            "Epoch 53/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 8.2694 - val_loss: 128.0431\n",
            "Epoch 54/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.6811 - val_loss: 120.8818\n",
            "Epoch 55/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.1771 - val_loss: 119.6303\n",
            "Epoch 56/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8571 - val_loss: 138.8430\n",
            "Epoch 57/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4741 - val_loss: 118.6381\n",
            "Epoch 58/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8848 - val_loss: 129.9866\n",
            "Epoch 59/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.7332 - val_loss: 127.0029\n",
            "Epoch 60/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.6924 - val_loss: 126.5820\n",
            "Epoch 61/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1239 - val_loss: 137.1362\n",
            "Epoch 62/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9895 - val_loss: 132.0979\n",
            "Epoch 63/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2343 - val_loss: 114.0145\n",
            "Epoch 64/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.0641 - val_loss: 135.2472\n",
            "Epoch 65/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.9327 - val_loss: 127.7070\n",
            "Epoch 66/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6531 - val_loss: 122.4300\n",
            "Epoch 67/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.0405 - val_loss: 126.1846\n",
            "Epoch 68/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7347 - val_loss: 130.3805\n",
            "Epoch 69/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4071 - val_loss: 125.6540\n",
            "Epoch 70/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.2523 - val_loss: 125.2134\n",
            "Epoch 71/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.7304 - val_loss: 124.5673\n",
            "Epoch 72/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1363 - val_loss: 125.5648\n",
            "Epoch 73/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5470 - val_loss: 118.5993\n",
            "Epoch 74/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1302 - val_loss: 128.8624\n",
            "Epoch 75/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.0861 - val_loss: 129.4858\n",
            "Epoch 76/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.5707 - val_loss: 141.7597\n",
            "Epoch 77/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6942 - val_loss: 123.5765\n",
            "Epoch 78/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3426 - val_loss: 113.0753\n",
            "Epoch 79/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2985 - val_loss: 132.0437\n",
            "Epoch 80/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.4330 - val_loss: 136.9445\n",
            "Epoch 81/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3370 - val_loss: 129.6009\n",
            "Epoch 82/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1273 - val_loss: 128.8665\n",
            "Epoch 83/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.8530 - val_loss: 128.7555\n",
            "Epoch 84/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.3466 - val_loss: 106.7912\n",
            "Epoch 85/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 9.2230 - val_loss: 137.7563\n",
            "Epoch 86/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.4503 - val_loss: 120.7586\n",
            "Epoch 87/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.7460 - val_loss: 134.6806\n",
            "Epoch 88/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3140 - val_loss: 125.5832\n",
            "Epoch 89/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.0659 - val_loss: 124.4767\n",
            "Epoch 90/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.3433 - val_loss: 144.5072\n",
            "Epoch 91/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5291 - val_loss: 127.4905\n",
            "Epoch 92/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5818 - val_loss: 129.4477\n",
            "Epoch 93/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9407 - val_loss: 134.6449\n",
            "Epoch 94/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8364 - val_loss: 123.1944\n",
            "Epoch 95/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5375 - val_loss: 126.0829\n",
            "Epoch 96/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9970 - val_loss: 144.9804\n",
            "Epoch 97/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1120 - val_loss: 138.2728\n",
            "Epoch 98/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9818 - val_loss: 132.2569\n",
            "Epoch 99/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.7187 - val_loss: 119.4980\n",
            "Epoch 100/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 4.9097 - val_loss: 134.0714\n",
            "Epoch 101/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9337 - val_loss: 142.6924\n",
            "Epoch 102/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9365 - val_loss: 122.7504\n",
            "Epoch 103/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1373 - val_loss: 139.8323\n",
            "Epoch 104/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0961 - val_loss: 141.1393\n",
            "Epoch 105/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.3547 - val_loss: 119.3702\n",
            "Epoch 106/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.3429 - val_loss: 127.0359\n",
            "Epoch 107/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.9548 - val_loss: 127.8182\n",
            "Epoch 108/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5265 - val_loss: 125.9716\n",
            "Epoch 109/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.8187 - val_loss: 124.9657\n",
            "Epoch 110/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9550 - val_loss: 125.1862\n",
            "Epoch 111/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.1599 - val_loss: 122.1651\n",
            "Epoch 112/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5629 - val_loss: 139.7338\n",
            "Epoch 113/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2599 - val_loss: 143.0835\n",
            "Epoch 114/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.6132 - val_loss: 108.6727\n",
            "Epoch 115/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 6.9558 - val_loss: 115.3756\n",
            "Epoch 116/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 7.7965 - val_loss: 155.3394\n",
            "Epoch 117/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.7568 - val_loss: 124.6431\n",
            "Epoch 118/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2746 - val_loss: 145.8196\n",
            "Epoch 119/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.1883 - val_loss: 112.2319\n",
            "Epoch 120/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4714 - val_loss: 126.6605\n",
            "Epoch 121/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.6192 - val_loss: 124.4411\n",
            "Epoch 122/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6612 - val_loss: 135.3994\n",
            "Epoch 123/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4822 - val_loss: 135.2192\n",
            "Epoch 124/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.9849 - val_loss: 141.8197\n",
            "Epoch 125/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.0855 - val_loss: 126.6094\n",
            "Epoch 126/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.0797 - val_loss: 131.3882\n",
            "Epoch 127/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.5566 - val_loss: 132.8164\n",
            "Epoch 128/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.9169 - val_loss: 127.4285\n",
            "Epoch 129/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.5106 - val_loss: 144.7046\n",
            "Epoch 130/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8311 - val_loss: 144.4291\n",
            "Epoch 131/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.0886 - val_loss: 141.4570\n",
            "Epoch 132/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4534 - val_loss: 134.3288\n",
            "Epoch 133/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.2677 - val_loss: 136.2263\n",
            "Epoch 134/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.2303 - val_loss: 137.8063\n",
            "Epoch 135/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 5.4232 - val_loss: 122.6769\n",
            "Epoch 136/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.8651 - val_loss: 141.4015\n",
            "Epoch 137/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.0822 - val_loss: 125.7750\n",
            "Epoch 138/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4646 - val_loss: 133.9116\n",
            "Epoch 139/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.8038 - val_loss: 140.2432\n",
            "Epoch 140/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.4436 - val_loss: 145.0199\n",
            "Epoch 141/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 5.9513 - val_loss: 127.7402\n",
            "Epoch 142/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 5.2531 - val_loss: 136.9873\n",
            "Epoch 143/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4231 - val_loss: 149.8390\n",
            "Epoch 144/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6821 - val_loss: 141.8847\n",
            "Epoch 145/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.6146 - val_loss: 124.2590\n",
            "Epoch 146/150\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 4.8715 - val_loss: 137.1976\n",
            "Epoch 147/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.4622 - val_loss: 113.2527\n",
            "Epoch 148/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.2827 - val_loss: 140.0885\n",
            "Epoch 149/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 4.4458 - val_loss: 121.3266\n",
            "Epoch 150/150\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 6.5259 - val_loss: 144.8931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fa59ae6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45lK4MLETPqe"
      },
      "source": [
        "#prediction\r\n",
        "model1.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFbjyvCJT3Sv"
      },
      "source": [
        "###LAB: Classification with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "q8pGHHxBUqGr"
      },
      "source": [
        "#### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "CD4Uew4dUqGs"
      },
      "source": [
        "In this lab, we will learn how to use the Keras library to build models for classificaiton problems. We will use the popular MNIST dataset, a dataset of images, for a change. \n",
        "\n",
        "The <strong>MNIST database</strong>, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n",
        "\n",
        "The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau.\n",
        "\n",
        "Also, this way, will get to compare how conventional neural networks compare to convolutional neural networks, that we will build in the next module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze_bFGWFUqGs"
      },
      "source": [
        "<h2>Classification Models with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. Use of MNIST database for training various image processing systems</h5>\n",
        "<h5> 2. Build a Neural Network </h5>\n",
        "<h5> 3. Train and Test the Network. </h5>\n",
        "\n",
        "<p>This link will be used by your peers to assess your project. In your web app, your peers will be able to upload an image, which will then be classified using your custom classifier you connected to the web app. Your project will be graded by how accurately your app can classify <b>Fire</b>, <b>Smoke</b> and <b>Neutral (No Fire or Smoke)</b>.<p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "7MyCmvcMUqGt"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "\n",
        "1.  <a href=\"#item312\">Import Keras and Packages</a>      \n",
        "2.  <a href=\"#item322\">Build a Neural Network</a>     \n",
        "3.  <a href=\"#item332\">Train and Test the Network</a>     \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2kih0lcUqGt"
      },
      "source": [
        "<a id='item312'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "KJ7rA2ZjUqGt"
      },
      "source": [
        "#### Import Keras and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ugzLCSUqGu"
      },
      "source": [
        "Let's start by importing Keras and some of its modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kmvg7ncT6dr"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "N2N54G7VUqGv"
      },
      "source": [
        "Since we are dealing we images, let's also import the Matplotlib scripting layer in order to view the images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "YZmx7J95UqGv"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "PJAAvcGZUqGv"
      },
      "source": [
        "The Keras library conveniently includes the MNIST dataset as part of its API. You can check other datasets within the Keras library [here](https://keras.io/datasets?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork-20718188&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ). \n",
        "\n",
        "So, let's load the MNIST dataset from the Keras library. The dataset is readily divided into a training set and a test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7moM_JxVmI3",
        "outputId": "f82deadd-22b2-4ce8-848c-7c041abcee02"
      },
      "source": [
        "#import the data\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "#read the data\r\n",
        "(xtr,ytr),(xte,yte) = mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "axVezyIMUqGw"
      },
      "source": [
        "Let's confirm the number of images in each set. According to the dataset's documentation, we should have 60000 images in X_train and 10000 images in the X_test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXAzSn9eaRL6",
        "outputId": "883d39e2-5888-493f-e21f-1e3567749d4a"
      },
      "source": [
        "xtr[0:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hREz0nVHWcgZ",
        "outputId": "23f65859-f13d-4347-cdca-e23548ae7f21"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOfPA5cUWdwh",
        "outputId": "4c61c96e-171e-4fbe-eef2-a84670e7dffb"
      },
      "source": [
        "xte.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "g2qIYypUUqGx"
      },
      "source": [
        "The first number in the output tuple is the number of images, and the other two numbers are the size of the images in datset. So, each image is 28 pixels by 28 pixels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WuDHiPtcUqGx"
      },
      "source": [
        "Let's visualize the first image in the training set using Matplotlib's scripting layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "eht9Ejs3WfFf",
        "outputId": "2be6db25-ecf7-43ef-c77c-f0041e413cde"
      },
      "source": [
        "plt.imshow(xtr[0])  # as u can see its 28*28 size"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8ce4d56438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "pdhVevmSUqGx"
      },
      "source": [
        "With conventional neural networks, we cannot feed in the image as input as is. So we need to flatten the images into one-dimensional vectors, each of size 1 x (28 x 28) = 1 x 784.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNYpyh7jW01I"
      },
      "source": [
        "#flatten images into 1d vector\r\n",
        "num_pixels  = xtr.shape[1]*xtr.shape[2]  # find size of one-dimensional vector\r\n",
        "#since we have same num of pixels in xtr,xte we can use the same \r\n",
        "xtr = xtr.reshape(xtr.shape[0],num_pixels).astype(\"float32\") # flatten training images\r\n",
        "xte = xte.reshape(xte.shape[0],num_pixels).astype(\"float32\") # flatten training images"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RTVNPz5bxkt",
        "outputId": "5b43ae03-8f26-44c9-8047-adb709b09f8d"
      },
      "source": [
        "xtr[0:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yly9pLkWbz0d",
        "outputId": "1d7d8f77-c4d3-4bb2-d199-2f38fd5b3481"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Ts_LoA1nUqGy"
      },
      "source": [
        "Since pixel values can range from 0 to 255, let's normalize the vectors to be between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS0p2hhlb2GN"
      },
      "source": [
        "#normalise inputs from 0-255 to 0-1\r\n",
        "xtr =xtr/255\r\n",
        "xte = xte/255"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg-x1zpfcGCZ",
        "outputId": "370ebb22-e7ea-4bcc-8fcd-e5241e63e1fe"
      },
      "source": [
        "ytr[0:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Chu8TNrcUqGy"
      },
      "source": [
        "Finally, before we start building our model, remember that for classification we need to divide our target variable into binary values. We use the to_categorical function from the Keras Utilities package.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmJlHtfIcI1a",
        "outputId": "85f93ae8-5019-4a7c-bd5d-fc044c79aad0"
      },
      "source": [
        "#one hot encode outputs, since we have 0-9 digits it will create 10columns\r\n",
        "ytr = to_categorical(ytr)\r\n",
        "yte = to_categorical(yte)\r\n",
        "\r\n",
        "num_classes = yte.shape[1]\r\n",
        "num_classes"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "nKKr7AcnUqGz"
      },
      "source": [
        "#### Build a Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_UunfdBc3EK"
      },
      "source": [
        "#define classification model\r\n",
        "def classification_model():\r\n",
        "  #create model\r\n",
        "  model =Sequential()\r\n",
        "  model.add(Dense(num_pixels,activation=\"relu\",input_shape=(num_pixels,)))\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(num_classes,activation=\"softmax\"))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\r\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "8nTpi3pYUqG0"
      },
      "source": [
        "#### Train and Test the Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCYouc77PIav",
        "outputId": "3a6c9f66-8d51-4d14-9e81-f6096c0df5e3"
      },
      "source": [
        "model = classification_model()\r\n",
        "\r\n",
        "#fit\r\n",
        "model.fit(xtr,ytr,validation_data=(xte,yte),epochs=10,verbose=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 - 11s - loss: 0.1861 - accuracy: 0.9445 - val_loss: 0.0856 - val_accuracy: 0.9742\n",
            "Epoch 2/10\n",
            "1875/1875 - 10s - loss: 0.0782 - accuracy: 0.9754 - val_loss: 0.0767 - val_accuracy: 0.9764\n",
            "Epoch 3/10\n",
            "1875/1875 - 10s - loss: 0.0526 - accuracy: 0.9838 - val_loss: 0.0774 - val_accuracy: 0.9763\n",
            "Epoch 4/10\n",
            "1875/1875 - 10s - loss: 0.0400 - accuracy: 0.9868 - val_loss: 0.0639 - val_accuracy: 0.9809\n",
            "Epoch 5/10\n",
            "1875/1875 - 10s - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.0940 - val_accuracy: 0.9745\n",
            "Epoch 6/10\n",
            "1875/1875 - 10s - loss: 0.0257 - accuracy: 0.9912 - val_loss: 0.0890 - val_accuracy: 0.9774\n",
            "Epoch 7/10\n",
            "1875/1875 - 10s - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.0944 - val_accuracy: 0.9798\n",
            "Epoch 8/10\n",
            "1875/1875 - 10s - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1185 - val_accuracy: 0.9732\n",
            "Epoch 9/10\n",
            "1875/1875 - 10s - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.1041 - val_accuracy: 0.9791\n",
            "Epoch 10/10\n",
            "1875/1875 - 10s - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.0992 - val_accuracy: 0.9808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8cd8a31400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzIDJeDfQQU_",
        "outputId": "3e4de07a-bbdf-41ca-9a40-4c006bc7ae0f"
      },
      "source": [
        "#evaluate the model\r\n",
        "scores = model.evaluate(xte,yte,verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0992 - accuracy: 0.9808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "bQ465tK0UqG0"
      },
      "source": [
        "Let's print the accuracy and the corresponding error.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR06l6jbQfoX",
        "outputId": "129e2a2c-27e6-42d6-9289-45886125cf34"
      },
      "source": [
        "print(\"Accuracy: {}% \\n Error: {}\".format(scores[1],1-scores[1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9807999730110168% \n",
            " Error: 0.019200026988983154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "2ScmlXgyUqG1"
      },
      "source": [
        "Just running 10 epochs could actually take over 20 minutes. But enjoy the results as they are getting generated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "0gCnsG2UUqG2"
      },
      "source": [
        "Sometimes, you cannot afford to retrain your model everytime you want to use it, especially if you are limited on computational resources and training your model can take a long time. Therefore, with the Keras library, you can save your model after training. To do that, we use the save method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohRutWC3RXwb"
      },
      "source": [
        "model.save(\"classification_model.h5\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "iEutyRCiUqG3"
      },
      "source": [
        "When you are ready to use your model again, you use the load_model function from <strong>keras.models</strong>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcJNpqkRurq"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "\r\n",
        "pretrained_model = load_model(\"classification_model.h5\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuGXaoxNU3TE"
      },
      "source": [
        "##Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTqWRcvbU6B4"
      },
      "source": [
        "###Shallow Versus Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x83raCU-V5cH"
      },
      "source": [
        ">So far, we have mostly been dealing with not very deep, or shallow, neural networks. And the main reason is that they really do serve as the building block of deep neural networks and are easier to understand due to their simplicity. There isn't really a consensus on the definition of a shallow neural network but a neural network with one hidden layer is considered a shallow neural network whereas a network with many hidden layers and a large number of neurons in each layer is considered a deep neural network. Also, unlike a shallow neural network which takes only input as vectors, deep neural networks are able to take raw data such as images and text and automatically extract the necessary features to learn the data better. We will start learning about deep learning algorithms in the next videos. But if neural networks have been around for quite some time, how come only recently did they turn deep and start taking off resulting in a plethora of cool and exciting applications? \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(902).png?raw=true)\r\n",
        "\r\n",
        ">The sudden boom in the deep learning field can be attributed to three main factors. Number one, advancement in the field itself. We talked about this briefly in the activation functions video, where we mentioned that the ReLU activation function helped overcome the challenge of the vanishing gradient problem, and therefore, opened the door to the creation of very deep networks. Therefore, advancement in the field itself is one factor that helped deep learning take off. Another main reason is the availability of data. Deep neural networks work best when trained with large and large amounts of data, since neural networks learn the training data so well, then large amounts of data have to be used in order to avoid overfitting of the training data. Now that large amounts of data are readily available and easy to acquire like never before, deep learning algorithms are being tried and tested like never before. Especially that the other conventional machine learning algorithms, while they do improve with more data, but up to a certain point. After that, no significant improvement would be observed with more data. That is definitely not the case with deep learning. The more data you feed it the better it performs. Finally, and this goes hand-in-hand with point number 2, is computational power. With NVIDIA's super powerful GPUs, we are now able to train very deep neural networks on tremendous amount of data in a matter of hours as opposed to days or weeks, which is how long it used to take to train very deep neural networks. Therefore, users are able to experiment with different deep neural networks and test different prototypes in much shorter periods of time. These three factors are the main reasons behind the boom of deep learning. In the next video, we will start learning about deep learning algorithms. We will start with supervised deep learning algorithms, and in the next video, we will learn about convolutional neural networks.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(903).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkjESU0ehgY"
      },
      "source": [
        "### Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jxl4xLZZhsC"
      },
      "source": [
        "####Convolutional Neural Networks(ConvNets or CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oREr1KkNZjOz"
      },
      "source": [
        "> Convolutional neural networks, which are supervised deep learning models have revolutionized the field of computer vision, especially object detection in images.Convolutional neural networks are very similar to the neural networks that we have seen so far in this course. They are made up of neurons, which need to have the weights and biases optimized. Each neuron combines the inputs that it receives by computing the dot product between each input and the corresponding weight before it fits the resulting total input into an activation function, ReLU most likely. So then, what is different with these networks and why are they called convolutional neural networks? Well convolutional neural networks, or CNNs for short, make the explicit assumption that the inputs are images, which allows us to incorporate certain properties into their architecture. These properties make the forward propagation step much more efficient and vastly reduces the amount of parameters in the network. Therefore, CNNs are best for solving problems related to image recognition, object detection, and other computer vision applications.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(904).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        ">Here is a typical architecture of a convolutional neural network. As you can see, the network consists of a series of convolutional, ReLU, and pooling layers as well as a number of fully connected layers which are necessary before the output is generated. Now, let's study what happens in each layer.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(905).png?raw=true)\r\n",
        "\r\n",
        ">So far, we have dealt only with conventional neural networks that take an ( n x 1) vector as their input. The input to a convolutional neural network, on the other hand, is mostly an (n x m x 1) for grayscale images or an (n x m x 3) for colored images, where the number 3 represents the red, green, and blue components of each pixel in the image.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(906).png?raw=true)\r\n",
        "\r\n",
        ">Convolutional layers are the major building blocks used in convolutional neural networks.A convolution is the simple application of a filter(kernel) to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image. So in the convolutional layer, we basically define filters(kernels) and we compute the convolution between the defined filters and each of the three images. If we take the red image for example, let's assume these are the pixel values. Now for a (2 x 2) filter with these values, let's create an empty matrix to save the results of the convolution process. We start by sliding(stride) the filter over the image and computing the dot product between the filter and the overlapping pixel values and storing the result in the empty matrix. We repeat this step moving our filter one cell, or one stride is the proper terminology, at a time, and we repeat this until we cover the entire image and fill the empty matrix. Here, I just showed one filter and only one of the three images. The same thing would be applied to the green and blue images and you can apply more than one filter. The more filters we use, the more we are able to preserve the spatial dimensions better. But one question you must be asking yourself at this point is, why would we need to use convolution? Why not flatten the input image into an (n x m) x 1 vector and use that as our input? Well, if we do that, we will end up with a massive number of parameters that will need to be optimized, and it will be super computationally expensive. Also, decreasing the number of parameters would definitely help in preventing the model from overfitting the training data. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(907).png?raw=true)\r\n",
        "\r\n",
        ">It is worth mentioning that a convolutional layer also consists of ReLU's which filter the output of the convolutional step passing only positive values and turning any negative values to 0. \r\n",
        "\r\n",
        ">The next layer in our convolutional neural network is the pooling layer. The pooling layer's main objective is to reduce the spatial dimensions of the data propagating through the network. There are two types of pooling that are widely used in convolutional neural networks. Max- pooling and average pooling. In max-pooling which is the most common of the two, for each section of the image we scan we keep the highest value, like so. Here our filter is moving two strides at a time. Similarly, with average pooling, we compute the average of each area we scan. In addition to reducing the dimension of the data, pooling, or max pooling in particular, provides spatial variance which enables the neural network to recognize objects in an image even if the object does not exactly resemble the original object. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(908).png?raw=true)\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(909).png?raw=true)\r\n",
        "\r\n",
        ">Finally, in the fully connected layer, we flatten the output of the last convolutional layer and connect every node of the current layer with every other node of the next layer. This layer basically takes as input the output from the preceding layer, whether it is a convolutional layer, ReLU, or pooling layer, and outputs an n-dimensional vector, where n is the number of classes pertaining to the problem at hand. For example, if you are building a network to classify images of digits, the dimension n would be 10, since there are 10 digits. You will be covering convolutional neural networks in much more details in the other courses in this specialization, but this information is more than enough to give you a general understanding of convolutional neural networks. Now let's see how we can use the Keras library to build a convolutional neural network. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(910).png?raw=true)\r\n",
        "\r\n",
        ">Here I will show you how you can use the Keras library to build a convolutional neural network. Training and testing of a convolutional neural network are the same as what we have seen so far. So to begin with, we use the sequential constructor to create our model. Then, we define our input to be the size of the input images. Assuming the input images are 128 by 128 color images, we define the input shape to be a tuple of (128, 128, 3). Next, we start adding layers to the network. We start with a convolutional layer, with 16 filters, each filter being of size 2x2 and slides through the image with a stride of magnitude 1 in the horizontal direction, and of magnitude 1 in the vertical direction. And the layer uses the ReLU activation function. Then, we add a pooling layer and we're using max-pooling here with a filter or pooling size of 2 and the filter slides through the image with a stride of magnitude 2. Next, we add another set of convolutional and pooling layers. The only difference here is we are using more filters in the convolutional layer, actually twice as many filters as the first convolutional layer. Finally, we flatten the output from these layers so that the data can proceed to the fully connected layers. We add another dense layer with 100 nodes and an output layer that has nodes equal to the number of classes in the problem at hand. And we use the softmax activation function in order to convert the outputs into probabilities.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(911).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwZ58hhzez48"
      },
      "source": [
        "####Recurrent Neural Networks(RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNoOK8fvgf87"
      },
      "source": [
        ">In this video, we will learn about another supervised deep learning model, which is the recurrent neural network. So far, we have seen neural networks and deep learning models that see datapoints as independent instances. However, let's say you want to build a model that can analyze scenes in a movie. Well, you cannot assume that scenes in a movie are independent, and therefore, traditional deep learning models are not suitable for this application. Recurrent neural networks overcome this issue. Recurrent neural networks or (RNNs) for short, are networks with loops that don't just take a new input at a time, but also take in as input the output from the previous dat point that was fed into the network. Accordingly, this is how the architecture of a recurrent neural network would look like. Essentially, we can start with a normal neural network. At time t = 0, the network takes in input x0 and outputs a0. Then, at time t = 1, in addition to the input x1, the network also takes a0 as input, weighted with weight w0,1, and so on and so forth. As a result, recurrent neural networks are very good at modelling patterns and sequences of data, such as texts, genomes, handwriting, and stock markets. These algorithms take time and sequence into account, which means that they have a temporal dimension.\r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(912).png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        "> A very popular type of recurrent neural network is the long short-term memory model or the (LSTM) model for short. It has been successfully used for many applications including image generation, where a model trained on many images is used to generate new novel images. Another application is handwriting generation, which I described in the welcome video of this course. Also LSTM models have been successfully used to build algorithms that can automatically describe images as well as streams of videos. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(913).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEKYZsbwhUG-"
      },
      "source": [
        "###Unsupervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYVc6KXvhZHU"
      },
      "source": [
        "####Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8a8mm43hz_f"
      },
      "source": [
        "> In this video, we will switch to an unsupervised deep learning model which is the autoencoder. So what are autoencoders? Autoencoding is a data compression algorithm where the compression and the decompression functions are learned automatically from data. instead of being engineered by a human. Such autoencoders are built using neural networks. Autoencoders are data specific, which means that they will only be able to compress data similar to what they have been trained on. Therefore, an autoencoder trained on pictures of cars would do a rather poor job of compressing pictures of buildings, because the features it would learn would be vehicle or car specific. Some interesting applications of autoencoders are data denoising and dimensionality reduction for data visualization. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(914).png?raw=true)\r\n",
        "\r\n",
        ">Here is the architecture of an autoencoder. It takes an image, for example, as an input and uses an encoder to find the optimal compressed representation of the input image. Then, using a decoder the original image is restored. So an autoencoder is an unsupervised neural network model. It uses backpropagation by setting the target variable to be the same as the input. In other words, it tries to learn an approximation of an identity function. Because of non-linear activation functions in neural networks, autoencoders can learn data projections that are more interesting than a principal component analysis PCA or other basic techniques, which can handle only linear transformations. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(915).png?raw=true)\r\n",
        "\r\n",
        "> A very popular type of autoencoders is the Restricted Boltzmann Machines or (RBMs) for short. RBMs have been successfully used for various applications, including fixing imbalanced datasets. Because RBMs learn the input in order to be able to regenerate it, then they can learn the distribution of the minority class in an imbalance dataset ,and then generate more data points of that class, transforming the imbalance dataset into a balanced data set. Similarly, RBMs can also be used to estimate missing values in different features of a data set. Another popular application of Restricted Boltzmann Machines is automatic feature extraction of especially unstructured data. \r\n",
        "![](https://github.com/harsha8745/IBM-WATSON/blob/Images/Screenshot%20(916).png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcpPN9c7kwVv"
      },
      "source": [
        "###LAB: Convolutional Neural Networks with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwlbvEdqkdEl"
      },
      "source": [
        "In this lab, we will learn how to use the Keras library to build convolutional neural networks. We will also use the popular MNIST dataset and we will compare our results to using a conventional neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3fQrafzkdEm"
      },
      "source": [
        "<h2>Convolutional Neural Networks with Keras</h2>\n",
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. How to use the Keras library to build convolutional neural networks.</h5>\n",
        "<h5> 2. Convolutional Neural Network with One Convolutional and Pooling Layers.</h5>\n",
        "<h5> 3. Convolutional Neural Network with Two Convolutional and Pooling Layers.</h5>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZkQAnRPkdEm"
      },
      "source": [
        "#### Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "      \n",
        "1. <a href=\"#item41\">Import Keras and Packages</a>   \n",
        "2. <a href=\"#item42\">Convolutional Neural Network with One Convolutional and Pooling Layers</a>  \n",
        "3. <a href=\"#item43\">Convolutional Neural Network with Two Convolutional and Pooling Layers</a>  \n",
        "\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLrUPKAkdEn"
      },
      "source": [
        "<a id='item41'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1wbrMETkdEn"
      },
      "source": [
        "#### Import Keras and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD-WjbfjkdEn"
      },
      "source": [
        "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5m9KJxck2oo"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nonYzI-IkdEo"
      },
      "source": [
        "When working with convolutional neural networks in particular, we will need additional packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c6j5JIDlUHh"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D        # to add convolutional layers\r\n",
        "from keras.layers.convolutional import MaxPooling2D  # to add pooling layers\r\n",
        "from keras.layers import Flatten                   # to flatten data for fully connected layers"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN6qr2bukdEp"
      },
      "source": [
        "#### Convolutional Layer with One set of convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrV5SrA2merk"
      },
      "source": [
        "#import data\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "#load data\r\n",
        "(xtr,ytr),(xte,yte) = mnist.load_data()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi4iwBdWm00y",
        "outputId": "12cef3e6-ea17-41d1-dbbf-bec7e91457c8"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjmGIASm3Qh",
        "outputId": "47258860-3608-42bb-c533-ca329a0fc00c"
      },
      "source": [
        "ytr.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlAcpYiNm6KE",
        "outputId": "f3937083-4192-4704-bfc8-3a028fd153e8"
      },
      "source": [
        "xte.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJU4ythCm8f9",
        "outputId": "609a63a0-2fce-4e85-9979-515bcb27b95b"
      },
      "source": [
        "yte.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDQ9DwmKm-qg"
      },
      "source": [
        "# reshape to be [samples][pixels][width][height]\r\n",
        "xtr = xtr.reshape(xtr.shape[0],28,28,1).astype(\"float32\")\r\n",
        "xte = xte.reshape(xte.shape[0],28,28,1).astype(\"float32\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrZ3r9vVnqSb",
        "outputId": "529c95ab-b168-47c1-b21a-99982e02e079"
      },
      "source": [
        "xtr.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXxYmILLnrem",
        "outputId": "c17b0def-3b8f-449e-da58-ff11747cce0d"
      },
      "source": [
        "xte.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNqxseEkdEq"
      },
      "source": [
        "Let's normalize the pixel values to be between 0 and 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMlf5CmjnyNV"
      },
      "source": [
        "xtr = xtr/255   # normalize training data\r\n",
        "xte = xte/255   # normalize test data"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NPT_yU1kdEr"
      },
      "source": [
        "Next, let's convert the target variable into binary categories\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNliT2Yyn-rX"
      },
      "source": [
        "ytr = to_categorical(ytr)\r\n",
        "yte = to_categorical(yte)\r\n",
        "\r\n",
        "num_classes = yte.shape[1]   # number of categories"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoLmsFNPkdEr"
      },
      "source": [
        "Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4rASNdyD9n"
      },
      "source": [
        "**defualt values for conv layer:**  \r\n",
        "filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None\r\n",
        "\r\n",
        "only filters and kernel_size parameters must be given, others are optional or has default values next to them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV2MIdjmoXS1"
      },
      "source": [
        "def convolution_model():\r\n",
        "  \r\n",
        "  #create model\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Conv2D(filters=16,kernel_size=(5,5),strides=(1,1),activation = \"relu\",input_shape=(28,28,1)))  #conolution layer\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))  #pooling layer\r\n",
        " \r\n",
        "  model.add(Flatten()) #flatten data into n*1 array\r\n",
        "  model.add(Dense(100,activation=\"relu\")) #fully connected layer\r\n",
        "  model.add(Dense(num_classes,activation=\"softmax\")) #output layer\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\r\n",
        "  return model"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yqxqYXgkdEs"
      },
      "source": [
        "Finally, let's call the function to create the model, and then let's train it and evaluate it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi17mddlr7zj"
      },
      "source": [
        ">The batch size defines the number of samples that will be propagated through the network.  \r\n",
        "For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdJZMLBqqc-G",
        "outputId": "a029af41-76c3-47e5-f8a9-18a7949aa0b7"
      },
      "source": [
        "model = convolution_model()\r\n",
        "\r\n",
        "#fit the model\r\n",
        "model.fit(xtr,ytr,validation_data=(xte,yte),epochs=10,batch_size=200,verbose=2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "Epoch 1/10\n",
            "300/300 - 17s - loss: 0.3038 - accuracy: 0.9153 - val_loss: 0.1026 - val_accuracy: 0.9695\n",
            "Epoch 2/10\n",
            "300/300 - 17s - loss: 0.0818 - accuracy: 0.9765 - val_loss: 0.0644 - val_accuracy: 0.9803\n",
            "Epoch 3/10\n",
            "300/300 - 17s - loss: 0.0554 - accuracy: 0.9838 - val_loss: 0.0480 - val_accuracy: 0.9856\n",
            "Epoch 4/10\n",
            "300/300 - 17s - loss: 0.0439 - accuracy: 0.9866 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
            "Epoch 5/10\n",
            "300/300 - 17s - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.0369 - val_accuracy: 0.9872\n",
            "Epoch 6/10\n",
            "300/300 - 17s - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0371 - val_accuracy: 0.9878\n",
            "Epoch 7/10\n",
            "300/300 - 17s - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.0388 - val_accuracy: 0.9869\n",
            "Epoch 8/10\n",
            "300/300 - 17s - loss: 0.0186 - accuracy: 0.9944 - val_loss: 0.0394 - val_accuracy: 0.9871\n",
            "Epoch 9/10\n",
            "300/300 - 17s - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.0380 - val_accuracy: 0.9876\n",
            "Epoch 10/10\n",
            "300/300 - 17s - loss: 0.0135 - accuracy: 0.9960 - val_loss: 0.0427 - val_accuracy: 0.9870\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8cd437aac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kT7sE6Rv7Xa",
        "outputId": "64e0b9ff-f566-49ce-bfb4-91ab1520649a"
      },
      "source": [
        "#to know the output names of the metrics used for a given loss function\r\n",
        "print(model.metrics_names)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4v6u41yswcJ",
        "outputId": "628264e5-85d0-4977-e773-63357e3fedf6"
      },
      "source": [
        "#evaluate the model\r\n",
        "scores = model.evaluate(xte,yte,verbose=0)\r\n",
        "print(\"accuracy = {} \\n error = {}\".format(scores[1],100-scores[1]*100))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.9869999885559082 \n",
            " error = 1.3000011444091797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOHvHPjHtZ38",
        "outputId": "8be51ea4-9279-4427-b1a8-c6acda960b4f"
      },
      "source": [
        "scores"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04270915687084198, 0.9869999885559082]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qVV95SGkdEu"
      },
      "source": [
        "#### Convolutional Layer with two sets of convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4dXVCslkdEv"
      },
      "source": [
        "Let's redefine our convolutional model so that it has two convolutional and pooling layers instead of just one layer of each.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJwrSmKHtbHI"
      },
      "source": [
        "def convolution_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Conv2D(16,(5,5),activation=\"relu\",input_shape=(28,28,1)))  #default stride=(1,1)\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\r\n",
        "\r\n",
        "  model.add(Conv2D(8,(2,2),activation=\"relu\"))\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\r\n",
        "\r\n",
        "  model.add(Flatten())\r\n",
        "  model.add(Dense(100,activation=\"relu\"))\r\n",
        "  model.add(Dense(num_classes,activation=\"softmax\"))\r\n",
        "\r\n",
        "  #compile\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\r\n",
        "  return model"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcH5Yjl1kdEx"
      },
      "source": [
        "Now, let's call the function to create our new convolutional neural network, and then let's train it and evaluate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3ggfc3Yz_ee",
        "outputId": "032cd832-fcee-4d64-d13e-2bd2fb4516c4"
      },
      "source": [
        "model = convolution_model()\r\n",
        "\r\n",
        "#fit\r\n",
        "model.fit(xtr,ytr,validation_data=(xte,yte),epochs=10,batch_size=200,verbose=2)\r\n",
        "\r\n",
        "#evaluate\r\n",
        "scores = model.evaluate(xte,yte,verbose=0)\r\n",
        "print(\"accuracy = {} \\n error = {}\".format(scores[1],100-scores[1]*100))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 19s - loss: 0.4334 - accuracy: 0.8750 - val_loss: 0.1060 - val_accuracy: 0.9674\n",
            "Epoch 2/10\n",
            "300/300 - 18s - loss: 0.1001 - accuracy: 0.9698 - val_loss: 0.0702 - val_accuracy: 0.9787\n",
            "Epoch 3/10\n",
            "300/300 - 18s - loss: 0.0752 - accuracy: 0.9772 - val_loss: 0.0592 - val_accuracy: 0.9817\n",
            "Epoch 4/10\n",
            "300/300 - 18s - loss: 0.0621 - accuracy: 0.9814 - val_loss: 0.0505 - val_accuracy: 0.9843\n",
            "Epoch 5/10\n",
            "300/300 - 18s - loss: 0.0533 - accuracy: 0.9837 - val_loss: 0.0457 - val_accuracy: 0.9859\n",
            "Epoch 6/10\n",
            "300/300 - 19s - loss: 0.0469 - accuracy: 0.9855 - val_loss: 0.0446 - val_accuracy: 0.9860\n",
            "Epoch 7/10\n",
            "300/300 - 19s - loss: 0.0431 - accuracy: 0.9867 - val_loss: 0.0438 - val_accuracy: 0.9859\n",
            "Epoch 8/10\n",
            "300/300 - 19s - loss: 0.0378 - accuracy: 0.9883 - val_loss: 0.0527 - val_accuracy: 0.9833\n",
            "Epoch 9/10\n",
            "300/300 - 18s - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.0362 - val_accuracy: 0.9881\n",
            "Epoch 10/10\n",
            "300/300 - 19s - loss: 0.0303 - accuracy: 0.9908 - val_loss: 0.0368 - val_accuracy: 0.9887\n",
            "accuracy = 0.9886999726295471 \n",
            " error = 1.130002737045288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLPMy5mlyHRo"
      },
      "source": [
        "##Final project: Building a Regression Model in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klg2-CZuXj2I"
      },
      "source": [
        "####Part A: Build a baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQciPOrmXFUH"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVzYfF4tX9Ds"
      },
      "source": [
        "!wget -q https://cocl.us/concrete_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "4R-4DYCxYA_i",
        "outputId": "c7bb27dd-5881-45ba-a84d-459e9fa3dd72"
      },
      "source": [
        "df = pd.read_csv(\"concrete_data\")\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbE-b9vYg5T"
      },
      "source": [
        "####Data wrangling and EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwvfIZa0Ygob",
        "outputId": "8dbdbb54-6f0f-47c1-b3dd-1cc26b10d087"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1030 entries, 0 to 1029\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Cement              1030 non-null   float64\n",
            " 1   Blast Furnace Slag  1030 non-null   float64\n",
            " 2   Fly Ash             1030 non-null   float64\n",
            " 3   Water               1030 non-null   float64\n",
            " 4   Superplasticizer    1030 non-null   float64\n",
            " 5   Coarse Aggregate    1030 non-null   float64\n",
            " 6   Fine Aggregate      1030 non-null   float64\n",
            " 7   Age                 1030 non-null   int64  \n",
            " 8   Strength            1030 non-null   float64\n",
            "dtypes: float64(8), int64(1)\n",
            "memory usage: 72.5 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PF2014_YS0J",
        "outputId": "12a0d3c3-ad15-4703-deb5-082aacd9857d"
      },
      "source": [
        "#checking for nulls\r\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cement                0\n",
              "Blast Furnace Slag    0\n",
              "Fly Ash               0\n",
              "Water                 0\n",
              "Superplasticizer      0\n",
              "Coarse Aggregate      0\n",
              "Fine Aggregate        0\n",
              "Age                   0\n",
              "Strength              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Zb7yQJKkYqcL",
        "outputId": "2d2a307e-46a3-43d1-9a43-000eb8502e2c"
      },
      "source": [
        "#descriptive statistics\r\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-En7NvM-acjX"
      },
      "source": [
        "#####Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "7sA6tf4OaaYi",
        "outputId": "6aebb7a4-3dda-4ffb-91ae-2b567136b382"
      },
      "source": [
        "x = df.loc[:,df.columns!=\"Strength\"]\r\n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Coarse Aggregate  Fine Aggregate  Age\n",
              "0   540.0                 0.0      0.0  ...            1040.0           676.0   28\n",
              "1   540.0                 0.0      0.0  ...            1055.0           676.0   28\n",
              "2   332.5               142.5      0.0  ...             932.0           594.0  270\n",
              "3   332.5               142.5      0.0  ...             932.0           594.0  365\n",
              "4   198.6               132.4      0.0  ...             978.4           825.5  360\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH3ZZ7D3bFLF",
        "outputId": "3d6e28e7-87cb-4b3f-bc77-c86f64286b52"
      },
      "source": [
        "#target variable\r\n",
        "y =df[\"Strength\"]\r\n",
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    79.99\n",
              "1    61.89\n",
              "2    40.27\n",
              "3    41.05\n",
              "4    44.30\n",
              "Name: Strength, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bju-0Ukl4Xi",
        "outputId": "4e9ba0ab-819b-4a8d-e37f-3a966d12d7b3"
      },
      "source": [
        "n_cols =x.shape[1]\r\n",
        "n_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jeSndzYbpb-"
      },
      "source": [
        "####Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHUZUpHlcXCg"
      },
      "source": [
        "#####Import keras, required libraries from keras "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiRuVl0ebma6"
      },
      "source": [
        "import keras"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe6dWofybwVj"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcO4RtVecVAT"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0F_i1bGeKEt",
        "outputId": "0bdee0f6-dd3f-4a9d-f78a-67d405b0bfda"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "  model.fit(xtr,ytr,epochs=50,verbose=0)\r\n",
        "  yhat = model.predict(xte)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[208.78719075219684,\n",
              " 110.21765980852055,\n",
              " 75.44642882028447,\n",
              " 57.179537742292446,\n",
              " 56.47588452747049,\n",
              " 56.30609398773349,\n",
              " 46.64302230558537,\n",
              " 44.6077556154711,\n",
              " 52.70194513936033,\n",
              " 58.34934149306482,\n",
              " 56.212630614337705,\n",
              " 56.49682386971899,\n",
              " 50.736910164330446,\n",
              " 47.067397453212266,\n",
              " 47.31957236864499,\n",
              " 52.51699821467591,\n",
              " 51.634915422363555,\n",
              " 51.2249242656657,\n",
              " 50.8234092497561,\n",
              " 57.833287754181,\n",
              " 50.94099504406682,\n",
              " 51.988153800656896,\n",
              " 46.15085897921341,\n",
              " 51.28483948004605,\n",
              " 46.604172953359225,\n",
              " 47.51488430830116,\n",
              " 50.601613160477235,\n",
              " 49.79918686274293,\n",
              " 49.82773759271403,\n",
              " 57.93126720184942,\n",
              " 49.44300303841611,\n",
              " 55.06164998385387,\n",
              " 49.40068809302663,\n",
              " 63.59704391510421,\n",
              " 47.55153618653243,\n",
              " 48.058632745777,\n",
              " 49.23373680839788,\n",
              " 49.27686962127714,\n",
              " 49.94821498457236,\n",
              " 48.995377631904,\n",
              " 46.97622441870908,\n",
              " 49.68288942900039,\n",
              " 55.33990990410741,\n",
              " 49.41690952812501,\n",
              " 58.686521942304935,\n",
              " 48.75190087889846,\n",
              " 42.968063550531575,\n",
              " 47.41789116646781,\n",
              " 45.20853947128713,\n",
              " 54.86924929032545]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vtHuQdMg8Zm",
        "outputId": "3fa33937-ebc4-4ea0-e741-ab654460c0af"
      },
      "source": [
        "len(mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6juhF7PmDV9",
        "outputId": "3822b15a-45e8-4f81-f0ee-a0971fe2c87a"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  56.02220583081826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGHfM6SBmpZV",
        "outputId": "ec83b2ec-1b71-4c0e-ac4e-f95bc8449b09"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  23.932111596695954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL8v3vGQm7gc"
      },
      "source": [
        "####Part B: Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TuoWwePoiWo"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvaWJVRvnkzi"
      },
      "source": [
        ">We have to normalize data after splitting inorder to avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzCd8qogm7Ho",
        "outputId": "187da696-7e0e-40f7-efd2-340b2b301609"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model2 = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "\r\n",
        "  #normalising train data and test data\r\n",
        "  xtr_norm = (xtr-xtr.mean())/xtr.std()\r\n",
        "  xte_norm = (xte-xte.mean())/xte.std()\r\n",
        "\r\n",
        "  model2.fit(xtr_norm,ytr,epochs=50,verbose=0)\r\n",
        "  yhat = model2.predict(xte_norm)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[439.84951565829823,\n",
              " 167.2881323805372,\n",
              " 116.52696255574693,\n",
              " 83.54628245267295,\n",
              " 76.60663386104797,\n",
              " 61.757964141753845,\n",
              " 56.253277830378764,\n",
              " 56.64436415803959,\n",
              " 54.520585508517506,\n",
              " 45.980364352914584,\n",
              " 49.14363910108146,\n",
              " 56.37780249059305,\n",
              " 46.39790492551496,\n",
              " 39.16821135704735,\n",
              " 41.974978936292786,\n",
              " 36.369992648610655,\n",
              " 48.216746779332325,\n",
              " 41.29339616500098,\n",
              " 41.626030411861464,\n",
              " 44.85540131943483,\n",
              " 40.44387756597624,\n",
              " 37.30751674425388,\n",
              " 35.29120919049977,\n",
              " 42.85916715023342,\n",
              " 36.067394689528435,\n",
              " 32.37308262335113,\n",
              " 33.67781526401075,\n",
              " 41.80650290679822,\n",
              " 29.221944303021292,\n",
              " 37.43589107325537,\n",
              " 28.78095468224503,\n",
              " 33.88428831292457,\n",
              " 31.07476143130202,\n",
              " 39.16864360017536,\n",
              " 31.385877187884176,\n",
              " 30.19296198398804,\n",
              " 29.71447800150181,\n",
              " 37.6188543020724,\n",
              " 36.15119229833056,\n",
              " 35.38366137592524,\n",
              " 33.790906348089905,\n",
              " 30.742139197309513,\n",
              " 31.611501065877704,\n",
              " 51.31374322062494,\n",
              " 36.54802304981925,\n",
              " 36.34287063024487,\n",
              " 33.746110452582286,\n",
              " 33.93495880154476,\n",
              " 40.0080606198992,\n",
              " 37.82650036534979]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vzCEkSppRFX",
        "outputId": "19333fd6-2866-46b3-84ac-820b80d775b7"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  53.402061509465945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6B64Bp7pRFa",
        "outputId": "54504aaf-59e1-445a-b51a-061e4c587b97"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  59.87128600220436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hro04rWXpaCd"
      },
      "source": [
        "Hence the mean of mse in part B is less than that of part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInrIIyqpnRq"
      },
      "source": [
        "####Part C: Increate the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVgimlfypvkl"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbELMosNpvku"
      },
      "source": [
        ">We have to normalize data after splitting inorder to avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCth-Fm9pvku",
        "outputId": "8ac69620-6253-4972-821d-793952b103ef"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model3 = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "\r\n",
        "  #normalising train data and test data\r\n",
        "  xtr_norm = (xtr-xtr.mean())/xtr.std()\r\n",
        "  xte_norm = (xte-xte.mean())/xte.std()\r\n",
        "\r\n",
        "  model3.fit(xtr_norm,ytr,epochs=100,verbose=0)  #using 100 epochs for training.\r\n",
        "  yhat = model3.predict(xte_norm)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[146.77663814658078,\n",
              " 93.36411251868697,\n",
              " 53.60856827550833,\n",
              " 54.160375988856494,\n",
              " 50.51217518092866,\n",
              " 42.72444408334272,\n",
              " 55.367063618185384,\n",
              " 52.721157784312624,\n",
              " 61.00882527227466,\n",
              " 40.69873948647878,\n",
              " 60.98705863141358,\n",
              " 58.68928764528565,\n",
              " 40.69438765651557,\n",
              " 48.67502476964363,\n",
              " 46.67612239466983,\n",
              " 41.95206048508998,\n",
              " 51.85971513273133,\n",
              " 48.0355478780723,\n",
              " 39.94775579386421,\n",
              " 49.31739933600077,\n",
              " 43.25639887985103,\n",
              " 43.50107340879277,\n",
              " 38.36548572429632,\n",
              " 48.24544365234896,\n",
              " 56.538003419331446,\n",
              " 38.00388134570902,\n",
              " 48.68388051667007,\n",
              " 43.580515103974314,\n",
              " 55.934734579020464,\n",
              " 45.840462613160085,\n",
              " 48.21558522152279,\n",
              " 57.90777854192687,\n",
              " 39.84193420736204,\n",
              " 36.00487964896491,\n",
              " 52.767859032660624,\n",
              " 45.70420930506415,\n",
              " 47.252787767547495,\n",
              " 48.21848723054029,\n",
              " 43.63666131184858,\n",
              " 39.89076369166843,\n",
              " 46.330738199374245,\n",
              " 42.68404208436195,\n",
              " 53.90925068991481,\n",
              " 42.09314420320541,\n",
              " 38.93413012029169,\n",
              " 44.34910885547187,\n",
              " 51.21606395428008,\n",
              " 47.93173970997465,\n",
              " 43.56194492410869,\n",
              " 46.28256957860196]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SgIu2hJpvkv",
        "outputId": "93ad0577-6aa0-4ebc-9fac-723914d90c01"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  50.32920035200576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJo7WJkupvkw",
        "outputId": "1630de8c-d282-4729-e1d5-0c29cf3e6aaf"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  16.401320812546707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CBO5vqpvkw"
      },
      "source": [
        "Hence the mean of mse in part C is less than that of part B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM7YTlrrqQZn"
      },
      "source": [
        "####Part D: Increase the number of hidden layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHi1kqQPqz0j"
      },
      "source": [
        "#define regression model\r\n",
        "def regression_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(10,activation=\"relu\",input_shape=(n_cols,)))\r\n",
        "  model.add(Dense(10,activation=\"relu\"))\r\n",
        "  model.add(Dense(10,activation=\"relu\"))\r\n",
        "  model.add(Dense(1))\r\n",
        "\r\n",
        "  #compile model\r\n",
        "  model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZkloCQpqz0v"
      },
      "source": [
        ">We have to normalize data after splitting inorder to avoid data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDopkmfXqz0v",
        "outputId": "a8a5b02a-d7a6-47ee-ce52-be74e7984b72"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "model4 = regression_model()\r\n",
        "mse = []  #empty mean squared error\r\n",
        "\r\n",
        "for i in range(0,50):\r\n",
        "  xtr,xte,ytr,yte = tts(x,y,test_size=0.3)\r\n",
        "\r\n",
        "  #normalising train data and test data\r\n",
        "  xtr_norm = (xtr-xtr.mean())/xtr.std()\r\n",
        "  xte_norm = (xte-xte.mean())/xte.std()\r\n",
        "\r\n",
        "  model4.fit(xtr_norm,ytr,epochs=50,verbose=0)\r\n",
        "  yhat = model4.predict(xte_norm)\r\n",
        "  error = MSE(yte,yhat)\r\n",
        "  mse.append(error)\r\n",
        "\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[167.13197699572015,\n",
              " 96.2181010256609,\n",
              " 68.9870875378906,\n",
              " 42.25445851966935,\n",
              " 43.125303642492945,\n",
              " 38.411045990772784,\n",
              " 43.05971860450492,\n",
              " 38.25554518064909,\n",
              " 41.131142892565244,\n",
              " 37.03294072599632,\n",
              " 36.32436007675568,\n",
              " 35.05650862971943,\n",
              " 32.80361489017227,\n",
              " 34.80491866617692,\n",
              " 32.394063226787836,\n",
              " 38.160598284182896,\n",
              " 35.21455992636211,\n",
              " 31.58021913726483,\n",
              " 31.401842463556147,\n",
              " 37.76445877871854,\n",
              " 32.85106630840039,\n",
              " 36.36219789288243,\n",
              " 50.485952043655494,\n",
              " 28.418990079617483,\n",
              " 32.543285587148425,\n",
              " 38.2472257795662,\n",
              " 34.67497844712749,\n",
              " 30.52514468383634,\n",
              " 65.3176453713059,\n",
              " 26.329527555873863,\n",
              " 26.83905808883635,\n",
              " 26.328902546426644,\n",
              " 28.20501476400095,\n",
              " 30.21827387605979,\n",
              " 27.850911193939886,\n",
              " 31.407096335029994,\n",
              " 28.94632415062459,\n",
              " 28.226737672174693,\n",
              " 30.480730782856906,\n",
              " 27.580197703325997,\n",
              " 36.23556069028866,\n",
              " 23.631209582008577,\n",
              " 27.007136601673878,\n",
              " 27.04049110129364,\n",
              " 27.326691068751657,\n",
              " 25.69224623322013,\n",
              " 23.726137453164625,\n",
              " 26.83396615744322,\n",
              " 29.003817274573755,\n",
              " 21.629016840631508]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BIXSnl1qz0w",
        "outputId": "2a61f0fd-6361-41e9-cff5-6c79e828dca9"
      },
      "source": [
        "#mean of mean squared errors.\r\n",
        "print(\"mean off mse = \",np.mean(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean off mse =  37.82155998122717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rYEUTc1qz0w",
        "outputId": "ca72987c-6b89-40c9-c661-746f0e90bae8"
      },
      "source": [
        "#standard deviation of the mean squared errors.\r\n",
        "print(\"standard deviation of the mse = \",np.std(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "standard deviation of the mse =  22.285606184757278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbG3QsJGqz0x"
      },
      "source": [
        "Hence the mean of mse in part D is less than that of part B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdhiHItHyLYU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOQ4SsPmFBSl"
      },
      "source": [
        "#Building Deep Learning Models with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crGfOJTTFCjl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}